{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRJ1_wsbi7cZ"
      },
      "source": [
        "<font size=10>**End-Term / Final Project**</font>\n",
        "\n",
        "<font size=6>**AI for Research Proposal Automation**</font>\n",
        "\n",
        "### **Business Problem - Create an AI system which will help you writing the research proposal aligning with the NOFO Document**\n",
        "   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsZ53h43336R"
      },
      "source": [
        "Meet Dr. Ian McCulloh, a seasoned research advisor and a leading voice in interdisciplinary science. Over the years, his lab has explored everything from AI for counterterrorism to social network analysis in neuroscience. His publication portfolio is vast, rich, and... chaotic.\n",
        "\n",
        "When the National Institute of Mental Health released a new NOFO (Notice of Funding Opportunity) seeking innovative digital health solutions for mental health equity, Dr. Ian saw an opportunity. But there was a problem: despite his extensive work, none of his existing research was directly aligned with digital mental health interventions. And with NIH deadlines looming, manually identifying relevant angles and generating a competitive proposal would be a massive lift.\n",
        "\n",
        "Dr. Ian wished for a smart assistant—one that could digest his past work, interpret the NOFO’s intent, spark new research directions, and even help draft proposal sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATYzMPf1333q"
      },
      "source": [
        "**The Challenge:**\n",
        "\n",
        "Organizations and researchers often maintain large archives of publications and prior work. When responding to competitive grants—especially highly specific ones like NIH NOFOs—it becomes extremely difficult and time-consuming to:\n",
        "\n",
        "1. Align past work with a new funding call.\n",
        "2. Extract relevant expertise from unrelated projects.\n",
        "3. Ideate novel, fundable research proposals tailored to complex criteria.\n",
        "4. Generate high-quality text for grant submission that satisfies technical and scientific review criteria.\n",
        "\n",
        "The manual effort to sift through dense research documents, match them to nuanced funding criteria, and write compelling, compliant proposals is labor-intensive, inconsistent, and prone to missed opportunities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsDECO7z5eMJ"
      },
      "source": [
        "### **The Case Study Approach**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syX7mfDi5eb8"
      },
      "source": [
        "**Objective**\n",
        "1. Develop a generative AI-powered system using LLMs to automate and optimize the creation of NIH research proposals.\n",
        "2. The tool will identify relevant prior research, generate aligned project ideas, and draft high-quality proposal content tailored to specific NOFO requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Given workflow:**\n",
        "\n",
        "```mermaid\n",
        "flowchart TD\n",
        "    A[Read NOFO Document] --> B[Analyze Research Papers]\n",
        "    B --> C[Filter Papers by Topic]\n",
        "    C --> D[Generate Research Ideas]\n",
        "    D --> E[Upload ideas to LLM]\n",
        "    E --> F[Generate Proposal]\n",
        "    F --> G[LLM Evaluation]\n",
        "    G --> H{Meets criteria?}\n",
        "    H -- NO --> F\n",
        "    H -- YES --> I[Human Review]\n",
        "    I --> J{Approved?}\n",
        "    J -- NO --> F\n",
        "    J -- YES --> K[Final Proposal]\n",
        "```\n",
        "\n",
        "**Enhanced workflow based on conversations with ChatGPT and Claude:**\n",
        "\n",
        "```mermaid\n",
        "flowchart TD\n",
        "    A[Read NOFO Document] --> B[Extract Key Requirements & Evaluation Criteria]\n",
        "    B --> C[Multi-Stage Paper Processing<br>(PyPDF → OCR → Table/Figure Extraction)]\n",
        "    C --> D[Hybrid Indexing & Filtering<br>(BM25 + Embeddings + Metadata)]\n",
        "    D --> E[Agentic Research Synthesis<br>(Research Analyst + Proposal Writer + Compliance Checker)]\n",
        "    E --> F[Generate Proposal Blueprint + Draft]\n",
        "    F --> G[Multi-Criteria Evaluation<br/>(RAG + Prompt Scoring + Guardrails)]\n",
        "    G --> H{Score ≥ Threshold?}\n",
        "    H -- NO --> I[Targeted Refinement Loop<br/>(Weakness-Specific Prompts)]\n",
        "    I --> F\n",
        "    H -- YES --> J[Caching + Persistence of Results]\n",
        "    J --> K[Human Review Interface]\n",
        "    K --> L{Approved?}\n",
        "    L -- NO --> M[Capture Feedback & Return to Refinement]\n",
        "    M --> F\n",
        "    L -- YES --> N[Final Proposal + Deliverables]\n",
        "    \n",
        "    subgraph \"Agentic Components\"\n",
        "        E1[Research Analyst Agent]\n",
        "        E2[Proposal Writer Agent]\n",
        "        E3[Compliance Checker Agent]\n",
        "        E1 --> E2\n",
        "        E2 --> E3\n",
        "        E3 --> E1\n",
        "    end\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cxtywSY4tq0"
      },
      "source": [
        "## **Setup - [2 Marks]**\n",
        "---\n",
        "<font color=Red>**Note:**</font> *1 marks is awarded for the Embedding Model configuration and 1 mark for the LLM Configuration.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b52EI78ZiY1X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# @title Run this cell => Restart the session => Start executing the below cells **(DO NOT EXECUTE THIS CELL AGAIN)**\n",
        "\n",
        "!pip install -q langchain==0.3.21 \\\n",
        "                huggingface_hub==0.29.3 \\\n",
        "                openai==1.68.2 \\\n",
        "                chromadb==0.6.3 \\\n",
        "                langchain-community==0.3.20 \\\n",
        "                langchain_openai==0.3.10 \\\n",
        "                lark==1.2.2\\\n",
        "                rank_bm25==0.2.2\\\n",
        "                numpy==2.2.4 \\\n",
        "                scipy==1.15.2 \\\n",
        "                scikit-learn==1.6.1 \\\n",
        "                transformers==4.50.0 \\\n",
        "                pypdf==5.4.0 \\\n",
        "                markdown-pdf==1.7 \\\n",
        "                tiktoken==0.9.0 \\\n",
        "                sentence_transformers==4.0.0 \\\n",
        "                torch==2.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ipywidgets==8.1.2 (from -r ../requirements.txt (line 1))\n",
            "  Downloading ipywidgets-8.1.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting matplotlib==3.8.4 (from -r ../requirements.txt (line 2))\n",
            "  Downloading matplotlib-3.8.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting pandas==2.2.2 (from -r ../requirements.txt (line 3))\n",
            "  Downloading pandas-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting torch==2.7.1 (from -r ../requirements.txt (line 4))\n",
            "  Downloading torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting torchvision==0.21.0 (from -r ../requirements.txt (line 5))\n",
            "  Downloading torchvision-0.21.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting tqdm==4.66.4 (from -r ../requirements.txt (line 6))\n",
            "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: comm>=0.1.3 in /home/codespace/.local/lib/python3.12/site-packages (from ipywidgets==8.1.2->-r ../requirements.txt (line 1)) (0.2.2)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from ipywidgets==8.1.2->-r ../requirements.txt (line 1)) (9.4.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from ipywidgets==8.1.2->-r ../requirements.txt (line 1)) (5.14.3)\n",
            "Collecting widgetsnbextension~=4.0.10 (from ipywidgets==8.1.2->-r ../requirements.txt (line 1))\n",
            "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting jupyterlab-widgets~=3.0.10 (from ipywidgets==8.1.2->-r ../requirements.txt (line 1))\n",
            "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib==3.8.4->-r ../requirements.txt (line 2)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib==3.8.4->-r ../requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib==3.8.4->-r ../requirements.txt (line 2)) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib==3.8.4->-r ../requirements.txt (line 2)) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from matplotlib==3.8.4->-r ../requirements.txt (line 2)) (2.2.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib==3.8.4->-r ../requirements.txt (line 2)) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib==3.8.4->-r ../requirements.txt (line 2)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib==3.8.4->-r ../requirements.txt (line 2)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib==3.8.4->-r ../requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas==2.2.2->-r ../requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas==2.2.2->-r ../requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from torch==2.7.1->-r ../requirements.txt (line 4)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch==2.7.1->-r ../requirements.txt (line 4)) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch==2.7.1->-r ../requirements.txt (line 4)) (80.9.0)\n",
            "Collecting sympy>=1.13.3 (from torch==2.7.1->-r ../requirements.txt (line 4))\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch==2.7.1->-r ../requirements.txt (line 4)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch==2.7.1->-r ../requirements.txt (line 4)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from torch==2.7.1->-r ../requirements.txt (line 4)) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.1->-r ../requirements.txt (line 4))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.1->-r ../requirements.txt (line 4))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.1->-r ../requirements.txt (line 4))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.1->-r ../requirements.txt (line 4))\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.1->-r ../requirements.txt (line 4))\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.1->-r ../requirements.txt (line 4))\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.1->-r ../requirements.txt (line 4))\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.1->-r ../requirements.txt (line 4))\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.1->-r ../requirements.txt (line 4))\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.1->-r ../requirements.txt (line 4))\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.1->-r ../requirements.txt (line 4))\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.1->-r ../requirements.txt (line 4))\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.1->-r ../requirements.txt (line 4))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.1->-r ../requirements.txt (line 4))\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.3.1 (from torch==2.7.1->-r ../requirements.txt (line 4))\n",
            "  Downloading triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install -r ../requirements.txt (line 5) and torch==2.7.1 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    The user requested torch==2.7.1\n",
            "    torchvision 0.21.0 depends on torch==2.6.0\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -r ../requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HihlEckiaCu"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCecgxIKiq1l"
      },
      "outputs": [],
      "source": [
        "# @title Loading the `config.json` file\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Load the JSON file and extract values\n",
        "file_name = 'config.json'\n",
        "with open(file_name, 'r') as file:\n",
        "    config = json.load(file)\n",
        "    os.environ['OPENAI_API_KEY'] = config.get(\"\") # Loading the API Key\n",
        "    os.environ[\"OPENAI_BASE_URL\"] = config.get(\"\") # Loading the API Base Url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZoxTajfis9P"
      },
      "outputs": [],
      "source": [
        "# @title Defining the LLM Model - Use `gpt-4o-mini` Model\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model=\"\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG1Ah1eDi7aK"
      },
      "source": [
        "## **Step 1: Topic Extraction - [3 Marks]**\n",
        "\n",
        "> **Read the NOFO doc and identify the topic for which the funding is to be given.**\n",
        "---\n",
        "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt and 1 mark for the successful completion of this section, including debugging or modifying the code if necessary.*\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgeaimvijXvC"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# Reading the NOFO Document\n",
        "pdf_file = \"\"\n",
        "pdf_loader = PyPDFLoader(pdf_file);\n",
        "NOFO_pdf = pdf_loader.load()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkW_lO_CHSpc"
      },
      "source": [
        "**TASK:** Write an LLM prompt to extract the Topic for what the funding is been provided, from the NOFO document, Ask the LLM to respond back with the topic name only and nothing else."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DcEpaUYNM_W"
      },
      "outputs": [],
      "source": [
        "topic_extraction_Prompt = f\"\"\"\n",
        "\n",
        "<WRITE YOUR PROMPT HERE>\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WtmCEbaKN9aW",
        "outputId": "7f9d4f38-5e38-4c9c-f70a-3976dfb8c881"
      },
      "outputs": [],
      "source": [
        "# Finding the topic for which the Funding is been given\n",
        "topic_extraction = llm.invoke(topic_extraction_Prompt)\n",
        "topic = topic_extraction.content\n",
        "topic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXHRa9IlMycZ"
      },
      "source": [
        "## **Step 2: Research Paper Relevance Assessment - [3 Marks]**\n",
        "> **Analyze all the Research Papers and filter out the research papers based on the topic of NOFO**\n",
        "---\n",
        "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt and 1 mark for the successful completion of this section, including debugging or modifying the code if necessary.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kWc0LaCGPo3"
      },
      "source": [
        "**TASK:** Write an Prompt which can be used to analyze the relevance of the provided research paper in relation to the topic outlined in the NOFO (Notice of Funding Opportunity) document. Determine whether the research aligns with the goals, objectives, and funding criteria specified in the NOFO. Additionally, assess whether the research paper can be used to support or develop a viable project idea that fits within the scope of the funding opportunity.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note:** If the paper does **not** significantly relate to the topic—by domain, method, theory, or application ask the LLM to return: **\"PAPER NOT RELATED TO TOPIC\"**\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "Ask the LLM to respond in the below specified structure:\n",
        "\n",
        "```\n",
        "### Output Format:\n",
        "\"summary\": \"<summary of the paper under 300 words, or return: PAPER NOT RELATED TO TOPIC>\"\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAsEsgCUkhT4"
      },
      "outputs": [],
      "source": [
        "# Unzipping the Research Papers - Replace your zip file path and extarct it in the contents folder only\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\" <YOUR ZIP FILE PATH OF RESEARCH PAPERS> \", 'r') as zip_ref:\n",
        "  zip_ref.extractall(\"/content/\") # No changes needed here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3LPwoNeyXC7"
      },
      "outputs": [],
      "source": [
        "relevance_prompt = f\"\"\"\n",
        "\n",
        "\n",
        "<WRITE YOUR PROMPT HERE>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Research Paper Context:\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9wo3Tg4vkhRT",
        "outputId": "781c7069-0ad4-41cb-94fc-cfca3ef34df9"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "# Reading all PDF files and storing it in 1 variable\n",
        "path = \"/content/Papers/\"\n",
        "documents = []\n",
        "total_files  = len(os.listdir(path))\n",
        "\n",
        "# Defining the max tokens to avoid error for context being to long\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
        "MAX_TOKENS = 127500\n",
        "\n",
        "progress_cnt = 1\n",
        "relevant_papers_count = 0\n",
        "irrelevant_papers_count = 0\n",
        "\n",
        "for filename in os.listdir(path):\n",
        "    if filename.endswith('.pdf'):\n",
        "        file_path = os.path.join(path, filename)\n",
        "\n",
        "        try:\n",
        "            # Load PDF\n",
        "            docs = PyPDFLoader(file_path,mode=\"single\").load()\n",
        "            # extracting the pages\n",
        "            pages = docs[0].page_content\n",
        "\n",
        "            # combining the prompt with the pages of the research paper within the context length\n",
        "            available_tokens = MAX_TOKENS - len(encoding.encode(relevance_prompt ))\n",
        "            truncated_pages = encoding.decode(encoding.encode(pages)[:available_tokens])\n",
        "            full_prompt = relevance_prompt + truncated_pages\n",
        "\n",
        "            # Calling the LLM\n",
        "            response = llm.invoke(full_prompt)\n",
        "\n",
        "            print(f\"Successfully processed: {progress_cnt}/{total_files}\")\n",
        "            progress_cnt += 1\n",
        "\n",
        "            #  If the paper is not relevant skipping the paper\n",
        "            if  \"PAPER NOT RELATED TO TOPIC\" in response.content:\n",
        "              irrelevant_papers_count += 1\n",
        "              continue\n",
        "\n",
        "            #  If the paper is relevant adding it to the documents variable\n",
        "            documents.append({ 'title': filename, 'llm_response': response.content, 'file_path':file_path})\n",
        "            relevant_papers_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"!!! Error processing {filename}: {str(e)}\")\n",
        "\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"Relevant Papers: {relevant_papers_count}/{total_files}\")\n",
        "print(f\"Irrelevant Papers: {irrelevant_papers_count}/{total_files}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjVMJE_wTaCc",
        "outputId": "515d6100-5139-4baf-be86-e3888fba6cdb"
      },
      "outputs": [],
      "source": [
        "documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNdBg6Iei7VJ"
      },
      "source": [
        "## **Step 3: Proposal Ideation Based on Filtered Research - [4 marks]**\n",
        "> **Use the filtered papers, to generate ideas for the Reseach Proposal.**\n",
        "---\n",
        "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt, 1 mark for the Generating Idea and 1 mark for fetching file path of chosen idea along with successful completion of this section, including debugging or modifying the code if necessary.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN5c3WhIEpzL"
      },
      "source": [
        "**TASK:** Write an Prompt which can be used to generate 5 ideas for the Research Proposal, each idea should consist:\n",
        "\n",
        "1. **Idea X:** [Concise Title of the Project Idea]  \\n\n",
        "2. **Description:** [Brief and targeted description summarizing the objectives, innovative elements, scientific rationale, and anticipated impact.]  \\n\n",
        "3. **Citation:** [Author(s), Year or Paper Title]  \\n\n",
        "4. **NOFO Alignment:** [List two or more specific NOFO requirements that this idea directly addresses]  \\n\n",
        "5. **File Path of the Research Paper:** [Exact file path, ending in .pdf]\n",
        "\n",
        "- Use the Delimiter `---` for defining the structure of the sample outputs in the prompt\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLgOVonjveNM"
      },
      "source": [
        "#### Generating 5 Ideas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoHQ6HH1kiD4"
      },
      "outputs": [],
      "source": [
        "gen_idea_prompt = f\"\"\"\n",
        "\n",
        "\n",
        "<WRITE YOUR PROMPT HERE>\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-oCJeXkcBKd"
      },
      "outputs": [],
      "source": [
        "ideas = llm.invoke(gen_idea_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "id": "TQpW_7cKco8Q",
        "outputId": "6b59af58-d64c-4804-f5a6-bfe89bb023dd"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "display(Markdown(ideas.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLD4_7trvhKL"
      },
      "source": [
        "#### Choosing 1 Idea and fetching details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3ulgD6_dkrJ"
      },
      "outputs": [],
      "source": [
        "# Modify the idea_number for choosing the different idea\n",
        "idea_number = 5   # change the number if you wish to choose and generate the research proposal for another idea\n",
        "chosen_idea = ideas.content.split(\"---\")[idea_number]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcV8QY1irIyH",
        "outputId": "d71e7380-e5d3-49d8-cd93-3dceedd57cf8"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Use a regular expression to find the file path of the research paper\n",
        "\n",
        "pattern = r\"File Path of the Research Paper:\\*\\*\\s*(.+?)\\n\"\n",
        "# If you are unable to extract the file path successfully using this pattern, use the `ChatGPT` or any other LLM to find the pattern that works for you, simply provide the LLM the sample response of your whole ideas and ask the LLM to generate the regex patterm for extracting the \"File Path of the Research Paper\"\n",
        "\n",
        "match = re.search(pattern, chosen_idea)\n",
        "\n",
        "if match:\n",
        "  idea_generated_from_research_paper = match.group(1).strip()\n",
        "  print(\"Filepath : \", idea_generated_from_research_paper)\n",
        "else:\n",
        "  print(\"File Path of the Research Paper not found in the chosen idea.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51quLkMgi7S5"
      },
      "source": [
        "## **Step 4: Proposal Blueprint Preparation - [3 Marks]**\n",
        "\n",
        "> **Select appropriate research ideas for the proposal and supply 'Sample Research Proposals' as templates to the LLM to support the generation of the final proposal.**\n",
        "---   \n",
        "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt and 1 mark for the successful completion of this section, including debugging or modifying the code if necessary.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJhO1BFHC7cE"
      },
      "source": [
        "**TASK:** Write an Prompt which can be used to generate the Research Proposal.\n",
        "\n",
        "The prompt should be able to craft a research proposal based on the sample research proposal template, using one of the ideas generated above. The proposal should include references to the actual research papers from which the ideas are derived and should align well with the NOFO documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMOe-9_AgKvN"
      },
      "outputs": [],
      "source": [
        "# Here we need to add the full papers instead of the summary\n",
        "chosen_idea_rp = PyPDFLoader(idea_generated_from_research_paper, mode=\"single\").load()\n",
        "\n",
        "# Loading the sample research proposal template\n",
        "research_proposal_template = PyPDFLoader(\" <Path of Research Proposal Template> \", mode=\"single\").load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OGcodZLo7VE"
      },
      "outputs": [],
      "source": [
        "research_proposal_template_prompt = f\"\"\"\n",
        "\n",
        "\n",
        "<WRITE YOUR PROMPT HERE>\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gXIZE0jkieg"
      },
      "outputs": [],
      "source": [
        "research_plan = llm.invoke(research_proposal_template_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1UFS-Vxlkib0",
        "outputId": "2761571a-d2a0-421c-b429-252cc78ddd41"
      },
      "outputs": [],
      "source": [
        "display(Markdown(research_plan.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkuiPncysdCa"
      },
      "outputs": [],
      "source": [
        "# @title **Optional Part - Creating a PDF of the Research Proposal**\n",
        "# The code in this cell block is used for printing out the output in the PDF format\n",
        "from markdown_pdf import MarkdownPdf, Section\n",
        "\n",
        "pdf = MarkdownPdf()\n",
        "pdf.add_section(Section(research_plan.content))\n",
        "pdf.save(\"Reseach Proposal First Draft.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77t_iYgni7QV"
      },
      "source": [
        "## **Step 5: Proposal Evaluation Against NOFO Criteria - [3 Marks]**\n",
        "> **Use the LLM to evaluate the generated proposal (LLM-as-Judge) and assess its alignment with the NOFO criteria.**\n",
        "   \n",
        "\n",
        "---\n",
        "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt and 1 mark for the successful completion of this section, including debugging or modifying the code if necessary.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXWK_mZewlim"
      },
      "source": [
        "**TASK:** Write an Prompt which can be used to evaluate the Research Proposal based on:\n",
        "1. **Innovation**\n",
        "2. **Significance**\n",
        "3. **Approach**\n",
        "4. **Investigator Expertise**\n",
        "\n",
        "- Ask the LLM to rate on each of the criteria from **1 (Poor)** to **5 (Excellent)**\n",
        "- Ask the LLM to provide the resonse in the json format\n",
        "```JSON\n",
        "name: Innovation\n",
        "    justification: \"<Justification>\"\n",
        "    score: <1-5>\n",
        "    strengths: \"<Strength 1>\"\n",
        "    weaknesses: \"<Weakness 1>\"\n",
        "    recommendations: \"<Recommendation 1>\"\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ax5H703ZhZ7y"
      },
      "outputs": [],
      "source": [
        "evaluation_prompt = f'''\n",
        "\n",
        "\n",
        "<WRITE YOUR PROMPT HERE>\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aZox8iNhZ5g"
      },
      "outputs": [],
      "source": [
        "eval_response = llm.invoke(evaluation_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCx7_am-hZ3H"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "json_resp = json.loads(eval_response.content[7:-3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwAFtxolhZpD",
        "outputId": "a5d2b346-065a-428a-8c5d-a38cd57a90ae"
      },
      "outputs": [],
      "source": [
        "for key, value in json_resp.items():\n",
        "  print(f\"---\\n{key}:\")\n",
        "  if isinstance(value, list):\n",
        "    for item in value:\n",
        "      for k, v in item.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "      print(\"=\"*50)\n",
        "  elif isinstance(value, dict):\n",
        "    for k, v in value.items():\n",
        "      print(f\"  {k}: {v}\")\n",
        "  else:\n",
        "    print(f\"  {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WvOSFDehZmg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fomQFyZAi7N4"
      },
      "source": [
        "## **Step 6: Human Review and Refinement of Proposal**\n",
        "> **Perform Human Evaluation of the generated Proposal. Edit or Modify the proposal as necessary.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HWwfHOXHmMOu",
        "outputId": "87320d16-f244-429e-e5cb-215e5eae01e7"
      },
      "outputs": [],
      "source": [
        "display(Markdown(research_plan.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRbWRHAJ_KXz"
      },
      "source": [
        "# **Step 7: Summary and Recommendation - [2 Marks]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC6F4JezA840"
      },
      "source": [
        "Based on the projects, learners are expected to share their observations, key learnings, and insights related to this business use case, including the challenges they encountered.\n",
        "\n",
        "Additionally, they should recommend or explain any changes that could improve the project, along with suggesting additional steps that could be taken for further enhancement.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8_WFIYIB12b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "3cxtywSY4tq0",
        "ZG1Ah1eDi7aK",
        "cXHRa9IlMycZ"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
