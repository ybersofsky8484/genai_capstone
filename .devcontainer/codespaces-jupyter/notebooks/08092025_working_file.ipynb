{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1942eb2c",
   "metadata": {},
   "source": [
    "\n",
    "# Notebook Map: Relevance Evaluation Pipeline with Few-Shot + Agentic Enhancements\n",
    "\n",
    "This table of contents provides a structured overview of the notebook, describing each section's purpose and how it fits into the workflow.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Quick Reference\n",
    "- Overview of semantic versioning, few-shot prompting, and agentic conflict resolver.\n",
    "\n",
    "## 2. Imports and Configuration\n",
    "- Load required libraries and define configuration constants (e.g., few-shot parameters, log paths).\n",
    "\n",
    "## 3. Core Utility Functions\n",
    "- `verify_decision`: Ensures model decisions are consistent.\n",
    "- `calculate_rule_confidence`: Computes rule-based confidence from criteria.\n",
    "- `get_next_prompt_version`: Auto-increments semantic prompt version.\n",
    "- `rebuild_few_shot_pool`: Builds balanced few-shot example set from log.\n",
    "- `agentic_conflict_resolver`: Resolves discrepancies between model and rule evaluations.\n",
    "\n",
    "## 4. Data Preparation\n",
    "- Load PDF research papers from `data/raw`.\n",
    "- Truncate text to fit LLM context window.\n",
    "\n",
    "## 5. Few-Shot Prompt Building\n",
    "- Retrieve high-confidence examples from log.\n",
    "- Prepend examples to base relevance prompt.\n",
    "\n",
    "## 6. Main Evaluation Loop\n",
    "- Iterate through PDFs.\n",
    "- Evaluate relevance using LLM.\n",
    "- Apply rule-based scoring and hybrid confidence calculation.\n",
    "- Flag documents for review when model vs. rule confidence diverges.\n",
    "\n",
    "## 7. Logging and Versioning\n",
    "- Append results to `prompt_evaluation_log.json`.\n",
    "- Add `prompt_version`, `decision_source`, and `agentic_resolution` where applicable.\n",
    "\n",
    "## 8. Visualization\n",
    "- Display confidence distribution, relevance drift, and flagged discrepancy trends.\n",
    "\n",
    "## 9. Enhancements (Appended)\n",
    "- Additional functions and logging improvements appended at the end for optional use.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRJ1_wsbi7cZ"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "<font size=10>**End-Term / Final Project**</font>\n",
    "\n",
    "<font size=6>**AI for Research Proposal Automation**</font>\n",
    "\n",
    "### **Business Problem - Create an AI system which will help you writing the research proposal aligning with the NOFO Document**\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsZ53h43336R"
   },
   "source": [
    "Meet Dr. Ian McCulloh, a seasoned research advisor and a leading voice in interdisciplinary science. Over the years, his lab has explored everything from AI for counterterrorism to social network analysis in neuroscience. His publication portfolio is vast, rich, and... chaotic.\n",
    "\n",
    "When the National Institute of Mental Health released a new NOFO (Notice of Funding Opportunity) seeking innovative digital health solutions for mental health equity, Dr. Ian saw an opportunity. But there was a problem: despite his extensive work, none of his existing research was directly aligned with digital mental health interventions. And with NIH deadlines looming, manually identifying relevant angles and generating a competitive proposal would be a massive lift.\n",
    "\n",
    "Dr. Ian wished for a smart assistant—one that could digest his past work, interpret the NOFO’s intent, spark new research directions, and even help draft proposal sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATYzMPf1333q"
   },
   "source": [
    "**The Challenge:**\n",
    "\n",
    "Organizations and researchers often maintain large archives of publications and prior work. When responding to competitive grants—especially highly specific ones like NIH NOFOs—it becomes extremely difficult and time-consuming to:\n",
    "\n",
    "1. Align past work with a new funding call.\n",
    "2. Extract relevant expertise from unrelated projects.\n",
    "3. Ideate novel, fundable research proposals tailored to complex criteria.\n",
    "4. Generate high-quality text for grant submission that satisfies technical and scientific review criteria.\n",
    "\n",
    "The manual effort to sift through dense research documents, match them to nuanced funding criteria, and write compelling, compliant proposals is labor-intensive, inconsistent, and prone to missed opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsDECO7z5eMJ"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "### **The Case Study Approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syX7mfDi5eb8"
   },
   "source": [
    "**Objective**\n",
    "1. Develop a generative AI-powered system using LLMs to automate and optimize the creation of NIH research proposals.\n",
    "2. The tool will identify relevant prior research, generate aligned project ideas, and draft high-quality proposal content tailored to specific NOFO requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given workflow:**\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Read NOFO Document] --> B[Analyze Research Papers]\n",
    "    B --> C[Filter Papers by Topic]\n",
    "    C --> D[Generate Research Ideas]\n",
    "    D --> E[Upload ideas to LLM]\n",
    "    E --> F[Generate Proposal]\n",
    "    F --> G[LLM Evaluation]\n",
    "    G --> H{Meets criteria?}\n",
    "    H -- NO --> F\n",
    "    H -- YES --> I[Human Review]\n",
    "    I --> J{Approved?}\n",
    "    J -- NO --> F\n",
    "    J -- YES --> K[Final Proposal]\n",
    "```\n",
    "\n",
    "**Enhanced workflow A based on conversations with ChatGPT and Claude:**\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Read NOFO Document] --> B[Extract Key Requirements & Evaluation Criteria]\n",
    "    B --> C[Multi-Stage Paper Processing<br>(PyPDF → OCR)]\n",
    "    C --> C1[Table Extraction]\n",
    "    C --> C2[Figure Extraction (OCR + Captioning)]\n",
    "    C1 --> D\n",
    "    C2 --> D\n",
    "    D[Hybrid Indexing & Filtering<br>(BM25 + Embeddings + Metadata)]\n",
    "    D --> E[Agentic Research Synthesis<br>(Research Analyst + Proposal Writer + Compliance Checker)]\n",
    "    E --> F[Generate Proposal Blueprint + Draft]\n",
    "    F --> G[Multi-Criteria Evaluation<br/>(RAG + LLM-as-Judge + Guardrails)]\n",
    "    G --> H{Score ≥ Threshold?}\n",
    "    H -- NO --> I[Targeted Refinement Loop<br/>(Weakness-Specific Prompts)]\n",
    "    I --> F\n",
    "    H -- YES --> J[Caching + Checkpointing of Results]\n",
    "    J --> K[Human Review Interface]\n",
    "    K --> L{Approved?}\n",
    "    L -- NO --> M[Capture Feedback & Return to Refinement]\n",
    "    M --> F\n",
    "    L -- YES --> N[Final Proposal + Deliverables]\n",
    "    \n",
    "    subgraph \"Agentic Components\"\n",
    "        E1[Research Analyst Agent]\n",
    "        E2[Proposal Writer Agent]\n",
    "        E3[Compliance Checker Agent]\n",
    "        E1 <--> E2\n",
    "        E2 <--> E3\n",
    "        E3 <--> E1\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Enhanced workflow B based iterations:**\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    %% ================================\n",
    "    %% DATA INGESTION & PREP\n",
    "    %% ================================\n",
    "    A[NOFO PDF]:::doc --> A1[Topic Extraction (LLM 3–8 words)]\n",
    "    B[112 Research Papers (PDFs)]:::doc --> B1[Pre-process PDFs<br/>(clean, dedupe, normalize)]\n",
    "    B1 --> B2[Chunk Papers] --> B3[Embed Chunks → Vector DB (FAISS)]\n",
    "    B2 --> B4[Keyword Index (BM25)]\n",
    "    \n",
    "    %% ================================\n",
    "    %% FEW-SHOT & CONFIG\n",
    "    %% ================================\n",
    "    C[Few-shot Examples<br/>(from prior logs)]:::meta --> C1[Build Prompt Prefix]\n",
    "    A1 --> C1\n",
    "    C1 --> P0[Prompt(s) for Relevance & Evaluation]:::meta\n",
    "    \n",
    "    %% ================================\n",
    "    %% PHASE 1: HYBRID RETRIEVAL\n",
    "    %% ================================\n",
    "    subgraph PH1[Phase 1 — Hybrid Retrieval & Paper Screening]\n",
    "      direction TB\n",
    "      B3 --> D1[Hybrid Search: BM25 + Cosine<br/>(overfetch top chunks)]\n",
    "      B4 --> D1\n",
    "      D1 --> D2[Group by Paper → Best Hybrid Score]\n",
    "      D2 --> D3[Chunk Count Normalization]\n",
    "      D3 --> D4[Rank Papers (normalized)]\n",
    "      D4 --> D5[Select Top N for Phase 2]\n",
    "    end\n",
    "    \n",
    "    %% ================================\n",
    "    %% PHASE 2: LLM RELEVANCE\n",
    "    %% ================================\n",
    "    subgraph PH2[Phase 2 — LLM Relevance (Nuanced)]\n",
    "      direction TB\n",
    "      D5 --> E1[Fetch all chunks for Top N]\n",
    "      E1 --> E2[Cap top-k chunks per paper (e.g., 10)]\n",
    "      E2 --> E3[LLM Relevance Scoring per Paper<br/>(confidence 0–100)]\n",
    "      E3 --> E4[Trim false positives; Final Ranked Set]\n",
    "    end\n",
    "    \n",
    "    %% ================================\n",
    "    %% IDEATION → BLUEPRINT → DRAFT\n",
    "    %% ================================\n",
    "    subgraph GEN[Proposal Generation]\n",
    "      direction TB\n",
    "      E4 --> G1[Proposal Ideation (5 ideas)<br/>NOFO + Priorities + Matched Papers]\n",
    "      G1 --> G2[Select 1 Idea + Fetch Details]\n",
    "      G2 --> G3[Proposal Blueprint<br/>(sections, objectives, methods)]\n",
    "      G3 --> G4[Draft Proposal from Template]\n",
    "    end\n",
    "    \n",
    "    %% ================================\n",
    "    %% EVALUATION & REVIEW\n",
    "    %% ================================\n",
    "    subgraph EVAL[Evaluation & Review]\n",
    "      direction TB\n",
    "      G4 --> H1[Evaluate vs NOFO Criteria<br/>(LLM-as-judge JSON)]\n",
    "      H1 --> H2[Human Review & Refinement]\n",
    "      H2 --> H3[Summary & Recommendations]\n",
    "    end\n",
    "    \n",
    "    %% ================================\n",
    "    %% LOGGING / VERSIONING / VIS\n",
    "    %% ================================\n",
    "    subgraph LOG[Logging, Versioning, Visualization]\n",
    "      direction TB\n",
    "      D1 --> L1[Log retrieval & scores]\n",
    "      E3 --> L1\n",
    "      G4 --> L2[Versioned Outputs]\n",
    "      H1 --> L3[Visualization (optional)]\n",
    "    end\n",
    "\n",
    "    classDef doc fill:#eef,stroke:#88a,stroke-width:1.5px;\n",
    "    classDef meta fill:#efe,stroke:#7a7,stroke-width:1.5px;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cxtywSY4tq0"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Setup - [2 Marks]**\n",
    "---\n",
    "<font color=Red>**Note:**</font> *1 marks is awarded for the Embedding Model configuration and 1 mark for the LLM Configuration.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "b52EI78ZiY1X"
   },
   "outputs": [],
   "source": [
    "# Install required packages with progress and output displayed\n",
    "\n",
    "# Encountered multiple conflicts between packages and within codespace core packages. Ended up installing all packages via the .venv\n",
    "\n",
    "# DISPLAY FINAL REQUIREMENTS.TXT for final file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for core functionality\n",
    "import os\n",
    "import warnings\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "base_url = os.getenv(\"OPENAI_BASE_URL\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM Model - Use `gpt-4o-mini` Model\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\")  # optional; only if using non-default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# FEW-SHOT AND LOGGING CONFIG\n",
    "# ------------------------------------------------------------\n",
    "# These constants control how many examples are retrieved and the minimum confidence threshold.\n",
    "# Modify here if you want more or fewer few-shot examples or to change the confidence cutoff.\n",
    "FEW_SHOT_MAX_EXAMPLES = 4         # Total examples (balanced between relevant/irrelevant if possible)\n",
    "# Minimum confidence threshold for including examples in few-shot prompting\n",
    "MIN_CONFIDENCE_FOR_FEWSHOT = 70   # Minimum hybrid confidence (%) to consider for few-shot retrieval\n",
    "\n",
    "# JSON log path\n",
    "# Path to the cleaned JSON log file where prompt evaluation iterations are stored\n",
    "LOG_PATH = \"prompt_evaluation_log_cleaned.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned PDF saved to: ../data/NOFO_cleaned.pdf\n",
      "Cleaning annotations for: cycon-final-draft.pdf\n",
      "Cleaned PDF saved to: data/raw/cycon-final-draft_cleaned.pdf\n",
      "Cleaning annotations for: Chat GPT Bias final w copyright.pdf\n",
      "Cleaned PDF saved to: data/raw/Chat GPT Bias final w copyright_cleaned.pdf\n",
      "Cleaning annotations for: Genetic_Algorithms_for_Prompt_Optimization.pdf\n",
      "Cleaned PDF saved to: data/raw/Genetic_Algorithms_for_Prompt_Optimization_cleaned.pdf\n",
      "Cleaning annotations for: DIVERSE_LLM_Dataset___IEEE_Big_Data.pdf\n",
      "Cleaned PDF saved to: data/raw/DIVERSE_LLM_Dataset___IEEE_Big_Data_cleaned.pdf\n",
      "Cleaning annotations for: Hashtag_Revival.pdf\n",
      "Cleaned PDF saved to: data/raw/Hashtag_Revival_cleaned.pdf\n",
      "Cleaning annotations for: FBI_Recruit_Hire_Final.pdf\n",
      "Cleaned PDF saved to: data/raw/FBI_Recruit_Hire_Final_cleaned.pdf\n",
      "Cleaning annotations for: Benson_MA491_NLP.pdf\n",
      "Cleaned PDF saved to: data/raw/Benson_MA491_NLP_cleaned.pdf\n",
      "Cleaning annotations for: Extreme Cohesion Darknet 20190815.pdf\n",
      "Cleaned PDF saved to: data/raw/Extreme Cohesion Darknet 20190815_cleaned.pdf\n",
      "Cleaning annotations for: Encyclopedia of SNA - R Packages.pdf\n",
      "Cleaned PDF saved to: data/raw/Encyclopedia of SNA - R Packages_cleaned.pdf\n",
      "Cleaning annotations for: RES2D.pdf\n",
      "Cleaned PDF saved to: data/raw/RES2D_cleaned.pdf\n",
      "Cleaning annotations for: ClassifiersCrowdSource.pdf\n",
      "Cleaned PDF saved to: data/raw/ClassifiersCrowdSource_cleaned.pdf\n",
      "Cleaning annotations for: FSS-19_paper_137.pdf\n",
      "Cleaned PDF saved to: data/raw/FSS-19_paper_137_cleaned.pdf\n",
      "Cleaning annotations for: Kidney_Behavioral.pdf\n",
      "Cleaned PDF saved to: data/raw/Kidney_Behavioral_cleaned.pdf\n",
      "Cleaning annotations for: Sim of Decon.pdf\n",
      "Cleaned PDF saved to: data/raw/Sim of Decon_cleaned.pdf\n",
      "Cleaning annotations for: Cross_Platform_Information_Spread_During_the_January_6th_Capitol_Riots.pdf\n",
      "Cleaned PDF saved to: data/raw/Cross_Platform_Information_Spread_During_the_January_6th_Capitol_Riots_cleaned.pdf\n",
      "Cleaning annotations for: Political_Networks_Conference.pdf\n",
      "Cleaned PDF saved to: data/raw/Political_Networks_Conference_cleaned.pdf\n",
      "Cleaning annotations for: On the Science of Networks.pdf\n",
      "Cleaned PDF saved to: data/raw/On the Science of Networks_cleaned.pdf\n",
      "Cleaning annotations for: Simmelian-Gamma-LDA.pdf\n",
      "Cleaned PDF saved to: data/raw/Simmelian-Gamma-LDA_cleaned.pdf\n",
      "Cleaning annotations for: BotBuster___AAAI.pdf\n",
      "Cleaned PDF saved to: data/raw/BotBuster___AAAI_cleaned.pdf\n",
      "Cleaning annotations for: Planning for AI Sustainment A Methodology for Maintenance and Cost Management_V5.pdf\n",
      "Cleaned PDF saved to: data/raw/Planning for AI Sustainment A Methodology for Maintenance and Cost Management_V5_cleaned.pdf\n",
      "Cleaning annotations for: Symbolic Generative AI 20231012.pdf\n",
      "Cleaned PDF saved to: data/raw/Symbolic Generative AI 20231012_cleaned.pdf\n",
      "Cleaning annotations for: 23-US-DHS-001.pdf\n",
      "Cleaned PDF saved to: data/raw/23-US-DHS-001_cleaned.pdf\n",
      "Cleaning annotations for: 2024_ICWSM_Data_Challenge__Post_API_Data_Collection.pdf\n",
      "Cleaned PDF saved to: data/raw/2024_ICWSM_Data_Challenge__Post_API_Data_Collection_cleaned.pdf\n",
      "Cleaning annotations for: Misinformation_Simulation.pdf\n",
      "Cleaned PDF saved to: data/raw/Misinformation_Simulation_cleaned.pdf\n",
      "Cleaning annotations for: Supply Chain Excellence.pdf\n",
      "Cleaned PDF saved to: data/raw/Supply Chain Excellence_cleaned.pdf\n",
      "Cleaning annotations for: 2021_EPJ_MVMCInfoOps.pdf\n",
      "Cleaned PDF saved to: data/raw/2021_EPJ_MVMCInfoOps_cleaned.pdf\n",
      "Cleaning annotations for: Network Simulation Models.pdf\n",
      "Cleaned PDF saved to: data/raw/Network Simulation Models_cleaned.pdf\n",
      "Cleaning annotations for: ALL18.pdf\n",
      "Cleaned PDF saved to: data/raw/ALL18_cleaned.pdf\n",
      "Cleaning annotations for: ONA-in-R.pdf\n",
      "Cleaned PDF saved to: data/raw/ONA-in-R_cleaned.pdf\n",
      "Cleaning annotations for: Helene_and_Milton_ACM.pdf\n",
      "Cleaned PDF saved to: data/raw/Helene_and_Milton_ACM_cleaned.pdf\n",
      "Cleaning annotations for: Unobtrusive Email.pdf\n",
      "Cleaned PDF saved to: data/raw/Unobtrusive Email_cleaned.pdf\n",
      "Cleaning annotations for: docnet.pdf\n",
      "Cleaned PDF saved to: data/raw/docnet_cleaned.pdf\n",
      "Cleaning annotations for: Utility Seeking in Complex Social Systems.pdf\n",
      "Cleaned PDF saved to: data/raw/Utility Seeking in Complex Social Systems_cleaned.pdf\n",
      "Cleaning annotations for: Lessons from Advising in Afghanistan.pdf\n",
      "Cleaned PDF saved to: data/raw/Lessons from Advising in Afghanistan_cleaned.pdf\n",
      "Cleaning annotations for: MOOC 20190828.pdf\n",
      "Cleaned PDF saved to: data/raw/MOOC 20190828_cleaned.pdf\n",
      "Cleaning annotations for: WEIRD.pdf\n",
      "Cleaned PDF saved to: data/raw/WEIRD_cleaned.pdf\n",
      "Cleaning annotations for: IkeNet.pdf\n",
      "Cleaned PDF saved to: data/raw/IkeNet_cleaned.pdf\n",
      "Cleaning annotations for: EmergencyResponseAI.pdf\n",
      "Cleaned PDF saved to: data/raw/EmergencyResponseAI_cleaned.pdf\n",
      "Cleaning annotations for: Quantifying_Information_Advantage.pdf\n",
      "Cleaned PDF saved to: data/raw/Quantifying_Information_Advantage_cleaned.pdf\n",
      "Cleaning annotations for: Confidence_Chaining.pdf\n",
      "Cleaned PDF saved to: data/raw/Confidence_Chaining_cleaned.pdf\n",
      "Cleaning annotations for: RatingsVRankings.pdf\n",
      "Cleaned PDF saved to: data/raw/RatingsVRankings_cleaned.pdf\n",
      "Cleaning annotations for: Analysis_of_Malware_Communities_Using_Multi_Modal_Features.pdf\n",
      "Cleaned PDF saved to: data/raw/Analysis_of_Malware_Communities_Using_Multi_Modal_Features_cleaned.pdf\n",
      "Cleaning annotations for: jfq-110_46-53_Cruickshank.pdf\n",
      "Cleaned PDF saved to: data/raw/jfq-110_46-53_Cruickshank_cleaned.pdf\n",
      "Cleaning annotations for: MLTEing_Models_for_NIER_at_ICSE_2023.pdf\n",
      "Cleaned PDF saved to: data/raw/MLTEing_Models_for_NIER_at_ICSE_2023_cleaned.pdf\n",
      "Cleaning annotations for: SocNetAlQaeda.pdf\n",
      "Cleaned PDF saved to: data/raw/SocNetAlQaeda_cleaned.pdf\n",
      "Cleaning annotations for: Leadership of Data Annotation 20180304v2.pdf\n",
      "Cleaned PDF saved to: data/raw/Leadership of Data Annotation 20180304v2_cleaned.pdf\n",
      "Cleaning annotations for: Parler_Disinformation_Challenge___CMOT_Extended.pdf\n",
      "Cleaned PDF saved to: data/raw/Parler_Disinformation_Challenge___CMOT_Extended_cleaned.pdf\n",
      "Cleaning annotations for: ICWSM_2025_Political_Bias.pdf\n",
      "Cleaned PDF saved to: data/raw/ICWSM_2025_Political_Bias_cleaned.pdf\n",
      "Cleaning annotations for: HIV.pdf\n",
      "Cleaned PDF saved to: data/raw/HIV_cleaned.pdf\n",
      "Cleaning annotations for: Limit Velocity.pdf\n",
      "Cleaned PDF saved to: data/raw/Limit Velocity_cleaned.pdf\n",
      "Cleaning annotations for: Spectral Analysis SNA.pdf\n",
      "Cleaned PDF saved to: data/raw/Spectral Analysis SNA_cleaned.pdf\n",
      "Cleaning annotations for: LLM_Confidence_Metrics.pdf\n",
      "Cleaned PDF saved to: data/raw/LLM_Confidence_Metrics_cleaned.pdf\n",
      "Cleaning annotations for: Food Addiction 20231222 v3.pdf\n",
      "Cleaned PDF saved to: data/raw/Food Addiction 20231222 v3_cleaned.pdf\n",
      "Cleaning annotations for: NBA Performance.pdf\n",
      "Cleaned PDF saved to: data/raw/NBA Performance_cleaned.pdf\n",
      "Cleaning annotations for: MIPB-CDA.pdf\n",
      "Cleaned PDF saved to: data/raw/MIPB-CDA_cleaned.pdf\n",
      "Cleaning annotations for: Vol33Iss1_INSNApdf.pdf\n",
      "Cleaned PDF saved to: data/raw/Vol33Iss1_INSNApdf_cleaned.pdf\n",
      "Cleaning annotations for: NeuroCogInfluence.pdf\n",
      "Cleaned PDF saved to: data/raw/NeuroCogInfluence_cleaned.pdf\n",
      "Cleaning annotations for: Frontiers COVID.pdf\n",
      "Cleaned PDF saved to: data/raw/Frontiers COVID_cleaned.pdf\n",
      "Cleaning annotations for: Evolution_of_Terrorism_PNAS.pdf\n",
      "Cleaned PDF saved to: data/raw/Evolution_of_Terrorism_PNAS_cleaned.pdf\n",
      "Cleaning annotations for: Text Analysis Using Automated Language Translators.pdf\n",
      "Cleaned PDF saved to: data/raw/Text Analysis Using Automated Language Translators_cleaned.pdf\n",
      "Cleaning annotations for: Vulnerable_Code_Detection.pdf\n",
      "Cleaned PDF saved to: data/raw/Vulnerable_Code_Detection_cleaned.pdf\n",
      "Cleaning annotations for: Dormant Bots 20190814.pdf\n",
      "Cleaned PDF saved to: data/raw/Dormant Bots 20190814_cleaned.pdf\n",
      "Cleaning annotations for: NAP Behavioral Sci Intel.pdf\n",
      "Cleaned PDF saved to: data/raw/NAP Behavioral Sci Intel_cleaned.pdf\n",
      "Cleaning annotations for: YouTube-COVID.pdf\n",
      "Cleaned PDF saved to: data/raw/YouTube-COVID_cleaned.pdf\n",
      "Cleaning annotations for: Organizational risk using network analysis.pdf\n",
      "Cleaned PDF saved to: data/raw/Organizational risk using network analysis_cleaned.pdf\n",
      "Cleaning annotations for: k-truss.pdf\n",
      "Cleaned PDF saved to: data/raw/k-truss_cleaned.pdf\n",
      "Cleaning annotations for: Political Party Cohesion.pdf\n",
      "Cleaned PDF saved to: data/raw/Political Party Cohesion_cleaned.pdf\n",
      "Cleaning annotations for: Sailer McCulloh Soc Net and Spatial Config.pdf\n",
      "Cleaned PDF saved to: data/raw/Sailer McCulloh Soc Net and Spatial Config_cleaned.pdf\n",
      "Cleaning annotations for: CUSUM Parameterization.pdf\n",
      "Cleaned PDF saved to: data/raw/CUSUM Parameterization_cleaned.pdf\n",
      "Cleaning annotations for: Multi_view_Clustering_for_Social_Based_Data.pdf\n",
      "Cleaned PDF saved to: data/raw/Multi_view_Clustering_for_Social_Based_Data_cleaned.pdf\n",
      "Cleaning annotations for: ONA-using-igraph.pdf\n",
      "Cleaned PDF saved to: data/raw/ONA-using-igraph_cleaned.pdf\n",
      "Cleaning annotations for: improving-decision-support-for-organ-transplant.pdf\n",
      "Cleaned PDF saved to: data/raw/improving-decision-support-for-organ-transplant_cleaned.pdf\n",
      "Cleaning annotations for: Tweets-to-touchdowns.pdf\n",
      "Cleaned PDF saved to: data/raw/Tweets-to-touchdowns_cleaned.pdf\n",
      "Cleaning annotations for: Social_Det_COVID_Mortality.pdf\n",
      "Cleaned PDF saved to: data/raw/Social_Det_COVID_Mortality_cleaned.pdf\n",
      "Cleaning annotations for: White Paper Brain Gaze.pdf\n",
      "Cleaned PDF saved to: data/raw/White Paper Brain Gaze_cleaned.pdf\n",
      "Cleaning annotations for: McCullohCarleyJOSS.pdf\n",
      "Cleaned PDF saved to: data/raw/McCullohCarleyJOSS_cleaned.pdf\n",
      "Cleaning annotations for: Course Info Security.pdf\n",
      "Cleaned PDF saved to: data/raw/Course Info Security_cleaned.pdf\n",
      "Cleaning annotations for: A_Complex_Network_Approach_to_Find_Latent_Terorrist_Communities.pdf\n",
      "Cleaned PDF saved to: data/raw/A_Complex_Network_Approach_to_Find_Latent_Terorrist_Communities_cleaned.pdf\n",
      "Cleaning annotations for: Knowing the Terrain.pdf\n",
      "Cleaned PDF saved to: data/raw/Knowing the Terrain_cleaned.pdf\n",
      "Cleaning annotations for: Social_Network_Probability_Mechanics.pdf\n",
      "Cleaned PDF saved to: data/raw/Social_Network_Probability_Mechanics_cleaned.pdf\n",
      "Cleaning annotations for: The ABCs of AI-Enabled Intelligence Analysis - War on the Rocks.pdf\n",
      "Cleaned PDF saved to: data/raw/The ABCs of AI-Enabled Intelligence Analysis - War on the Rocks_cleaned.pdf\n",
      "Cleaning annotations for: SocNetChgDet.pdf\n",
      "Cleaned PDF saved to: data/raw/SocNetChgDet_cleaned.pdf\n",
      "Cleaning annotations for: Take_boards.pdf\n",
      "Cleaned PDF saved to: data/raw/Take_boards_cleaned.pdf\n",
      "Cleaning annotations for: Arrow White Paper DExTra.pdf\n",
      "Cleaned PDF saved to: data/raw/Arrow White Paper DExTra_cleaned.pdf\n",
      "Cleaning annotations for: TrainingSetSize.pdf\n",
      "Cleaned PDF saved to: data/raw/TrainingSetSize_cleaned.pdf\n",
      "Cleaning annotations for: CausalOrgInorgContent.pdf\n",
      "Cleaned PDF saved to: data/raw/CausalOrgInorgContent_cleaned.pdf\n",
      "Cleaning annotations for: Overcoming_Social_Media_API_Restrictions__Building_an_Effective_Web_Scraper.pdf\n",
      "Cleaned PDF saved to: data/raw/Overcoming_Social_Media_API_Restrictions__Building_an_Effective_Web_Scraper_cleaned.pdf\n",
      "Cleaning annotations for: LLM_UQ.pdf\n",
      "Cleaned PDF saved to: data/raw/LLM_UQ_cleaned.pdf\n",
      "Cleaning annotations for: Kent2022_Chapter_MicroscopicMarkovChainApproach.pdf\n",
      "Cleaned PDF saved to: data/raw/Kent2022_Chapter_MicroscopicMarkovChainApproach_cleaned.pdf\n",
      "Cleaning annotations for: Acquiring Maintainable AI_Enable Systems_Final.pdf\n",
      "Cleaned PDF saved to: data/raw/Acquiring Maintainable AI_Enable Systems_Final_cleaned.pdf\n",
      "Cleaning annotations for: Characterizing_Communities_of_Hashtag_Usage_on_Twitter_During_the_2020_COVID_19_Pandemic.pdf\n",
      "Cleaned PDF saved to: data/raw/Characterizing_Communities_of_Hashtag_Usage_on_Twitter_During_the_2020_COVID_19_Pandemic_cleaned.pdf\n",
      "Cleaning annotations for: SecurityPrivAIML.pdf\n",
      "Cleaned PDF saved to: data/raw/SecurityPrivAIML_cleaned.pdf\n",
      "Cleaning annotations for: AAAI IAA CV.pdf\n",
      "Cleaned PDF saved to: data/raw/AAAI IAA CV_cleaned.pdf\n",
      "Cleaning annotations for: LSA email.pdf\n",
      "Cleaned PDF saved to: data/raw/LSA email_cleaned.pdf\n",
      "Cleaning annotations for: Clustering_Analysis_of_Website_Usage_on_Twitter_during_the_COVID_19_Pandemic.pdf\n",
      "Cleaned PDF saved to: data/raw/Clustering_Analysis_of_Website_Usage_on_Twitter_during_the_COVID_19_Pandemic_cleaned.pdf\n",
      "Cleaning annotations for: Designed Networks.pdf\n",
      "Cleaned PDF saved to: data/raw/Designed Networks_cleaned.pdf\n",
      "Cleaning annotations for: COVID Bayesian Data Aug.pdf\n",
      "Cleaned PDF saved to: data/raw/COVID Bayesian Data Aug_cleaned.pdf\n",
      "Cleaning annotations for: Cohort_Optimization_Methods_SNAMS_2021_working_draft (4).pdf\n",
      "Cleaned PDF saved to: data/raw/Cohort_Optimization_Methods_SNAMS_2021_working_draft (4)_cleaned.pdf\n",
      "Cleaning annotations for: SM Customer Feedback_FAB_2019_rev3.pdf\n",
      "Cleaned PDF saved to: data/raw/SM Customer Feedback_FAB_2019_rev3_cleaned.pdf\n",
      "Cleaning annotations for: IkekNet1.pdf\n",
      "Cleaned PDF saved to: data/raw/IkekNet1_cleaned.pdf\n",
      "Cleaning annotations for: Social Media Mental Health Final.pdf\n",
      "Cleaned PDF saved to: data/raw/Social Media Mental Health Final_cleaned.pdf\n",
      "Cleaning annotations for: Reforming Sectarian Beliefs.pdf\n",
      "Cleaned PDF saved to: data/raw/Reforming Sectarian Beliefs_cleaned.pdf\n",
      "Cleaning annotations for: Savas.pdf\n",
      "Cleaned PDF saved to: data/raw/Savas_cleaned.pdf\n",
      "Cleaning annotations for: Data_Education__Emerging_Challenges_and_Opportunities.pdf\n",
      "Cleaned PDF saved to: data/raw/Data_Education__Emerging_Challenges_and_Opportunities_cleaned.pdf\n",
      "Cleaning annotations for: Dissertation.pdf\n",
      "Cleaned PDF saved to: data/raw/Dissertation_cleaned.pdf\n",
      "Cleaning annotations for: Leveraging_AI_to_Improve_Viral_Information_Detection_in_Online_Discourse.pdf\n",
      "Cleaned PDF saved to: data/raw/Leveraging_AI_to_Improve_Viral_Information_Detection_in_Online_Discourse_cleaned.pdf\n",
      "Cleaning annotations for: ICWSM___Use_of_Large_Language_Models_for_Stance_Classification.pdf\n",
      "Cleaned PDF saved to: data/raw/ICWSM___Use_of_Large_Language_Models_for_Stance_Classification_cleaned.pdf\n",
      "Cleaning annotations for: NeuroSynchrony.pdf\n",
      "Cleaned PDF saved to: data/raw/NeuroSynchrony_cleaned.pdf\n",
      "Cleaning annotations for: Multi_Agent_Systems_for_Frame_Detection.pdf\n",
      "Cleaned PDF saved to: data/raw/Multi_Agent_Systems_for_Frame_Detection_cleaned.pdf\n",
      "Cleaning annotations for: Review of R Packages_20161026.pdf\n",
      "Cleaned PDF saved to: data/raw/Review of R Packages_20161026_cleaned.pdf\n",
      "Cleaning annotations for: Lead-Azide.pdf\n",
      "Cleaned PDF saved to: data/raw/Lead-Azide_cleaned.pdf\n",
      "Cleaning annotations for: LongNetViewerORA.pdf\n",
      "Cleaned PDF saved to: data/raw/LongNetViewerORA_cleaned.pdf\n",
      "Annotation removal log written to: annotation_log.json\n",
      "All research PDFs cleaned and saved in data/raw/\n"
     ]
    }
   ],
   "source": [
    "# PDF Cleaning Step: Remove non-visual annotations (comments, links, form fields)\n",
    "# Keeps images, diagrams, and visible callouts intact\n",
    "\n",
    "# Import required libraries for core functionality\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import json  # <-- NEW: Required to write annotation logs\n",
    "\n",
    "# Create a global dictionary to store removed annotations\n",
    "annotation_log = {}  # <-- NEW: Accumulates logs of all removed annotations\n",
    "\n",
    "# Initial standardization step to remove annotations for parsing\n",
    "def clean_pdf_annotations(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Strips non-visual annotations (comments, form fields, links) from a PDF\n",
    "    while preserving visible images and diagrams.\n",
    "    Also logs removed annotations to a global dictionary.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(input_path)\n",
    "    removed_annots = []  # <-- NEW: Stores removed annotations for this PDF\n",
    "\n",
    "    for page in doc:\n",
    "        # Iterate over all annotations (not images)\n",
    "        annot = page.first_annot\n",
    "        while annot:\n",
    "            next_annot = annot.next  # store reference to next annotation\n",
    "            \n",
    "            # Try to extract meaningful annotation content\n",
    "            try:\n",
    "                annot_info = annot.info  # Dictionary of annotation metadata\n",
    "                content = annot_info.get(\"content\", \"\").strip()\n",
    "                subtype = annot_info.get(\"subtype\", \"\").strip()\n",
    "                if content:\n",
    "                    removed_annots.append(f\"{subtype}: {content}\")\n",
    "                else:\n",
    "                    removed_annots.append(f\"{subtype}: [no content]\")\n",
    "            except Exception as e:\n",
    "                # Fallback if annotation metadata is inaccessible\n",
    "                removed_annots.append(\"Unknown annotation (could not extract content)\")\n",
    "\n",
    "            # Remove annotation object (highlights, comments, links)\n",
    "            page.delete_annot(annot)\n",
    "            annot = next_annot\n",
    "\n",
    "    # Save cleaned PDF\n",
    "    doc.save(output_path, garbage=4, deflate=True)\n",
    "    doc.close()\n",
    "\n",
    "    # Add entry to annotation log using the input filename as key\n",
    "    annotation_log[os.path.basename(input_path)] = removed_annots  # <-- NEW: Log entries keyed by file\n",
    "\n",
    "# Clean NOFO file\n",
    "input_pdf = \"../data/NOFO.pdf\"\n",
    "cleaned_pdf = \"../data/NOFO_cleaned.pdf\"\n",
    "clean_pdf_annotations(input_pdf, cleaned_pdf)\n",
    "print(f\"Cleaned PDF saved to: {cleaned_pdf}\")\n",
    "\n",
    "# Get de-annotated NOFO doc content using PyPDFLoader for evaluation step\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "pdf_file = \"../data/NOFO_cleaned.pdf\"\n",
    "pdf_loader = PyPDFLoader(pdf_file)\n",
    "NOFO_pdf = pdf_loader.load()\n",
    "\n",
    "# Prepare output folder for de-annotated research papers\n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "\n",
    "# Set variables for de-annotating the research paper PDF collection \n",
    "source_dir = \"../content\"\n",
    "output_dir = \"data/raw\"\n",
    "\n",
    "# Loop through content folder, de-annotate each PDF, and save to a 'clean' output directory\n",
    "for file_name in os.listdir(source_dir):\n",
    "    if file_name.lower().endswith(\".pdf\"):\n",
    "        input_pdf = os.path.join(source_dir, file_name)\n",
    "        cleaned_pdf = os.path.join(output_dir, file_name.replace(\".pdf\", \"_cleaned.pdf\"))\n",
    "        print(f\"Cleaning annotations for: {file_name}\")\n",
    "        clean_pdf_annotations(input_pdf, cleaned_pdf)\n",
    "        print(f\"Cleaned PDF saved to: {cleaned_pdf}\")\n",
    "\n",
    "# Write annotation log to disk after all PDFs are processed\n",
    "log_path = \"annotation_log.json\"  # <-- NEW: File to store the annotation log\n",
    "with open(log_path, \"w\", encoding=\"utf-8\") as log_file:\n",
    "    json.dump(annotation_log, log_file, indent=2, ensure_ascii=False)  # <-- NEW: Write log to file\n",
    "print(f\"Annotation removal log written to: {log_path}\")  # <-- NEW: Confirm log creation\n",
    "\n",
    "print(\"All research PDFs cleaned and saved in data/raw/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract, Clean, and Chunk Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Function to split cleaned text into 3000-token chunks with overlap for RAG\n",
    "# ---------------------------------------------------------------\n",
    "# This function breaks long text into overlapping token-based chunks for use in\n",
    "# Retrieval-Augmented Generation (RAG) pipelines. Overlapping chunks help\n",
    "# preserve context continuity across boundaries, improving answer quality.\n",
    "\n",
    "import tiktoken  # OpenAI tokenizer library for counting and managing tokens\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Load tokenizer for the target model\n",
    "# ---------------------------------------------------------------\n",
    "# `tiktoken` provides tokenization rules tailored to specific OpenAI models.\n",
    "# Here we select the encoding used by gpt-4o-mini to ensure our token counting\n",
    "# aligns with how the model actually interprets input.\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Define chunking function\n",
    "# ---------------------------------------------------------------\n",
    "# Inputs:\n",
    "# - text: full string to be split into chunks\n",
    "# - chunk_size: max number of tokens per chunk (default 3000)\n",
    "# - overlap: number of tokens to repeat from the previous chunk (default 200)\n",
    "# This overlap preserves some context from earlier chunks in each new chunk.\n",
    "\n",
    "def chunk_text(text, chunk_size=3000, overlap=200):\n",
    "    # Convert text into a list of token IDs using the tokenizer\n",
    "    tokens = encoding.encode(text)\n",
    "\n",
    "    # Initialize an empty list to store the final chunks\n",
    "    chunks = []\n",
    "\n",
    "    # Step through the token list in increments of (chunk_size - overlap)\n",
    "    # This ensures that each new chunk shares `overlap` tokens with the previous one\n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        # Slice the token list to get a window of `chunk_size` tokens\n",
    "        chunk_tokens = tokens[i:i+chunk_size]\n",
    "\n",
    "        # Decode the token slice back into text and add it to the list of chunks\n",
    "        chunks.append(encoding.decode(chunk_tokens))\n",
    "\n",
    "    # Return the full list of overlapping text chunks\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Function to clean extracted text by:\n",
    "# - removing headers/footers\n",
    "# - removing noise\n",
    "# - fixing multi-column layout issues\n",
    "# ---------------------------------------------------------------\n",
    "# This function is useful for preprocessing text extracted from PDFs\n",
    "# (e.g., via OCR or PDF parsers), which often contain artifacts such as\n",
    "# page numbers, repeating headers/footers, hyphenated line breaks,\n",
    "# and broken column layouts.\n",
    "\n",
    "import re  # Regular expressions for pattern matching and substitution\n",
    "\n",
    "def clean_extracted_text(text):\n",
    "    \"\"\"Remove noise (page numbers, headers, footers), merge hyphenated words,\n",
    "    and flatten potential two-column layouts.\"\"\"\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # 1. Remove page numbering and common artifacts\n",
    "    # ---------------------------------------------------------------\n",
    "    # These patterns often appear in academic papers, reports, and government documents.\n",
    "    # Removing them improves the quality of downstream embedding and summarization.\n",
    "\n",
    "    text = re.sub(r'\\bPage \\d+\\b', '', text, flags=re.IGNORECASE)  # Remove 'Page X'\n",
    "    text = re.sub(r'\\d+ of \\d+', '', text, flags=re.IGNORECASE)    # Remove 'X of Y' style page counts\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # 2. Identify and remove repeating headers/footers\n",
    "    # ---------------------------------------------------------------\n",
    "    # Strategy: count how many times each line occurs.\n",
    "    # Merge two-column text by pairing lines\n",
    "    merged_lines = []\n",
    "    lines = text.split('\\n')\n",
    "    for i in range(0, len(lines), 2):\n",
    "        if i+1 < len(lines):\n",
    "            merged_lines.append(lines[i] + \" \" + lines[i+1])\n",
    "        else:\n",
    "            merged_lines.append(lines[i])\n",
    "    return \"\\n\".join(merged_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Function to extract, clean, and chunk research paper PDFs\n",
    "# ---------------------------------------------------------------\n",
    "# This function performs a full preprocessing pipeline for PDF documents,\n",
    "# including text extraction (via PyPDF), cleaning (removing headers/footers, noise),\n",
    "# and token-based chunking for use in downstream RAG pipelines.\n",
    "\n",
    "from pypdf import PdfReader  # PyPDF is used for reading PDF documents and extracting text\n",
    "\n",
    "# Additional imports for patching\n",
    "from pdf2image import convert_from_path  # Convert PDF pages to images\n",
    "import pytesseract  # OCR engine to extract text from images\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def process_pdf_multistage(file_path):\n",
    "    # Initialize an empty string to collect the full text from the PDF\n",
    "    content = \"\"\n",
    "\n",
    "    # Extract filename for metadata\n",
    "    filename = os.path.basename(file_path)\n",
    "    author = None\n",
    "    creation_date = None\n",
    "    num_pages = None\n",
    "\n",
    "    try:\n",
    "        # ---------------------------------------------------------------\n",
    "        # 1. Attempt to load and parse the PDF\n",
    "        # ---------------------------------------------------------------\n",
    "        reader = PdfReader(file_path)  # Create a PdfReader object from the file path\n",
    "\n",
    "        # Get basic document metadata (if available)\n",
    "        meta = reader.metadata or {}\n",
    "        author = meta.get('/Author', None)\n",
    "\n",
    "        # Iterate through each page in the PDF\n",
    "        for page in reader.pages:\n",
    "            # Extract text from the page; if extraction fails or returns None, use an empty string\n",
    "            page_text = page.extract_text() or \"\"\n",
    "\n",
    "            # Append the page's text to the full document content\n",
    "            content += page_text\n",
    "\n",
    "    except Exception as e:\n",
    "        # ---------------------------------------------------------------\n",
    "        # 2. Handle extraction failures gracefully\n",
    "        # ---------------------------------------------------------------\n",
    "        # If any exception is raised during PDF reading or parsing,\n",
    "        # log the error and allow the function to continue (returning empty chunks).\n",
    "        print(f\"PyPDF extraction failed: {e}\")\n",
    "        print(\"Falling back to OCR...\")\n",
    "\n",
    "        try:\n",
    "            # Convert PDF pages to images using pdf2image\n",
    "            images = convert_from_path(file_path)\n",
    "            ocr_text_list = []\n",
    "\n",
    "            for i, img in enumerate(images):\n",
    "                # Run OCR on each image page using pytesseract\n",
    "                page_text = pytesseract.image_to_string(img)\n",
    "                ocr_text_list.append(page_text)\n",
    "\n",
    "            # Combine all OCR'd page text into one document\n",
    "            content = \"\\n\".join(ocr_text_list)\n",
    "\n",
    "        except Exception as ocr_error:\n",
    "            print(f\"OCR fallback also failed: {ocr_error}\")\n",
    "            return []\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # 3. Clean the raw extracted text\n",
    "    # ---------------------------------------------------------------\n",
    "    # Use a dedicated cleaning function to:\n",
    "    # - Remove headers, footers, and page numbers\n",
    "    # - Merge hyphenated line breaks\n",
    "    # - Flatten multi-column layouts\n",
    "    # This improves the quality of embeddings and downstream retrieval.\n",
    "    cleaned_text = clean_extracted_text(content)\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # 4. Chunk the cleaned text into token-bounded segments\n",
    "    # ---------------------------------------------------------------\n",
    "    # Break the cleaned document into overlapping token chunks (e.g., 3000 tokens with 200-token overlap),\n",
    "    # ensuring context continuity across chunks. This is critical for performance in RAG.\n",
    "    chunks = chunk_text(cleaned_text)\n",
    "\n",
    "    def safe_str(obj):\n",
    "        \"\"\"\n",
    "        Convert a potentially non-serializable object (e.g., PyPDF's IndirectObject)\n",
    "        into a JSON-compatible Python string or None.\n",
    "\n",
    "        This is especially useful when working with metadata fields extracted from PDFs,\n",
    "        where objects may be wrapped in non-primitive types (like PyPDF2.generic.IndirectObject),\n",
    "        which the `json` module cannot serialize directly.\n",
    "\n",
    "        Returns:\n",
    "            - `str(obj)` if the object can be stringified without error\n",
    "            - `None` if string conversion fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Attempt to cast the object to a string (e.g., IndirectObject → str)\n",
    "            # This is usually sufficient for basic metadata like author, title, date, etc.\n",
    "            return str(obj)\n",
    "        \n",
    "        except Exception:\n",
    "            # If casting to string fails (e.g., object is not readable or triggers an exception),\n",
    "            # return None instead, making the output JSON-safe.\n",
    "            return None\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # 4.1 Attach file-level metadata to each chunk\n",
    "    # ---------------------------------------------------------------\n",
    "    # This metadata can help with filtering, attribution, and retrieval analysis.\n",
    "    chunks_with_metadata = [\n",
    "        {\n",
    "            \"text\": chunk,\n",
    "            \"metadata\": {\n",
    "                \"source_file\": safe_str(filename),\n",
    "                \"author\": safe_str(author),\n",
    "                \"creation_date\": safe_str(creation_date),\n",
    "                \"num_pages\": safe_str(num_pages),\n",
    "                \"chunk_index\": i\n",
    "            }\n",
    "        }\n",
    "        for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # 5. Return the processed chunks\n",
    "    # ---------------------------------------------------------------\n",
    "    # The final output is a list of text chunks, ready for embedding, storage, or retrieval.\n",
    "    return chunks_with_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: data/raw/AAAI IAA CV_cleaned.pdf\n",
      "Processing: data/raw/Sim of Decon_cleaned.pdf\n",
      "Processing: data/raw/BotBuster___AAAI_cleaned.pdf\n",
      "Processing: data/raw/Political_Networks_Conference_cleaned.pdf\n",
      "Processing: data/raw/EmergencyResponseAI_cleaned.pdf\n",
      "Processing: data/raw/FSS-19_paper_137_cleaned.pdf\n",
      "Processing: data/raw/DIVERSE_LLM_Dataset___IEEE_Big_Data_cleaned.pdf\n",
      "Processing: data/raw/Clustering_Analysis_of_Website_Usage_on_Twitter_during_the_COVID_19_Pandemic_cleaned.pdf\n",
      "Processing: data/raw/Cohort_Optimization_Methods_SNAMS_2021_working_draft (4)_cleaned.pdf\n",
      "Processing: data/raw/Lead-Azide_cleaned.pdf\n",
      "Processing: data/raw/Knowing the Terrain_cleaned.pdf\n",
      "Processing: data/raw/Leadership of Data Annotation 20180304v2_cleaned.pdf\n",
      "Processing: data/raw/A_Complex_Network_Approach_to_Find_Latent_Terorrist_Communities_cleaned.pdf\n",
      "Processing: data/raw/Designed Networks_cleaned.pdf\n",
      "Processing: data/raw/Organizational risk using network analysis_cleaned.pdf\n",
      "Processing: data/raw/LongNetViewerORA_cleaned.pdf\n",
      "Processing: data/raw/Unobtrusive Email_cleaned.pdf\n",
      "Processing: data/raw/SM Customer Feedback_FAB_2019_rev3_cleaned.pdf\n",
      "Processing: data/raw/Benson_MA491_NLP_cleaned.pdf\n",
      "Processing: data/raw/SecurityPrivAIML_cleaned.pdf\n",
      "Processing: data/raw/SocNetAlQaeda_cleaned.pdf\n",
      "Processing: data/raw/COVID Bayesian Data Aug_cleaned.pdf\n",
      "Processing: data/raw/Savas_cleaned.pdf\n",
      "Processing: data/raw/Lessons from Advising in Afghanistan_cleaned.pdf\n",
      "Processing: data/raw/Confidence_Chaining_cleaned.pdf\n",
      "Processing: data/raw/Limit Velocity_cleaned.pdf\n",
      "Processing: data/raw/Text Analysis Using Automated Language Translators_cleaned.pdf\n",
      "Processing: data/raw/Multi_Agent_Systems_for_Frame_Detection_cleaned.pdf\n",
      "Processing: data/raw/IkekNet1_cleaned.pdf\n",
      "Processing: data/raw/Dissertation_cleaned.pdf\n",
      "Processing: data/raw/LLM_UQ_cleaned.pdf\n",
      "Processing: data/raw/Parler_Disinformation_Challenge___CMOT_Extended_cleaned.pdf\n",
      "Processing: data/raw/LSA email_cleaned.pdf\n",
      "Processing: data/raw/Political Party Cohesion_cleaned.pdf\n",
      "Processing: data/raw/Spectral Analysis SNA_cleaned.pdf\n",
      "Processing: data/raw/NeuroCogInfluence_cleaned.pdf\n",
      "Processing: data/raw/Simmelian-Gamma-LDA_cleaned.pdf\n",
      "Processing: data/raw/MIPB-CDA_cleaned.pdf\n",
      "Processing: data/raw/CausalOrgInorgContent_cleaned.pdf\n",
      "Processing: data/raw/Genetic_Algorithms_for_Prompt_Optimization_cleaned.pdf\n",
      "Processing: data/raw/ALL18_cleaned.pdf\n",
      "Processing: data/raw/LLM_Confidence_Metrics_cleaned.pdf\n",
      "Processing: data/raw/FBI_Recruit_Hire_Final_cleaned.pdf\n",
      "Processing: data/raw/ICWSM_2025_Political_Bias_cleaned.pdf\n",
      "Processing: data/raw/Review of R Packages_20161026_cleaned.pdf\n",
      "Processing: data/raw/Evolution_of_Terrorism_PNAS_cleaned.pdf\n",
      "Processing: data/raw/Planning for AI Sustainment A Methodology for Maintenance and Cost Management_V5_cleaned.pdf\n",
      "Processing: data/raw/Social Media Mental Health Final_cleaned.pdf\n",
      "Processing: data/raw/Cross_Platform_Information_Spread_During_the_January_6th_Capitol_Riots_cleaned.pdf\n",
      "Processing: data/raw/Multi_view_Clustering_for_Social_Based_Data_cleaned.pdf\n",
      "Processing: data/raw/Extreme Cohesion Darknet 20190815_cleaned.pdf\n",
      "Processing: data/raw/Social_Det_COVID_Mortality_cleaned.pdf\n",
      "Processing: data/raw/White Paper Brain Gaze_cleaned.pdf\n",
      "Processing: data/raw/Encyclopedia of SNA - R Packages_cleaned.pdf\n",
      "Processing: data/raw/CUSUM Parameterization_cleaned.pdf\n",
      "Processing: data/raw/Tweets-to-touchdowns_cleaned.pdf\n",
      "Processing: data/raw/SocNetChgDet_cleaned.pdf\n",
      "Processing: data/raw/23-US-DHS-001_cleaned.pdf\n",
      "Processing: data/raw/Analysis_of_Malware_Communities_Using_Multi_Modal_Features_cleaned.pdf\n",
      "Processing: data/raw/Acquiring Maintainable AI_Enable Systems_Final_cleaned.pdf\n",
      "Processing: data/raw/Food Addiction 20231222 v3_cleaned.pdf\n",
      "Processing: data/raw/HIV_cleaned.pdf\n",
      "Processing: data/raw/TrainingSetSize_cleaned.pdf\n",
      "Processing: data/raw/WEIRD_cleaned.pdf\n",
      "Processing: data/raw/Dormant Bots 20190814_cleaned.pdf\n",
      "Processing: data/raw/The ABCs of AI-Enabled Intelligence Analysis - War on the Rocks_cleaned.pdf\n",
      "Processing: data/raw/improving-decision-support-for-organ-transplant_cleaned.pdf\n",
      "Processing: data/raw/Leveraging_AI_to_Improve_Viral_Information_Detection_in_Online_Discourse_cleaned.pdf\n",
      "Processing: data/raw/Vulnerable_Code_Detection_cleaned.pdf\n",
      "Processing: data/raw/Overcoming_Social_Media_API_Restrictions__Building_an_Effective_Web_Scraper_cleaned.pdf\n",
      "Processing: data/raw/Supply Chain Excellence_cleaned.pdf\n",
      "Processing: data/raw/Reforming Sectarian Beliefs_cleaned.pdf\n",
      "Processing: data/raw/ICWSM___Use_of_Large_Language_Models_for_Stance_Classification_cleaned.pdf\n",
      "Processing: data/raw/Course Info Security_cleaned.pdf\n",
      "Processing: data/raw/Take_boards_cleaned.pdf\n",
      "Processing: data/raw/Hashtag_Revival_cleaned.pdf\n",
      "Processing: data/raw/Misinformation_Simulation_cleaned.pdf\n",
      "Processing: data/raw/Network Simulation Models_cleaned.pdf\n",
      "Processing: data/raw/cycon-final-draft_cleaned.pdf\n",
      "Processing: data/raw/NAP Behavioral Sci Intel_cleaned.pdf\n",
      "Processing: data/raw/Utility Seeking in Complex Social Systems_cleaned.pdf\n",
      "Processing: data/raw/k-truss_cleaned.pdf\n",
      "Processing: data/raw/Arrow White Paper DExTra_cleaned.pdf\n",
      "Processing: data/raw/Helene_and_Milton_ACM_cleaned.pdf\n",
      "Processing: data/raw/Data_Education__Emerging_Challenges_and_Opportunities_cleaned.pdf\n",
      "Processing: data/raw/RatingsVRankings_cleaned.pdf\n",
      "Processing: data/raw/RES2D_cleaned.pdf\n",
      "Processing: data/raw/McCullohCarleyJOSS_cleaned.pdf\n",
      "Processing: data/raw/ONA-using-igraph_cleaned.pdf\n",
      "Processing: data/raw/Vol33Iss1_INSNApdf_cleaned.pdf\n",
      "Processing: data/raw/Kidney_Behavioral_cleaned.pdf\n",
      "Processing: data/raw/ClassifiersCrowdSource_cleaned.pdf\n",
      "Processing: data/raw/MLTEing_Models_for_NIER_at_ICSE_2023_cleaned.pdf\n",
      "Processing: data/raw/Quantifying_Information_Advantage_cleaned.pdf\n",
      "Processing: data/raw/NeuroSynchrony_cleaned.pdf\n",
      "Processing: data/raw/Frontiers COVID_cleaned.pdf\n",
      "Processing: data/raw/Kent2022_Chapter_MicroscopicMarkovChainApproach_cleaned.pdf\n",
      "Processing: data/raw/jfq-110_46-53_Cruickshank_cleaned.pdf\n",
      "Processing: data/raw/Sailer McCulloh Soc Net and Spatial Config_cleaned.pdf\n",
      "Processing: data/raw/MOOC 20190828_cleaned.pdf\n",
      "Processing: data/raw/2021_EPJ_MVMCInfoOps_cleaned.pdf\n",
      "Processing: data/raw/ONA-in-R_cleaned.pdf\n",
      "Processing: data/raw/docnet_cleaned.pdf\n",
      "Processing: data/raw/Characterizing_Communities_of_Hashtag_Usage_on_Twitter_During_the_2020_COVID_19_Pandemic_cleaned.pdf\n",
      "Processing: data/raw/Symbolic Generative AI 20231012_cleaned.pdf\n",
      "Processing: data/raw/NBA Performance_cleaned.pdf\n",
      "Processing: data/raw/Chat GPT Bias final w copyright_cleaned.pdf\n",
      "Processing: data/raw/IkeNet_cleaned.pdf\n",
      "Processing: data/raw/On the Science of Networks_cleaned.pdf\n",
      "Processing: data/raw/Social_Network_Probability_Mechanics_cleaned.pdf\n",
      "Processing: data/raw/2024_ICWSM_Data_Challenge__Post_API_Data_Collection_cleaned.pdf\n",
      "Processing: data/raw/YouTube-COVID_cleaned.pdf\n",
      "Saved cleaned + chunked text for 112 PDFs to data/cleaned_chunked_papers.json\n"
     ]
    }
   ],
   "source": [
    "# Extract, clean, chunk, and store raw chunks for all research paper PDFs\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1. Import necessary libraries\n",
    "# ---------------------------------------------------------------\n",
    "import os              # Used for file path manipulation and directory handling\n",
    "import json            # Used to save the final result as a JSON file\n",
    "from glob import glob  # Used to match all PDF files in a directory\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2. Set input/output paths\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# Folder containing raw research paper PDFs (to be processed)\n",
    "pdf_folder = \"data/raw\"\n",
    "\n",
    "# Output file to save cleaned + chunked results\n",
    "output_json_path = \"data/cleaned_chunked_papers.json\"\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3. Initialize storage for processed results\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# This list will store the result for each paper.\n",
    "# Each element is a dictionary with:\n",
    "#   - 'id': PDF filename\n",
    "#   - 'chunks': list of cleaned and tokenized text chunks from that PDF\n",
    "all_chunks = []\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4. Loop through all PDF files in the target folder\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# `glob` finds all .pdf files in the specified folder\n",
    "for pdf_path in glob(os.path.join(pdf_folder, \"*.pdf\")):\n",
    "    doc_name = os.path.basename(pdf_path)  # Extract just the filename (used as a unique ID)\n",
    "    print(f\"Processing: {pdf_path}\")       # Log the file being processed\n",
    "    \n",
    "    try:\n",
    "        # ---------------------------------------------------------------\n",
    "        # Attempt to extract, clean, and chunk the PDF content\n",
    "        # ---------------------------------------------------------------\n",
    "        # `process_pdf_multistage()` is your custom pipeline that:\n",
    "        #   1. Extracts text using PyPDF (and optionally OCR if needed)\n",
    "        #   2. Cleans the text (removes noise, merges hyphenated lines, etc.)\n",
    "        #   3. Chunks the cleaned text into token-bounded segments\n",
    "        chunks = process_pdf_multistage(pdf_path)\n",
    "\n",
    "        # ---------------------------------------------------------------\n",
    "        # Append the processed result to the `all_chunks` list\n",
    "        # ---------------------------------------------------------------\n",
    "        # Each record contains the filename (as ID) and a list of chunks\n",
    "        all_chunks.append({\n",
    "            \"id\": doc_name,\n",
    "            \"chunks\": chunks\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        # ---------------------------------------------------------------\n",
    "        # If anything goes wrong during processing, catch the error\n",
    "        # ---------------------------------------------------------------\n",
    "        print(f\"Error processing {pdf_path}: {e}\")  # Log the error for debugging\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 5. Save all processed results to a JSON file\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# Write the list of all processed documents to a single JSON file\n",
    "# - `indent=2` for human-readable formatting\n",
    "# - `ensure_ascii=False` allows Unicode characters (like symbols or accents)\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Final confirmation message\n",
    "print(f\"Saved cleaned + chunked text for {len(all_chunks)} PDFs to {output_json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-compute and Store Embeddings for RAG-enabled Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sqlite version: 3.34.1\n",
      "Transformers version: 4.37.2\n",
      "Torch version: 2.3.0+cpu\n",
      "Torch path: /workspaces/genai_capstone/.venv/lib/python3.10/site-packages/torch/__init__.py\n",
      "Transformers version: 4.37.2\n",
      "Transformers path: /workspaces/genai_capstone/.venv/lib/python3.10/site-packages/transformers/__init__.py\n",
      "uint64 exists: True\n",
      "safetensors version: 0.6.1\n",
      "safetensors.torch available\n"
     ]
    }
   ],
   "source": [
    "# Check torch version\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_AUDIO\"] = \"1\"\n",
    "\n",
    "try:\n",
    "    import safetensors\n",
    "    import safetensors.torch\n",
    "    import torch\n",
    "    import transformers\n",
    "    import sqlite3\n",
    "    print(\"Sqlite version:\", sqlite3.sqlite_version)\n",
    "    print(\"Transformers version:\", transformers.__version__)\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "    print(\"Torch path:\", torch.__file__)\n",
    "    print(\"Transformers version:\", transformers.__version__)\n",
    "    print(\"Transformers path:\", transformers.__file__)\n",
    "    print(\"uint64 exists:\", hasattr(torch, \"uint64\"))\n",
    "    print(\"safetensors version:\", safetensors.__version__)\n",
    "    print(\"safetensors.torch available\")\n",
    "except Exception as e:\n",
    "    print(\"Transformers import failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS index saved to: data/faiss_index (files: index.faiss, index.pkl)\n",
      "✅ Document list saved to data/vectorstore_docs.pkl for future vectorstore reloading.\n",
      "✅ FAISS vectorstore created in memory (Chroma disabled due to sqlite version).\n",
      "📁 Saved FAISS files: ['index.faiss', 'index.pkl']\n"
     ]
    }
   ],
   "source": [
    "# Import HuggingFace embedding wrapper from LangChain\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1. Initialize embedding model\n",
    "# ---------------------------------------------------------------\n",
    "# This wraps a HuggingFace model (MiniLM) so it can be used with LangChain.\n",
    "# MiniLM is a lightweight transformer model that produces sentence embeddings.\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2. Set the persistent storage directory for Chroma\n",
    "# ---------------------------------------------------------------\n",
    "# This is where Chroma will store the vector index on disk.\n",
    "# The directory is placed *outside the repo* to avoid accidentally committing large files to Git.\n",
    "persist_dir = \"/workspaces/chroma_storage/chroma_embeddings\"\n",
    "\n",
    "# Create the directory if it doesn’t exist (idempotent)\n",
    "os.makedirs(persist_dir, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3. Prepare LangChain Document objects\n",
    "# ---------------------------------------------------------------\n",
    "# LangChain expects documents in a specific format: each one must be a Document object\n",
    "# containing `page_content` (the raw text) and optional `metadata`.\n",
    "# Here, we pair each text chunk with its corresponding paper ID.\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# ⬇️ PATCHED SECTION: Load and flatten all_chunks + chunk_to_paper from saved JSON\n",
    "# ---------------------------------------------------------------\n",
    "import json\n",
    "\n",
    "# Path to the preprocessed chunked papers JSON file\n",
    "chunked_data_path = \"data/cleaned_chunked_papers.json\"\n",
    "\n",
    "# Load the saved chunked results from disk\n",
    "with open(chunked_data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    saved_papers = json.load(f)\n",
    "\n",
    "# Initialize flat lists to store chunk text and paper ID metadata\n",
    "all_chunks_flat = []       # Each entry will be a string (chunk text)\n",
    "chunk_to_paper = []        # Each entry will be the paper ID (filename)\n",
    "\n",
    "# Flatten all chunks from all papers into a single list\n",
    "for paper in saved_papers:\n",
    "    paper_id = paper[\"id\"]\n",
    "    for chunk in paper[\"chunks\"]:\n",
    "        all_chunks_flat.append(chunk[\"text\"])        # Extract chunk text\n",
    "        chunk_to_paper.append(paper_id)              # Track source paper ID\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# ✅ UPDATED: Build Document objects from flattened chunks\n",
    "# ---------------------------------------------------------------\n",
    "docs = [\n",
    "    Document(page_content=chunk, metadata={\"paper_id\": paper_id})\n",
    "    for chunk, paper_id in zip(all_chunks_flat, chunk_to_paper)\n",
    "]\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4. Create FAISS vectorstore (patched to replace Chroma)\n",
    "# ---------------------------------------------------------------\n",
    "# 🔁 PREVIOUSLY USED: Chroma (commented out due to unsupported sqlite3 version)\n",
    "# vectorstore = Chroma.from_documents(\n",
    "#     documents=docs,\n",
    "#     embedding=embedding_model,\n",
    "#     collection_name=\"research_chunks\",\n",
    "#     persist_directory=persist_dir,\n",
    "#     client_settings={\"is_persistent\": True}\n",
    "# )\n",
    "\n",
    "# ✅ PATCHED: Use FAISS instead of Chroma\n",
    "# ---------------------------------------------------------------\n",
    "# WHY FAISS?\n",
    "# - FAISS (Facebook AI Similarity Search) is a fast and widely used vector indexing library\n",
    "# - FAISS does NOT rely on sqlite3 or any external DB, making it ideal for CPU-only or restricted environments\n",
    "# - It supports fast nearest-neighbor searches in memory\n",
    "# ---------------------------------------------------------------\n",
    "# PROS:\n",
    "# - No sqlite dependency, cross-platform compatible\n",
    "# - Fast, well-tested, and LangChain compatible\n",
    "# - Simple and lightweight\n",
    "# CONS:\n",
    "# - Does not include built-in persistent metadata store like Chroma\n",
    "# - If you want disk persistence, you must manually serialize the FAISS index\n",
    "# ---------------------------------------------------------------\n",
    "from langchain_community.vectorstores import FAISS  # ✅ Import FAISS from LangChain\n",
    "\n",
    "# Build FAISS vectorstore from documents and embedding model (lives in memory initially)\n",
    "vectorstore = FAISS.from_documents(docs, embedding_model)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# ✅ NEW: Persist the FAISS index to disk for later reloading\n",
    "# ---------------------------------------------------------------\n",
    "# Unlike Chroma (which persists automatically when `persist_directory` is set),\n",
    "# FAISS requires an explicit save call. This ensures that a later cell can\n",
    "# successfully call `FAISS.load_local(...)` without recomputing embeddings.\n",
    "#\n",
    "# What gets written:\n",
    "#   - <save_dir>/index.faiss : the raw FAISS vector index (binary)\n",
    "#   - <save_dir>/index.pkl   : LangChain docstore + metadata (pickled)\n",
    "#\n",
    "# Notes:\n",
    "# - The directory is created if it doesn't exist.\n",
    "# - Use the SAME `save_dir` when calling `FAISS.load_local(...)` later.\n",
    "# - If you change embedding models between save/load, retrieval quality will degrade.\n",
    "faiss_index_dir = \"data/faiss_index\"\n",
    "\n",
    "# Ensure the save directory exists (idempotent)\n",
    "os.makedirs(faiss_index_dir, exist_ok=True)\n",
    "\n",
    "# Save the FAISS index + docstore metadata to disk\n",
    "vectorstore.save_local(faiss_index_dir)\n",
    "\n",
    "print(f\"✅ FAISS index saved to: {faiss_index_dir} (files: index.faiss, index.pkl)\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# ✅ NEW STEP: Save flattened document list for later rehydration\n",
    "# ---------------------------------------------------------------\n",
    "# Even though FAISS index can be persisted, we save the LangChain documents list\n",
    "# so we can rebuild the FAISS vectorstore later if needed.\n",
    "import pickle\n",
    "\n",
    "vectorstore_doc_path = \"data/vectorstore_docs.pkl\"\n",
    "\n",
    "# Serialize the docs list to disk\n",
    "with open(vectorstore_doc_path, \"wb\") as f:\n",
    "    pickle.dump(docs, f)\n",
    "\n",
    "print(f\"✅ Document list saved to {vectorstore_doc_path} for future vectorstore reloading.\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 5. Confirm FAISS index object was created\n",
    "# ---------------------------------------------------------------\n",
    "# Unlike Chroma, FAISS doesn't persist by default. We can inspect the index manually if needed.\n",
    "# You can optionally save the FAISS index to disk using:\n",
    "# vectorstore.save_local(\"data/faiss_index\")\n",
    "print(\"✅ FAISS vectorstore created in memory (Chroma disabled due to sqlite version).\")\n",
    "\n",
    "# (Optional) Tiny sanity check: list the saved files so the next cell can load them confidently.\n",
    "try:\n",
    "    print(\"📁 Saved FAISS files:\", os.listdir(faiss_index_dir))\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not list {faiss_index_dir}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reuse vectorstore later if needed\n",
    "\n",
    "def rehydrate_faiss_vectorstore(\n",
    "    save_dir: str = \"data/faiss_index\",\n",
    "    docs_pickle_path: str = \"data/vectorstore_docs.pkl\",\n",
    "    model_name: str = \"all-MiniLM-L6-v2\",\n",
    "    embedding_kwargs: dict | None = None,\n",
    "    allow_dangerous_deserialization: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Rehydrate a FAISS vectorstore that was previously saved with `vectorstore.save_local(save_dir)`.\n",
    "\n",
    "    What this does:\n",
    "    1) Rebuilds the SAME embedding function you used before (defaults to MiniLM).\n",
    "    2) Loads the FAISS index from `save_dir` using LangChain's `FAISS.load_local`.\n",
    "    3) Optionally reloads your original `docs` list from a pickle file (if present).\n",
    "       - This is useful for provenance, debugging, exporting, or rebuilding indices again later.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    save_dir : str\n",
    "        Directory that contains the saved FAISS index files. Use the exact path you passed to `save_local`.\n",
    "    docs_pickle_path : str\n",
    "        Path to the pickle file where you saved the `docs` (Document list). If missing, we’ll warn and return None.\n",
    "    model_name : str\n",
    "        Name of the HuggingFace sentence embedding model to rebuild.\n",
    "        Must match what you used to CREATE the index, otherwise search quality will degrade.\n",
    "    embedding_kwargs : dict | None\n",
    "        Extra kwargs for HuggingFaceEmbeddings (e.g., {\"model_kwargs\": {\"device\": \"cpu\"}}).\n",
    "        Keep this consistent with the original build for reproducibility.\n",
    "    allow_dangerous_deserialization : bool\n",
    "        LangChain’s FAISS loader uses pickle internally for metadata. This flag must be True to load.\n",
    "        Only set to True for trusted artifacts you created yourself.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vectorstore : langchain_community.vectorstores.faiss.FAISS\n",
    "        The loaded FAISS vectorstore, ready for `.similarity_search(...)`, etc.\n",
    "    docs : list[langchain_core.documents.Document] | None\n",
    "        The reloaded `docs` list if `docs_pickle_path` exists, else None.\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    # 1) Save (during build time):\n",
    "    # vectorstore.save_local(\"data/faiss_index\")\n",
    "\n",
    "    # 2) Rehydrate later:\n",
    "    # vs, docs = rehydrate_faiss_vectorstore(\n",
    "    #     save_dir=\"data/faiss_index\",\n",
    "    #     docs_pickle_path=\"data/vectorstore_docs.pkl\",\n",
    "    #     model_name=\"all-MiniLM-L6-v2\"\n",
    "    # )\n",
    "    # results = vs.similarity_search(\"your query\", k=5)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - If you change embedding models between save/load, vector dimensions won’t match queries,\n",
    "      and retrieval quality will tank (even if it doesn’t crash). Keep the SAME model+settings.\n",
    "    - If you moved machines or containers, ensure the same or compatible versions of:\n",
    "        * langchain, langchain-community, faiss, sentence-transformers, transformers\n",
    "      Exact matches are ideal for reproducibility.\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # 0) Imports (kept inside for portability)\n",
    "    # -----------------------------\n",
    "    import os\n",
    "    import pickle\n",
    "    from langchain_huggingface import HuggingFaceEmbeddings\n",
    "    from langchain_community.vectorstores import FAISS\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1) Rebuild the embedding function\n",
    "    # -----------------------------\n",
    "    # IMPORTANT: Use the same model + kwargs as when you created the index.\n",
    "    embedding_kwargs = embedding_kwargs or {}\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name, **embedding_kwargs)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2) Load the FAISS index\n",
    "    # -----------------------------\n",
    "    if not os.path.isdir(save_dir):\n",
    "        raise FileNotFoundError(\n",
    "            f\"FAISS index directory not found: {save_dir}\\n\"\n",
    "            \"Make sure you previously called `vectorstore.save_local(save_dir)` with this path.\"\n",
    "        )\n",
    "\n",
    "    vectorstore = FAISS.load_local(\n",
    "        save_dir,\n",
    "        embeddings=embeddings,\n",
    "        allow_dangerous_deserialization=allow_dangerous_deserialization,\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3) Try to reload original Document list (optional but handy)\n",
    "    # -----------------------------\n",
    "    docs = None\n",
    "    if os.path.exists(docs_pickle_path):\n",
    "        try:\n",
    "            with open(docs_pickle_path, \"rb\") as f:\n",
    "                docs = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Warning: Failed to load docs from {docs_pickle_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"ℹ️  No pickled docs found at {docs_pickle_path}. Returning vectorstore only.\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4) Smoke test (optional): ensure the index is queryable\n",
    "    # -----------------------------\n",
    "    try:\n",
    "        _ = vectorstore.similarity_search(\"health check\", k=1)\n",
    "    except Exception as e:\n",
    "        print(\"⚠️  Vectorstore loaded but test query failed. Check model compatibility and versions.\")\n",
    "        print(f\"Details: {e}\")\n",
    "\n",
    "    print(\"✅ FAISS vectorstore rehydrated successfully.\")\n",
    "    return vectorstore, docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Social Media Mental Health Final_cleaned.pdf:\n",
      "ASONAM ’23, November 6-9, 2023, Kusadasi, Turkey © 2023 Association for Computing Machinery.  ACM ISBN 979-8-4007-0409-3/23/11. . . $15.00 https://doi.org/10.1145/3625007.3627490  Fragile Minds: Exploring the Link Between Social  Media and Young Adul...\n",
      "\n",
      "Food Addiction 20231222 v3_cleaned.pdf:\n",
      "\tbut\talso\tinfluence\tthe\tquality\tof\thealthcare\tprovided\tto\tobese\tpatients.\t\t 7\t\t Discussion Obesity\t adversely\t affects\t health\t involving\t multiple\t systems\t at\t multiple\t levels:\t\tendocrine,\t environmental,\t gastrointestinal,\t genomic,\t immunologic,...\n",
      "\n",
      "Food Addiction 20231222 v3_cleaned.pdf:\n",
      "\t 1\t\t Viewpoint \tIan\tMcCulloh,\tPh.D.\tian@brainrisefoundation.org,\timccull4@jhu.edu\t\tMichael\tOler,\tM.D.\tmike@brainrisefoundation.org\t\tAnna\tMcCulloh.\tamccul16@jhu.edu\t\t\tThe\tBrain\tRise\tFoundation\t14749\tWalcott\tAve\tOrlando,\tFL\t32827\t\tJohns\tHopkins\tBloomb...\n",
      "\n",
      "NeuroCogInfluence_cleaned.pdf:\n",
      " persuasiveness of public narratives.   Journal of personality and social psychology, 79(5), 701.  Hovland, C. I., Harvey, O. J., & Sherif, M. (1957). Assimilation and contrast effects in reactions to   communication and attitude change. The Journal ...\n",
      "\n",
      "NeuroSynchrony_cleaned.pdf:\n",
      "Social Cognitive and Affective Neuroscience, 2021, 117–128 doi:10.1093/scan/nsaa115 Advance Access Publication Date: 7 October 2020 Original Manuscript Interpersonal Synchrony Special Issue Neural reference groups: a synchrony-based classification ap...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Generic query to test retrieval from FAISS\n",
    "# ---------------------------------------------------------------\n",
    "# This block performs a semantic search query over text chunks that were already\n",
    "# embedded and indexed into a FAISS vector store. It shows how to *reload* a\n",
    "# previously saved FAISS index (no re-embedding required) and retrieve the\n",
    "# top-k most relevant chunks for a natural-language query.\n",
    "#\n",
    "# Why FAISS here (quick recap):\n",
    "# - We switched from Chroma to FAISS because the container’s SQLite (3.34.1)\n",
    "#   is below Chroma’s minimum requirement (3.35.0). FAISS avoids SQLite entirely.\n",
    "# - Functionally, you still get fast nearest-neighbor search over your embeddings.\n",
    "# - Persistence works via FAISS’s own `save_local`/`load_local` helpers in LangChain.\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Reload vectorstore (no need to re-embed)\n",
    "# ---------------------------------------------------------------\n",
    "# We reload the FAISS vectorstore from disk using the same directory path that\n",
    "# we used during initial indexing when calling `vectorstore.save_local(<dir>)`.\n",
    "# Re-creating the SAME embedding function is important to ensure query vectors\n",
    "# live in the same space as the stored vectors.\n",
    "#\n",
    "# NOTE:\n",
    "# - `faiss_index_dir` should match the directory you passed to `save_local`.\n",
    "# - `allow_dangerous_deserialization=True` is required by LangChain to unpickle\n",
    "#   metadata. Only load indices you trust (i.e., ones you saved yourself).\n",
    "# ---------------------------------------------------------------\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "faiss_index_dir = \"data/faiss_index\"  # <-- must match your earlier `vectorstore.save_local(...)`\n",
    "\n",
    "# (Re)create the SAME embedding function used during indexing.\n",
    "# If you already have `embedding_model` in scope (MiniLM), we reuse it.\n",
    "# If not, uncomment the two lines below to rebuild it.\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load the FAISS index from disk. This does NOT recompute embeddings.\n",
    "vectorstore = FAISS.load_local(\n",
    "    faiss_index_dir,\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True,  # required due to pickle-based metadata\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Set up the query topic (can be static or dynamic)\n",
    "# ---------------------------------------------------------------\n",
    "# This string will be embedded with the SAME model as your documents (MiniLM),\n",
    "# and FAISS will retrieve the most similar vectors by inner product / L2 (as used by LangChain).\n",
    "# In production, this could come from a NOFO, user input, or an upstream pipeline.\n",
    "# ---------------------------------------------------------------\n",
    "priority_topic = \"mental health\"  # Example query topic, e.g., extracted from a NOFO\n",
    "query = priority_topic  # Alias for clarity — makes it easy to swap in a different source later\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Run similarity search\n",
    "# ---------------------------------------------------------------\n",
    "# FAISS performs a nearest-neighbor search in vector space using the underlying\n",
    "# index built earlier. `k=5` returns the 5 most similar chunks.\n",
    "# Each result is a LangChain `Document` with `.page_content` and `.metadata`.\n",
    "# ---------------------------------------------------------------\n",
    "results = vectorstore.similarity_search(query, k=5)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Display results\n",
    "# ---------------------------------------------------------------\n",
    "# For each result, we print:\n",
    "# - The `paper_id` from metadata (so you know which PDF it came from)\n",
    "# - A 250-character snippet of the text for quick inspection\n",
    "# This is handy for validating that the retrieval matches your intent.\n",
    "# ---------------------------------------------------------------\n",
    "for r in results:\n",
    "    # Be defensive: metadata keys can vary. Use `.get()` to avoid KeyErrors.\n",
    "    source_id = r.metadata.get(\"paper_id\", r.metadata.get(\"source_file\", \"unknown_source\"))\n",
    "    preview = (r.page_content or \"\")[:250].replace(\"\\n\", \" \")\n",
    "    print(f\"{source_id}:\\n{preview}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZG1Ah1eDi7aK"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Step 1: Topic Extraction - [3 Marks]**\n",
    "\n",
    "> **Read the NOFO doc and identify the topic for which the funding is to be given.**\n",
    "---\n",
    "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt and 1 mark for the successful completion of this section, including debugging or modifying the code if necessary.*\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkW_lO_CHSpc"
   },
   "source": [
    "**TASK:** Write an LLM prompt to extract the Topic for what the funding is been provided, from the NOFO document, Ask the LLM to respond back with the topic name only and nothing else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "1DcEpaUYNM_W"
   },
   "outputs": [],
   "source": [
    "# Topic extraction prompt\n",
    "topic_extraction_prompt = f\"\"\"\n",
    "You are a research grant specialist with expertise in analyzing NIH funding announcements and extracting key research priorities.\n",
    "\n",
    "Your task: Analyze this NOFO document from the National Institute of Mental Health (NIMH) to identify the PRIMARY funding topic.\n",
    "\n",
    "The document may describe multiple research areas, objectives, and priorities. Extract the single overarching topic that encompasses the main focus of this funding opportunity.\n",
    "\n",
    "Return ONLY the primary topic in 3-8 words. No explanations, descriptions, or additional text.\n",
    "\n",
    "Document:\n",
    "{NOFO_pdf[0].page_content}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "WtmCEbaKN9aW",
    "outputId": "7f9d4f38-5e38-4c9c-f70a-3976dfb8c881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digital mental health interventions\n"
     ]
    }
   ],
   "source": [
    "# Finding the topic for which the Funding is been given\n",
    "topic_extraction = llm.invoke(topic_extraction_prompt)\n",
    "topic = topic_extraction.content\n",
    "print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot Prompt Setup for Assessing Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXHRa9IlMycZ"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Step 2: Research Paper Relevance Assessment - [3 Marks]**\n",
    "> **Analyze all the Research Papers and filter out the research papers based on the topic of NOFO**\n",
    "---\n",
    "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt and 1 mark for the successful completion of this section, including debugging or modifying the code if necessary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kWc0LaCGPo3"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "**TASK:** Write an Prompt which can be used to analyze the relevance of the provided research paper in relation to the topic outlined in the NOFO (Notice of Funding Opportunity) document. Determine whether the research aligns with the goals, objectives, and funding criteria specified in the NOFO. Additionally, assess whether the research paper can be used to support or develop a viable project idea that fits within the scope of the funding opportunity.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Note:** If the paper does **not** significantly relate to the topic—by domain, method, theory, or application ask the LLM to return: **\"PAPER NOT RELATED TO TOPIC\"**\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Ask the LLM to respond in the below specified structure:\n",
    "\n",
    "```\n",
    "### Output Format:\n",
    "\"summary\": \"<summary of the paper under 300 words, or return: PAPER NOT RELATED TO TOPIC>\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "F3LPwoNeyXC7"
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# RELEVANCE PROMPT (revised to match instructor's required output)\n",
    "# ------------------------------------------------------------\n",
    "relevance_prompt_a = f\"\"\"\n",
    "You are a research grant specialist evaluating whether a research paper is relevant to the NIH NOFO topic: {topic}.\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "Determine if the paper relates to digital mental health interventions through ANY of:\n",
    "- Direct focus on digital/technology-based mental health solutions\n",
    "- Mental health conditions, treatments, or outcomes (even if not digital)\n",
    "- Digital health technologies that could be applied to mental health\n",
    "- Intervention design, implementation, or evaluation methodologies\n",
    "- User engagement, adherence, or behavior change in health contexts\n",
    "- Relevant populations, settings, or delivery mechanisms\n",
    "\n",
    "DECISION:\n",
    "- If the paper has NO reasonable connection to the topic area: \n",
    "  return exactly: PAPER NOT RELATED TO TOPIC\n",
    "- If the paper has ANY potential relevance:\n",
    "  return a <300 word summary highlighting key findings, methods, or insights\n",
    "\n",
    "OUTPUT FORMAT (return ONLY valid JSON):\n",
    "{{\n",
    "  \"summary\": \"<summary under 300 words OR exactly: PAPER NOT RELATED TO TOPIC>\"\n",
    "  \"relevance_confidence\": \"high|medium|low\"\n",
    "}}\n",
    "\n",
    "### Paper content:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# FEW-SHOT RETRIEVAL FUNCTION (updated to emit summary-only examples)\n",
    "# ------------------------------------------------------------\n",
    "def get_few_shot_examples(\n",
    "    json_path,\n",
    "    max_examples=4,                 # total examples to include\n",
    "    min_confidence=70               # minimum confidence threshold (used only if log has old shape)\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieve few-shot examples for prompt building, normalized to the REQUIRED output:\n",
    "      { \"summary\": \"<summary or PAPER NOT RELATED TO TOPIC>\" }\n",
    "\n",
    "    We adapt older log formats if needed by extracting/deriving a summary.\n",
    "    \"\"\"\n",
    "    import os, json, random\n",
    "\n",
    "    def _coerce_to_summary_only(example_obj):\n",
    "        \"\"\"\n",
    "        Accepts prior example objects (possibly with older fields) and returns\n",
    "        a JSON string containing ONLY the 'summary' field per spec.\n",
    "        \"\"\"\n",
    "        # If already summary-only JSON string, pass through.\n",
    "        if isinstance(example_obj, str):\n",
    "            # Try to detect if it's already JSON with \"summary\"; if not, wrap it.\n",
    "            try:\n",
    "                data = json.loads(example_obj)\n",
    "                if isinstance(data, dict) and \"summary\" in data:\n",
    "                    return json.dumps({\"summary\": data[\"summary\"]}, ensure_ascii=False)\n",
    "            except Exception:\n",
    "                # treat as raw summary text\n",
    "                pass\n",
    "            return json.dumps({\"summary\": example_obj}, ensure_ascii=False)\n",
    "\n",
    "        # If dict-like, try to derive summary:\n",
    "        if isinstance(example_obj, dict):\n",
    "            # Preferred: already has 'summary'\n",
    "            if \"summary\" in example_obj:\n",
    "                return json.dumps({\"summary\": example_obj[\"summary\"]}, ensure_ascii=False)\n",
    "\n",
    "            # If older shape (criteria + decision), infer:\n",
    "            decision = example_obj.get(\"decision\", \"\")\n",
    "            if isinstance(decision, str) and \"NOT RELATED\" in decision.upper():\n",
    "                return json.dumps({\"summary\": \"PAPER NOT RELATED TO TOPIC\"}, ensure_ascii=False)\n",
    "\n",
    "            # Fallback: try to shrink any 'llm_reasoning' field to <=300 words\n",
    "            reasoning = example_obj.get(\"llm_reasoning\") or example_obj.get(\"reasoning\")\n",
    "            if isinstance(reasoning, str) and reasoning.strip():\n",
    "                # naive trim; in practice we had model generate the summary already\n",
    "                return json.dumps({\"summary\": reasoning[:2000]}, ensure_ascii=False)\n",
    "\n",
    "        # Last resort: unrelated\n",
    "        return json.dumps({\"summary\": \"PAPER NOT RELATED TO TOPIC\"}, ensure_ascii=False)\n",
    "\n",
    "    examples = []\n",
    "\n",
    "    # 1) Try to load from log\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                data = []\n",
    "\n",
    "        relevant, irrelevant = [], []\n",
    "\n",
    "        for iteration in data:\n",
    "            for doc in iteration.get(\"relevant_documents\", []):\n",
    "                # Old logs may have confidences and long reasoning; coerce\n",
    "                hybrid_conf = max(doc.get(\"model_confidence\", 0) or 0, doc.get(\"rule_confidence\", 0) or 0)\n",
    "                if hybrid_conf >= min_confidence:\n",
    "                    title = doc.get(\"title\", \"Untitled\")\n",
    "                    reasoning_json = _coerce_to_summary_only({\n",
    "                        \"summary\": doc.get(\"summary\") or doc.get(\"llm_reasoning\") or \"\"\n",
    "                    })\n",
    "                    relevant.append((title, reasoning_json))\n",
    "\n",
    "            for doc in iteration.get(\"irrelevant_documents\", []):\n",
    "                # 'doc' may be a title or dict; normalize\n",
    "                title = doc if isinstance(doc, str) else doc.get(\"title\", \"Untitled\")\n",
    "                reasoning_json = json.dumps({\"summary\": \"PAPER NOT RELATED TO TOPIC\"})\n",
    "                irrelevant.append((title, reasoning_json))\n",
    "\n",
    "        half = max_examples // 2\n",
    "        random.shuffle(relevant)\n",
    "        random.shuffle(irrelevant)\n",
    "        examples = relevant[:half] + irrelevant[:half]\n",
    "\n",
    "    # 2) Fallback seed examples (already in summary-only format)\n",
    "    if not examples:\n",
    "        print(\"No high-confidence examples found. Using fallback seed examples.\")\n",
    "        examples = [\n",
    "            (\n",
    "                \"Digital CBT for Adolescents\",\n",
    "                json.dumps({\n",
    "                    \"summary\": \"A randomized study of a mobile CBT app for adolescents shows clinically meaningful reductions in anxiety/depression versus control and provides implementation insights for school-based deployment.\"\n",
    "                }, ensure_ascii=False)\n",
    "            ),\n",
    "            (\n",
    "                \"Oncology Drug Delivery Review\",\n",
    "                json.dumps({\n",
    "                    \"summary\": \"PAPER NOT RELATED TO TOPIC\"\n",
    "                })\n",
    "            )\n",
    "        ][:max_examples]\n",
    "\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# FUNCTION: build_prompt_with_examples (unchanged behavior, new output format)\n",
    "# ------------------------------------------------------------\n",
    "def build_prompt_with_examples(topic, base_prompt, examples):\n",
    "    \"\"\"\n",
    "    Build a few-shot prompt for the relevance task.\n",
    "    Few-shot examples are *already* normalized to summary-only JSON outputs.\n",
    "    \"\"\"\n",
    "    examples_str = \"\\n\\n\".join(\n",
    "        [f\"Example ({title}):\\n{reasoning}\" for title, reasoning in examples]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a research grant specialist evaluating research papers for relevance to NIH NOFO objectives: {topic}.\n",
    "\n",
    "Below are examples of prior evaluations for context (note: each returns ONLY a JSON object with a 'summary' field):\n",
    "{examples_str}\n",
    "\n",
    "Now evaluate the following paper using the SAME OUTPUT FORMAT:\n",
    "\n",
    "{base_prompt}\n",
    "\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BOOTSTRAP / SAFETY NET\n",
    "# Place this ABOVE PHASE 1 and run it once per session\n",
    "# =========================\n",
    "\n",
    "# Tokenizer + MAX_TOKENS (define if missing)\n",
    "try:\n",
    "    MAX_TOKENS  # noqa: F821\n",
    "except NameError:\n",
    "    MAX_TOKENS = 300_000  # safety ceiling for prompt+context\n",
    "\n",
    "try:\n",
    "    encoding  # noqa: F821\n",
    "except NameError:\n",
    "    import tiktoken\n",
    "    # Use your target model’s encoding (you used gpt-4o-mini elsewhere)\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# LIVE BUDGET GUARDRAIL\n",
    "# =========================\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# --- Pricing table per 1K tokens (USD). Adjust if you switch models. ---\n",
    "PRICING = {\n",
    "    \"gpt-4o-mini\": {\"input\": 0.00015, \"output\": 0.00060},\n",
    "    \"gpt-4o\":      {\"input\": 0.00250, \"output\": 0.01000},\n",
    "    # Add others if needed...\n",
    "}\n",
    "\n",
    "# --- Your daily budget from the course: ---\n",
    "DAILY_BUDGET_USD = 4.00\n",
    "STOP_MARGIN = 0.05   # stop if predicted total would exceed budget - margin\n",
    "\n",
    "# Global trackers (idempotent definitions)\n",
    "try: total_input_tokens\n",
    "except NameError: total_input_tokens = 0\n",
    "try: total_output_tokens\n",
    "except NameError: total_output_tokens = 0\n",
    "try: total_cost_usd\n",
    "except NameError: total_cost_usd = 0.0\n",
    "\n",
    "@dataclass\n",
    "class CostBreakdown:\n",
    "    prompt_tokens: int\n",
    "    completion_tokens: int\n",
    "    input_cost: float\n",
    "    output_cost: float\n",
    "    total_cost: float\n",
    "\n",
    "def get_model_prices(model_name: str):\n",
    "    # Default to 4o-mini if unknown (prevents KeyError)\n",
    "    p = PRICING.get(model_name, PRICING[\"gpt-4o-mini\"])\n",
    "    return p[\"input\"], p[\"output\"]\n",
    "\n",
    "def estimate_cost(model_name: str, prompt_tokens: int, completion_tokens: int) -> CostBreakdown:\n",
    "    in_p_per_1k, out_p_per_1k = get_model_prices(model_name)\n",
    "    input_cost  = (prompt_tokens    / 1000.0) * in_p_per_1k\n",
    "    output_cost = (completion_tokens / 1000.0) * out_p_per_1k\n",
    "    return CostBreakdown(prompt_tokens, completion_tokens, input_cost, output_cost, input_cost + output_cost)\n",
    "\n",
    "def will_exceed_budget(predicted_increment_usd: float) -> bool:\n",
    "    return (total_cost_usd + predicted_increment_usd) >= (DAILY_BUDGET_USD - STOP_MARGIN)\n",
    "\n",
    "def invoke_with_budget_guardrail(llm, prompt: str, model_name: str = \"gpt-4o-mini\", trace_info: dict | None = None):\n",
    "    \"\"\"\n",
    "    Wraps llm.invoke to:\n",
    "      1) Daily reset check (UTC)\n",
    "      2) Pre-check cost estimate (budget guard)\n",
    "      3) Call model\n",
    "      4) Use real usage if available; else fallback estimate\n",
    "      5) Update counters\n",
    "      6) LOG each call (CSV + JSONL) with context from `trace_info`\n",
    "    \"\"\"\n",
    "    global total_input_tokens, total_output_tokens, total_cost_usd\n",
    "    _maybe_daily_reset()  # <-- new\n",
    "\n",
    "    # --- Pre-flight rough estimate ---\n",
    "    approx_prompt_tokens = len(encoding.encode(prompt)) if \"encoding\" in globals() else int(len(prompt.split()) * 1.3)\n",
    "    approx_completion_tokens = 300\n",
    "    pre_cost = estimate_cost(model_name, approx_prompt_tokens, approx_completion_tokens)\n",
    "    if will_exceed_budget(pre_cost.total_cost):\n",
    "        raise RuntimeError(\n",
    "            f\"[BUDGET GUARD] Aborting: this request is predicted to exceed the daily cap. \"\n",
    "            f\"(current=${total_cost_usd:.4f}, +${pre_cost.total_cost:.4f} >= ${DAILY_BUDGET_USD:.2f})\"\n",
    "        )\n",
    "\n",
    "    # --- Call the model ---\n",
    "    resp = llm.invoke(prompt)\n",
    "\n",
    "    # --- Pull usage (prefer real numbers) ---\n",
    "    prompt_tokens = approx_prompt_tokens\n",
    "    completion_tokens = approx_completion_tokens\n",
    "    meta = getattr(resp, \"response_metadata\", {}) or {}\n",
    "    usage = meta.get(\"token_usage\") or meta.get(\"usage\") or {}\n",
    "    if usage:\n",
    "        prompt_tokens = int(usage.get(\"prompt_tokens\", approx_prompt_tokens) or 0)\n",
    "        completion_tokens = int(usage.get(\"completion_tokens\", approx_completion_tokens) or 0)\n",
    "\n",
    "    inc = estimate_cost(model_name, prompt_tokens, completion_tokens)\n",
    "\n",
    "    # Final guard with real usage\n",
    "    if will_exceed_budget(inc.total_cost):\n",
    "        raise RuntimeError(\n",
    "            f\"[BUDGET GUARD] Aborting post-call: usage would exceed cap. \"\n",
    "            f\"(current=${total_cost_usd:.4f}, +${inc.total_cost:.4f} >= ${DAILY_BUDGET_USD:.2f})\"\n",
    "        )\n",
    "\n",
    "    # --- Update counters ---\n",
    "    total_input_tokens += prompt_tokens\n",
    "    total_output_tokens += completion_tokens\n",
    "    total_cost_usd += inc.total_cost\n",
    "\n",
    "    # --- Build log row ---\n",
    "    ctx = trace_info or {}  # optional metadata about the call (paper_id, phase, etc.)\n",
    "    log_row = {\n",
    "        \"ts_utc\": _now_iso(),\n",
    "        \"model\": model_name,\n",
    "        \"paper_id\": ctx.get(\"paper_id\"),\n",
    "        \"phase\": ctx.get(\"phase\"),                 # e.g., \"relevance_eval\" / \"summary\"\n",
    "        \"batch\": ctx.get(\"batch\"),\n",
    "        \"iteration\": ctx.get(\"iteration\"),\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"completion_tokens\": completion_tokens,\n",
    "        \"input_cost_usd\": round(inc.input_cost, 6),\n",
    "        \"output_cost_usd\": round(inc.output_cost, 6),\n",
    "        \"increment_cost_usd\": round(inc.total_cost, 6),\n",
    "        \"run_total_cost_usd\": round(total_cost_usd, 6),\n",
    "        \"run_total_prompt_tokens\": total_input_tokens,\n",
    "        \"run_total_completion_tokens\": total_output_tokens,\n",
    "        \"daily_budget_usd\": DAILY_BUDGET_USD,\n",
    "    }\n",
    "\n",
    "    # --- Persist logs (CSV + JSONL) ---\n",
    "    _append_csv(COST_LOG_CSV, log_row)\n",
    "    _append_jsonl(COST_LOG_JSONL, log_row)\n",
    "\n",
    "    print(f\"[COST] +${inc.total_cost:.4f} (in={prompt_tokens}, out={completion_tokens}) \"\n",
    "          f\"→ run=${total_cost_usd:.4f} | logged.\")\n",
    "\n",
    "    return resp\n",
    "\n",
    "\n",
    "# =========================\n",
    "# COST LOGGING + DAILY RESET\n",
    "# =========================\n",
    "import os, csv, json, datetime\n",
    "from dataclasses import asdict\n",
    "\n",
    "# Where to store logs\n",
    "COST_LOG_JSONL = \"logs/cost_usage.ndjson\"   # one JSON object per line\n",
    "COST_LOG_CSV   = \"logs/cost_usage.csv\"      # tabular\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Track the current UTC day to auto-reset counters\n",
    "try:\n",
    "    _BILLING_DAY_UTC\n",
    "except NameError:\n",
    "    _BILLING_DAY_UTC = datetime.datetime.utcnow().date()\n",
    "\n",
    "def _ensure_csv_header(path: str, fieldnames: list[str]):\n",
    "    \"\"\"Create CSV with header if it doesn't exist yet.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "def _append_csv(path: str, row: dict):\n",
    "    \"\"\"Append one row to CSV.\"\"\"\n",
    "    fieldnames = list(row.keys())\n",
    "    _ensure_csv_header(path, fieldnames)\n",
    "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writerow(row)\n",
    "\n",
    "def _append_jsonl(path: str, obj: dict):\n",
    "    \"\"\"Append one JSON object per line (NDJSON).\"\"\"\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def _maybe_daily_reset():\n",
    "    \"\"\"\n",
    "    If UTC day changed, reset the in-memory counters so the guardrail\n",
    "    enforces the $4/day limit per UTC day.\n",
    "    \"\"\"\n",
    "    global _BILLING_DAY_UTC, total_input_tokens, total_output_tokens, total_cost_usd\n",
    "    today = datetime.datetime.utcnow().date()\n",
    "    if today != _BILLING_DAY_UTC:\n",
    "        _BILLING_DAY_UTC = today\n",
    "        total_input_tokens = 0\n",
    "        total_output_tokens = 0\n",
    "        total_cost_usd = 0.0\n",
    "        print(f\"[BUDGET] New UTC day detected → counters reset ({today.isoformat()}).\")\n",
    "\n",
    "def _now_iso():\n",
    "    return datetime.datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# SUMMARIZATION HELPER (must be defined before first use)\n",
    "# ------------------------------------------------------------\n",
    "def summarize_text(paper_text: str, max_words: int = 300) -> str:\n",
    "    \"\"\"\n",
    "    Summarizes a chunk or full paper to ~max_words words.\n",
    "    Prefers the LLM path if `llm` is available; otherwise, falls back to a\n",
    "    simple heuristic (first N words after whitespace normalization).\n",
    "\n",
    "    This function is placed BEFORE it's first use (e.g., in PHASE 2 early-summary),\n",
    "    to avoid NameError.\n",
    "    \"\"\"\n",
    "    # Try LLM path if present\n",
    "    if \"llm\" in globals() and llm is not None:\n",
    "        summary_prompt = f\"\"\"\n",
    "        Summarize the following research paper into ~{max_words} words,\n",
    "        focusing on digital mental health interventions, methods, and outcomes.\n",
    "\n",
    "        Return plain text only (no markdown):\n",
    "\n",
    "        {paper_text}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            summary_response = invoke_with_budget_guardrail(\n",
    "                llm,\n",
    "                summary_prompt,\n",
    "                model_name=\"gpt-4o-mini\",\n",
    "                trace_info={\n",
    "                    \"paper_id\": paper_id,\n",
    "                    \"phase\": \"relevance_eval\",\n",
    "                    \"batch\": batch_start // BATCH_SIZE + 1,\n",
    "                    \"iteration\": progress_cnt,\n",
    "                },\n",
    "            )\n",
    "\n",
    "            text = (summary_response.content or \"\").strip()\n",
    "            if text:\n",
    "                return text\n",
    "        except Exception as e:\n",
    "            print(f\"[summarize_text] LLM summarization failed -> fallback. Error: {e}\")\n",
    "\n",
    "    # Heuristic fallback: whitespace normalize then take first N words\n",
    "    # Keeps you unblocked if the LLM client isn't initialized yet.\n",
    "    compact = \" \".join((paper_text or \"\").split())\n",
    "    words = compact.split()\n",
    "    return \" \".join(words[:max_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- SAFETY SHIM: ensure logger exists BEFORE we call it ----------\n",
    "if \"log_prompt_iteration\" not in globals():\n",
    "    import json, os\n",
    "    from datetime import datetime\n",
    "\n",
    "    def log_prompt_iteration(\n",
    "        json_path,\n",
    "        prompt,\n",
    "        relevant_docs_with_reasoning,\n",
    "        irrelevant_docs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Minimal logger so the pipeline won't crash if the full logger cell\n",
    "        hasn't run yet. Writes/updates a JSON list at `json_path`.\n",
    "        \"\"\"\n",
    "        # Load existing log (if any)\n",
    "        if os.path.exists(json_path):\n",
    "            try:\n",
    "                with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                if not isinstance(data, list):\n",
    "                    data = []\n",
    "            except Exception:\n",
    "                data = []\n",
    "        else:\n",
    "            data = []\n",
    "\n",
    "        iteration_id = len(data) + 1\n",
    "        from datetime import datetime\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "        entry = {\n",
    "            \"iteration_id\": iteration_id,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"prompt\": prompt,\n",
    "            \"relevant_documents\": relevant_docs_with_reasoning,\n",
    "            \"irrelevant_documents\": irrelevant_docs,\n",
    "        }\n",
    "\n",
    "        data.append(entry)\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"Logged iteration {iteration_id} to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "%% --- CHUNK ANALYSIS START ---\n",
    "flowchart TD\n",
    "    A[All 112 Research Papers] --> B[Split into Chunks per Paper]\n",
    "    B --> C[Embed Chunks into Vector DB + Build BM25 Index]\n",
    "    \n",
    "    %% PHASE 1\n",
    "    C --> D[Phase 1 Retrieval: Overfetch Top Chunks (BM25 + Cosine Hybrid)]\n",
    "    D --> E[Group Retrieved Chunks by Paper]\n",
    "    E --> F[For Each Paper: Get Best Hybrid Score]\n",
    "    F --> G[Count Chunks Seen per Paper]\n",
    "    G --> H[Normalize Score: best_score / (1 + log1p(chunk_count))]\n",
    "    H --> I[Sort Papers by Normalized Score (Lower = Better)]\n",
    "    I --> J[Select Top N Papers for Phase 2]\n",
    "    \n",
    "    %% PHASE 2\n",
    "    J --> K[Retrieve All Chunks for Top N Papers]\n",
    "    K --> L[Cap Chunks per Paper (e.g., Max 10)]\n",
    "    L --> M[Send Capped Chunks to LLM for Full Relevance Eval]\n",
    "    M --> N[Final Ranked & Filtered Papers + Summaries]\n",
    "%% --- END CHUNK ANALYSIS ---\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected 'else' after 'if' expression (790476105.py, line 84)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 84\u001b[0;36m\u001b[0m\n\u001b[0;31m    out = llm_call(v.system, v.user.format(**ex)) if \"{\"\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected 'else' after 'if' expression\n"
     ]
    }
   ],
   "source": [
    "# --- Few-shot setup ---\n",
    "\n",
    "# ==============================================================\n",
    "# SHARED UTILITIES: LoRA loader + tiny prompt optimizer scaffold\n",
    "# Place this once near your imports/utilities.\n",
    "# ==============================================================\n",
    "\n",
    "import os, json, statistics as stats\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Callable\n",
    "\n",
    "# ---- LoRA adapter loader (PEFT) ----\n",
    "def maybe_load_lora(model, adapter_path: str, enabled: bool):\n",
    "    \"\"\"\n",
    "    Load a LoRA adapter into a model if enabled and present.\n",
    "    Safe no-op if the adapter folder is missing or PEFT is unavailable.\n",
    "    \"\"\"\n",
    "    if not enabled:\n",
    "        return model\n",
    "    if not os.path.isdir(adapter_path):\n",
    "        print(f\"[LoRA] Adapter not found at {adapter_path}. Running base model.\")\n",
    "        return model\n",
    "    try:\n",
    "        from peft import PeftModel\n",
    "        model = PeftModel.from_pretrained(model, adapter_path)\n",
    "        print(f\"[LoRA] Loaded adapter from: {adapter_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[LoRA] Failed to load adapter: {e}\")\n",
    "    return model\n",
    "\n",
    "# ---- Minimal multi-agent prompt optimizer (proposer/evaluator/coord) ----\n",
    "@dataclass\n",
    "class Variant:\n",
    "    name: str\n",
    "    system: str\n",
    "    user: str\n",
    "    meta: Dict[str, Any]\n",
    "\n",
    "@dataclass\n",
    "class EvalResult:\n",
    "    name: str\n",
    "    overall: float\n",
    "    details: Dict[str, Any]\n",
    "\n",
    "def propose_variants(base_system: str, base_user: str, n: int) -> List[Variant]:\n",
    "    \"\"\"\n",
    "    Proposer agent: emits n small variations that encourage different emphases.\n",
    "    You can enrich this later with your prompt-pattern library.\n",
    "    \"\"\"\n",
    "    variants = []\n",
    "    knobs = [\n",
    "        \"emphasize conceptual equivalence over keyword overlap\",\n",
    "        \"penalize keyword-only matches lacking conceptual tie-in\",\n",
    "        \"reward explicit linkage to NOFO topic constraints\",\n",
    "        \"require citing paper passages (low-temp extraction)\",\n",
    "        \"downweight very short abstracts with no methods detail\",\n",
    "        \"require a 3-point justification checklist before scoring\"\n",
    "    ]\n",
    "    for i in range(n):\n",
    "        sys_append = f\"\\n- Additional rule: {knobs[i % len(knobs)]}.\"\n",
    "        variants.append(\n",
    "            Variant(\n",
    "                name=f\"v{i+1}\",\n",
    "                system=base_system + sys_append,\n",
    "                user=base_user,\n",
    "                meta={\"rule\": knobs[i % len(knobs)]}\n",
    "            )\n",
    "        )\n",
    "    return variants\n",
    "\n",
    "def evaluate_variants(variants: List[Variant], devset: List[Dict[str, Any]],\n",
    "                      llm_call: Callable[[str,str], str],\n",
    "                      judge_call: Callable[[Dict[str,Any], str], float]) -> List[EvalResult]:\n",
    "    \"\"\"\n",
    "    Evaluator agent: runs each variant on a small calibration set and returns mean scores.\n",
    "    - llm_call(system, user) -> model text\n",
    "    - judge_call(example, output) -> numeric score (0..1 or 0..100)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for v in variants:\n",
    "        scores = []\n",
    "        case_notes = []\n",
    "        for ex in devset:\n",
    "            out = llm_call(v.system, v.user.format(**ex)) if \"{\"\n",
    "            in v.user else llm_call(v.system, v.user + \"\\n\\n\" + ex.get(\"content\",\"\"))\n",
    "            s = judge_call(ex, out)\n",
    "            scores.append(s)\n",
    "            case_notes.append({\"id\": ex.get(\"id\"), \"score\": s})\n",
    "        mean = float(sum(scores) / max(1, len(scores)))\n",
    "        results.append(EvalResult(name=v.name, overall=mean, details={\"cases\": case_notes, \"meta\": v.meta}))\n",
    "    return results\n",
    "\n",
    "def coordinate_prompt_optimization(base_system: str, base_user: str, devset: List[Dict[str,Any]],\n",
    "                                   llm_call: Callable[[str,str], str],\n",
    "                                   judge_call: Callable[[Dict[str,Any], str], float],\n",
    "                                   max_generations: int = 3, n_variants: int = 6, early_stop_delta: float = 0.5):\n",
    "    \"\"\"\n",
    "    Coordinator: iterates proposer → evaluator; stops when improvement < delta.\n",
    "    Returns (best_system, best_user, history).\n",
    "    \"\"\"\n",
    "    history = []\n",
    "    best_overall = -1e9\n",
    "    best_system, best_user = base_system, base_user\n",
    "    for gen in range(max_generations):\n",
    "        variants = propose_variants(best_system, best_user, n_variants)\n",
    "        results = evaluate_variants(variants, devset, llm_call, judge_call)\n",
    "        results.sort(key=lambda r: r.overall, reverse=True)\n",
    "        history.append({\"gen\": gen+1, \"results\": [r.__dict__ for r in results]})\n",
    "        lift = results[0].overall - best_overall\n",
    "        print(f\"[PromptOpt] Gen {gen+1}: best={results[0].name} score={results[0].overall:.3f} (Δ={lift:.3f})\")\n",
    "        if lift < early_stop_delta:\n",
    "            print(\"[PromptOpt] Early stop: minimal improvement.\")\n",
    "            break\n",
    "        best_overall = results[0].overall\n",
    "        # adopt best variant’s system (user stays the same for grading prompts)\n",
    "        best_system = results[0].details.get(\"system\", best_system) if \"system\" in results[0].details else variants[0].system\n",
    "    return best_system, best_user, history\n",
    "\n",
    "\n",
    "LOG_PATH = \"prompt_evaluation_log_cleaned.json\"\n",
    "few_shot_examples = get_few_shot_examples(LOG_PATH)\n",
    "prompt_with_examples = build_prompt_with_examples(topic, relevance_prompt_a, few_shot_examples)\n",
    "\n",
    "# Imports (kept)\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import tiktoken\n",
    "from datetime import datetime\n",
    "# --- Patch: avoid shadowing the datetime module ---\n",
    "# Some helpers (e.g., invoke_with_budget_guardrail) call datetime.datetime.now().\n",
    "# But `from datetime import datetime` above binds `datetime` to the *class*,\n",
    "# breaking references to `datetime.datetime`. Rebind the name to the module if needed.\n",
    "import datetime as _datetime_module\n",
    "if not hasattr(datetime, \"now\"):          # If datetime is the class, it has .now but not .datetime attr\n",
    "    datetime = _datetime_module           # Rebind to module so datetime.datetime is valid\n",
    "elif not hasattr(datetime, \"datetime\"):   # Extra guard: ensure module-like attr is present\n",
    "    datetime = _datetime_module\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CONFIGURATION (kept)\n",
    "# ------------------------------------------------------------\n",
    "TEST_MODE = True\n",
    "DISCREPANCY_THRESHOLD = 20  # (kept for backwards-compat; no longer used in FAISS flow)\n",
    "\n",
    "FAST_MODE = True\n",
    "TOP_K_PAPERS = 60\n",
    "\n",
    "CHUNK_OVERFETCH_FACTOR = 5\n",
    "CHUNK_CAP_PER_PAPER = 10\n",
    "LONG_PAPER_THRESHOLD = 30\n",
    "TOKEN_LIMIT_BEFORE_SUMMARY = 100000\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "BATCH_DELAY = 3\n",
    "\n",
    "INPUT_COST_PER_1K = 0.00015\n",
    "OUTPUT_COST_PER_1K = 0.0006\n",
    "\n",
    "# --- Hybrid & Agent Settings ---\n",
    "LLM_RECHECK_MIN_CONFIDENCE = 60        # %; if Phase 2 judge < this, treat as not relevant\n",
    "AGENTIC_EXPANSION_MAX_TERMS = 12       # request up to N new terms\n",
    "AGENTIC_TOP_SNIPPETS_PER_PAPER = 2     # snippets per top paper for the agent\n",
    "\n",
    "# --- Retrieval toggles & weights (A/B friendly) ---\n",
    "# Options: \"faiss\" (cosine only), \"hybrid\" (cosine+bm25), \"hybrid_tiebreak\" (cosine primary, bm25 as tie-breaker)\n",
    "RETRIEVAL_MODE = \"hybrid\"\n",
    "HYBRID_WEIGHTS = {\"cos\": 0.7, \"bm25\": 0.3}    # soften BM25 so it nudges, not dominates\n",
    "TIEBREAK_WINDOW_MULTIPLIER = 2                # how big a window (in papers) to consider for tie-breaking\n",
    "# --------------------------------------------------\n",
    "\n",
    "total_input_tokens = 0\n",
    "total_output_tokens = 0\n",
    "total_cost_usd = 0.0\n",
    "\n",
    "prior_classification = {\"relevant\": [], \"irrelevant\": [], \"unknown\": []}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# FAISS INTEGRATION (replaces Chroma — no sqlite dependency)\n",
    "# ------------------------------------------------------------\n",
    "# We reload the FAISS index that you saved earlier with vectorstore.save_local(\"data/faiss_index\").\n",
    "# IMPORTANT: FAISS 'similarity_search_with_score' returns (doc, distance);\n",
    "# lower distance = *more* similar. We handle ranking accordingly below.\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "faiss_index_dir = \"data/faiss_index\"\n",
    "\n",
    "vectorstore = FAISS.load_local(\n",
    "    faiss_index_dir,\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 1: PAPER-LEVEL PRE-FILTER (OVERFETCH + RANKING)\n",
    "# ============================================================\n",
    "\n",
    "print(f\"[PHASE 1] Overfetching chunks for paper-level scoring (factor={CHUNK_OVERFETCH_FACTOR})\")\n",
    "\n",
    "# Step 1: Retrieve top-N chunks by similarity (overfetch)\n",
    "overfetch_k = TOP_K_PAPERS * CHUNK_OVERFETCH_FACTOR\n",
    "retrieved_chunks = vectorstore.similarity_search_with_score(topic, k=overfetch_k)  # -> list[(Document, distance)]\n",
    "\n",
    "# --- Hybrid BM25 + Embedding Cosine per-chunk ---\n",
    "# ------------------------------------------------------------------------------------\n",
    "# OVERVIEW (added):\n",
    "# We combine a lexical scorer (BM25) with a semantic scorer (embedding-based cosine via FAISS).\n",
    "# - BM25 captures *exact/term-frequency* matches. It is strong when the query contains\n",
    "#   must-have keywords (e.g., \"EMA\", \"randomized controlled trial\", \"wearable sensor\").\n",
    "# - Embedding cosine captures *semantic* similarity. It helps when papers use different\n",
    "#   phrasing/synonyms for the same idea that BM25 might miss.\n",
    "# How they work together in this notebook:\n",
    "#   1) Here, we compute BM25 scores for each retrieved chunk. We also compute a second\n",
    "#      BM25 \"soft boost\" using your rubric/criteria text (relevance_prompt_a). We\n",
    "#      min–max normalize both and blend them (weighted sum) to get a single BM25 score\n",
    "#      per chunk (`bm25_raw`).\n",
    "#   2) Downstream (outside this block), we convert FAISS distances to a cosine-like\n",
    "#      similarity (`cos_sim = 1/(1+distance)`) and average it with the normalized BM25\n",
    "#      score: `hybrid = 0.5 * (bm25_norm + cos_sim)`.\n",
    "# This hybrid balances *recall* (semantic) and *precision on keywords* (BM25), and the\n",
    "# rubric boost keeps Phase 1 aligned with Phase 2’s judging criteria—without being a\n",
    "# hard filter in Phase 1.\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "# Build a BM25 index over the *retrieved* chunk texts and blend with cosine (from FAISS distance)\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# 1) Prepare a BM25 corpus from the overfetched candidate chunks.\n",
    "#    - We use the chunk text already returned by FAISS as our \"documents\".\n",
    "#    - `or \"\"` avoids None; BM25Okapi expects lists of strings.\n",
    "chunk_texts = [doc.page_content or \"\" for (doc, _d) in retrieved_chunks]\n",
    "\n",
    "# 2) Very simple tokenization (whitespace split). This is intentionally lightweight to\n",
    "#    keep parity with your current preprocessing; you can swap in a smarter tokenizer\n",
    "#    later (e.g., regex, spaCy) without changing the surrounding logic.\n",
    "tokenized = [t.split() for t in chunk_texts]\n",
    "\n",
    "# 3) Initialize BM25 with the tokenized candidate set.\n",
    "bm25 = BM25Okapi(tokenized)\n",
    "\n",
    "# 4) Optional \"soft boost\" from rubric criteria:\n",
    "#    - `relevance_prompt_a` encodes what *matters* per the NOFO (Phase 2 rubric).\n",
    "#    - We score the same chunks against these criteria tokens and then blend that\n",
    "#      score with the topic-only score below. This gently nudges ranking toward\n",
    "#      rubric-aligned chunks without excluding anything at this stage.\n",
    "criteria_text = (relevance_prompt_a or \"\").strip()\n",
    "criteria_tokens = criteria_text.split() if criteria_text else []\n",
    "\n",
    "# 5) Compute raw BM25 scores for (a) the topic and (b) the rubric criteria.\n",
    "#    - Both arrays are the length of `retrieved_chunks`, aligned by index.\n",
    "bm25_topic_raw = bm25.get_scores(topic.split())\n",
    "bm25_crit_raw  = bm25.get_scores(criteria_tokens) if criteria_tokens else np.zeros_like(bm25_topic_raw)\n",
    "\n",
    "# 6) Helper: min–max normalization to [0,1] per candidate set.\n",
    "#    - This prevents one scorer from dominating due to scale differences.\n",
    "#    - The small epsilon (1e-9) guards against division by zero when all values are equal.\n",
    "def _mm(x):\n",
    "    x = np.array(x, dtype=float)\n",
    "    mn, mx = x.min(), x.max()\n",
    "    return (x - mn) / (mx - mn + 1e-9)\n",
    "\n",
    "# 7) Normalize both BM25 channels to [0,1].\n",
    "bm25_topic = _mm(bm25_topic_raw)\n",
    "bm25_crit  = _mm(bm25_crit_raw) if criteria_tokens else np.zeros_like(bm25_topic)\n",
    "\n",
    "# 8) Blend topic vs. criteria with explicit weights.\n",
    "#    - W_TOPIC high preserves broad recall on the main query.\n",
    "#    - W_CRITERIA injects rubric awareness as a *soft* signal.\n",
    "W_TOPIC, W_CRITERIA = 0.7, 0.3  # keep recall strong on topic\n",
    "bm25_raw = W_TOPIC * bm25_topic + W_CRITERIA * bm25_crit  # blended BM25 per chunk\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ORIGINAL (topic-only) BM25 overwrite (DEFUNCT):\n",
    "# The next two lines would *override* the blended `bm25_raw` with topic-only scores,\n",
    "# nullifying the rubric soft boost we just computed. Per your rules, we DO NOT delete\n",
    "# them; we comment them out and explain why.\n",
    "# ------------------------------------------------------------------------------------\n",
    "# # BM25 scores for the topic\n",
    "# bm25_raw = bm25.get_scores(topic.split())  # DEFUNCT: would discard rubric blending above.\n",
    "\n",
    "# 9) Final min–max normalization of the (now blended) BM25 to [0,1] for fusion.\n",
    "#    - Downstream we combine `bm25_norm` with `cos_sim` (converted from FAISS distance)\n",
    "#      as `hybrid = 0.5 * (bm25_norm + cos_sim)`.\n",
    "bm25_min, bm25_max = float(np.min(bm25_raw)), float(np.max(bm25_raw))\n",
    "bm25_norm = [(s - bm25_min) / (bm25_max - bm25_min + 1e-9) for s in bm25_raw]\n",
    "\n",
    "# Convert FAISS distance to a [0,1] similarity (smaller distance => larger similarity)\n",
    "def dist_to_sim(d: float) -> float:\n",
    "    return 1.0 / (1.0 + float(d))\n",
    "\n",
    "# Score helpers (use RETRIEVAL_MODE + HYBRID_WEIGHTS)\n",
    "def chunk_score(bm25_val: float, distance: float) -> float:\n",
    "    \"\"\"Return the per-chunk score used for ranking.\n",
    "    Larger is better (cosine-like similarity / normalized BM25).\"\"\"\n",
    "    cos = 1.0 / (1.0 + float(distance))\n",
    "    if RETRIEVAL_MODE == \"faiss\":\n",
    "        return cos\n",
    "    elif RETRIEVAL_MODE == \"hybrid\":\n",
    "        return HYBRID_WEIGHTS[\"cos\"] * cos + HYBRID_WEIGHTS[\"bm25\"] * float(bm25_val)\n",
    "    elif RETRIEVAL_MODE == \"hybrid_tiebreak\":\n",
    "        # Primary key is cosine; BM25 used later as secondary tie-breaker at paper level.\n",
    "        return cos\n",
    "    else:\n",
    "        return cos  # safe default\n",
    "\n",
    "paper_chunk_counts = {}       # {paper_id: count}\n",
    "paper_chunk_hybrids = {}      # {paper_id: [hybrid_chunk_scores]}\n",
    "\n",
    "paper_bm25_best = {}          # {paper_id: best bm25_norm}  # for hybrid_tiebreak secondary key\n",
    "\n",
    "for (doc, distance), b_s in zip(retrieved_chunks, bm25_norm):\n",
    "    pid = doc.metadata.get(\"paper_id\", \"Unknown_Paper\")\n",
    "    # DEFUNCT fixed 50/50 fusion (kept for traceability):\n",
    "    # cos_sim = dist_to_sim(distance)\n",
    "    # hybrid = 0.5 * (b_s + cos_sim)\n",
    "\n",
    "    score = chunk_score(b_s, distance)       # honors RETRIEVAL_MODE + HYBRID_WEIGHTS\n",
    "    paper_chunk_counts[pid] = paper_chunk_counts.get(pid, 0) + 1\n",
    "    paper_chunk_hybrids.setdefault(pid, []).append(score)\n",
    "\n",
    "    # Track best BM25 per paper for later tie-breaking (only used in 'hybrid_tiebreak')\n",
    "    if b_s is not None:\n",
    "        prev = paper_bm25_best.get(pid, 0.0)\n",
    "        if b_s > prev:\n",
    "            paper_bm25_best[pid] = float(b_s)\n",
    "\n",
    "# Per-paper: use BEST hybrid chunk, then normalize by chunk count to avoid big-paper bias\n",
    "paper_hybrid_best = {pid: max(sc_list) for pid, sc_list in paper_chunk_hybrids.items()}\n",
    "paper_norm_score = {\n",
    "    pid: best / (1.0 + np.log1p(paper_chunk_counts.get(pid, 1)))   # smooth penalty by chunk count\n",
    "    for pid, best in paper_hybrid_best.items()\n",
    "}\n",
    "\n",
    "# Step 2: Aggregate distances per paper_id\n",
    "# --- NEW: accumulate distances across both retrieval passes ---\n",
    "paper_distances = {}  # {paper_id: [distances]}  # unified, first + expanded pass\n",
    "# --------------------------------------------------------------\n",
    "# NOTE: In FAISS, *smaller* distance is better. We will rank by MIN distance per paper.\n",
    "paper_scores = {}  # {paper_id: [distances]}\n",
    "for doc, distance in retrieved_chunks:\n",
    "    pid = doc.metadata.get(\"paper_id\", \"Unknown_Paper\")\n",
    "    paper_scores.setdefault(pid, []).append(distance)       # kept\n",
    "    paper_distances.setdefault(pid, []).append(distance)    # NEW: unified\n",
    "\n",
    "# Step 3: Rank papers by **min distance** (primary) and print **mean** for debugging\n",
    "\n",
    "# ranked_papers = sorted(\n",
    "#     paper_scores.items(),\n",
    "#     key=lambda x: min(x[1])  # smaller distance = closer\n",
    "# )\n",
    "\n",
    "# --- New: rank by normalized per-paper score (higher is better) ---\n",
    "ranked_papers = sorted(paper_norm_score.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Optional tie-break: if RETRIEVAL_MODE == \"hybrid_tiebreak\",\n",
    "# refine ordering for the top window using best BM25 as a secondary key.\n",
    "if RETRIEVAL_MODE == \"hybrid_tiebreak\":\n",
    "    window = max(1, TIEBREAK_WINDOW_MULTIPLIER * TOP_K_PAPERS)\n",
    "    head = ranked_papers[:window]\n",
    "    tail = ranked_papers[window:]\n",
    "    # Within the head, sort by (norm_score primary, bm25_best secondary)\n",
    "    head = sorted(\n",
    "        head,\n",
    "        key=lambda x: (x[1], paper_bm25_best.get(x[0], 0.0)),\n",
    "        reverse=True\n",
    "    )\n",
    "    ranked_papers = head + tail\n",
    "\n",
    "# Step 4: Select top-K unique papers\n",
    "top_paper_ids = [pid for pid, _ in ranked_papers[:TOP_K_PAPERS]]\n",
    "\n",
    "# --- Debug: Show Phase 1 normalization details for top-ranked papers ---\n",
    "print(\"[PHASE 1] Normalization preview for top papers (higher score = more relevant after penalty):\")\n",
    "for pid, norm_score in ranked_papers[:TOP_K_PAPERS]:\n",
    "    chunks_seen = paper_chunk_counts.get(pid, 0)\n",
    "    divisor = 1.0 + np.log1p(chunks_seen)\n",
    "    best_hybrid = paper_hybrid_best.get(pid, float(\"nan\"))\n",
    "    print(\n",
    "        f\" - {pid}\\n\"\n",
    "        f\"    best_hybrid_score = {best_hybrid:.4f}\\n\"\n",
    "        f\"    chunks_seen       = {chunks_seen}\\n\"\n",
    "        f\"    divisor           = {divisor:.3f}  # penalty for chunk count\\n\"\n",
    "        f\"    normalized_score  = {norm_score:.4f}\"\n",
    "    )\n",
    "\n",
    "# --- Agentic query expansion (2nd pass) ---\n",
    "try:\n",
    "    # Collect seed snippets from the already-retrieved chunks for top papers\n",
    "    by_pid_chunks = {}\n",
    "    for doc, dist in retrieved_chunks:\n",
    "        pid = doc.metadata.get(\"paper_id\", \"Unknown_Paper\")\n",
    "        by_pid_chunks.setdefault(pid, []).append((dist, doc.page_content or \"\"))\n",
    "\n",
    "    seed_snippets = []\n",
    "    for pid in top_paper_ids:\n",
    "        # take the closest snippets for each top paper\n",
    "        for dist, txt in sorted(by_pid_chunks.get(pid, []), key=lambda x: x[0])[:AGENTIC_TOP_SNIPPETS_PER_PAPER]:\n",
    "            seed_snippets.append(txt[:500])\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # ORIGINAL PROMPT (now commented out): This placed the bullet list *outside*\n",
    "    # the f-string, so \"SNIPPETS:\" appeared but the bullets were appended after\n",
    "    # the string literal. It also lacked CRITERIA guidance and had no guard for\n",
    "    # empty snippets or None criteria. Keeping for traceability.\n",
    "    # ----------------------------------------------------------------------\n",
    "    # expansion_prompt = f\"\"\"\n",
    "    # You are assisting literature retrieval for an NIH NOFO with topic:\n",
    "    # \"{topic}\"\n",
    "    #\n",
    "    # From the following snippets of already-retrieved papers, identify 5–{AGENTIC_EXPANSION_MAX_TERMS} domain terms,\n",
    "    # phrases, or synonyms that are likely important BUT NOT explicitly present in the query above.\n",
    "    # Prefer multi-word phrases when meaningful. Return ONLY a comma-separated list (no numbering, no quotes).\n",
    "    #\n",
    "    # SNIPPETS:\n",
    "    #\n",
    "    # - \"\"\" + \"\\n- \".join(seed_snippets[:80])\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # PATCHED PROMPT (active):\n",
    "    # - Ensures SNIPPETS bullet list is rendered *directly under* the SNIPPETS header.\n",
    "    # - Adds CRITERIA guidance (from relevance_prompt_a) *after* the evidence.\n",
    "    # - Guards against None/empty: shows \"(no snippets available)\" when no snippets;\n",
    "    #   uses empty string for criteria if relevance_prompt_a is None.\n",
    "    # - Extensively commented per user rules.\n",
    "    # ----------------------------------------------------------------------\n",
    "    crit_text = (relevance_prompt_a or \"\").strip()  # Safe: if None, becomes \"\"\n",
    "    # Clean and cap snippets (keep existing 80-cap); drop empties/whitespace-only\n",
    "    snips = [s.strip() for s in seed_snippets[:80] if s and s.strip()]\n",
    "    snip_block = (\"- \" + \"\\n- \".join(snips)) if snips else \"(no snippets available)\"\n",
    "\n",
    "    expansion_prompt = (\n",
    "        f\"\"\"You are assisting literature retrieval for an NIH NOFO with topic:\n",
    "\"{topic}\"\n",
    "\n",
    "From the following snippets of already-retrieved papers, identify 5–{AGENTIC_EXPANSION_MAX_TERMS} domain terms,\n",
    "phrases, or synonyms that are likely important BUT NOT explicitly present in the query above.\n",
    "Prefer multi-word phrases when meaningful. Return ONLY a comma-separated list (no numbering, no quotes).\n",
    "\n",
    "SNIPPETS:\n",
    "{snip_block}\n",
    "\n",
    "CRITERIA (use these to guide which concepts are missing):\n",
    "{crit_text}\n",
    "\"\"\"\n",
    "    )\n",
    "    # ----------------------------------------------------------------------\n",
    "\n",
    "    exp = invoke_with_budget_guardrail(\n",
    "        llm, expansion_prompt, model_name=\"gpt-4o-mini\",\n",
    "        trace_info={\"phase\": \"agentic_query_expansion\"}\n",
    "    )\n",
    "    raw_terms = exp.content or \"\"\n",
    "    new_terms = [t.strip() for t in re.split(r\"[,\\n;]\", raw_terms) if t.strip()]\n",
    "    new_terms = [t for t in new_terms if t.lower() not in topic.lower()]\n",
    "\n",
    "    if new_terms:\n",
    "        expanded_query = topic + \" \" + \" \".join(new_terms[:AGENTIC_EXPANSION_MAX_TERMS])\n",
    "        print(f\"[AGENT] Expanded query with {len(new_terms)} terms -> rerunning retrieval...\")\n",
    "\n",
    "        # Rerun retrieval with expanded query\n",
    "        retrieved_chunks_exp = vectorstore.similarity_search_with_score(expanded_query, k=overfetch_k)\n",
    "        # NEW: record distances from expanded pass too\n",
    "        for doc, dist in retrieved_chunks_exp:\n",
    "            pid = doc.metadata.get(\"paper_id\", \"Unknown_Paper\")\n",
    "            paper_distances.setdefault(pid, []).append(dist)\n",
    "\n",
    "        # Recompute BM25 on the expanded candidate set and hybrid scores\n",
    "        exp_texts = [doc.page_content or \"\" for (doc, _d) in retrieved_chunks_exp]\n",
    "        exp_tokenized = [t.split() for t in exp_texts]\n",
    "        bm25_exp = BM25Okapi(exp_tokenized)\n",
    "        bm25_raw_exp = bm25_exp.get_scores(expanded_query.split())\n",
    "        bmin_e, bmax_e = float(np.min(bm25_raw_exp)), float(np.max(bm25_raw_exp))\n",
    "        bm25_norm_exp = [(s - bmin_e) / (bmax_e - bmin_e + 1e-9) for s in bm25_raw_exp]\n",
    "\n",
    "        # Update normalized per-paper hybrid scores with the expanded pass (merge + dedupe)\n",
    "        paper_norm_score_exp = dict(paper_norm_score)  # start with first-pass scores\n",
    "        # (also reuse best hybrid we already tracked)\n",
    "        for (doc, dist), b_s in zip(retrieved_chunks_exp, bm25_norm_exp):\n",
    "            pid = doc.metadata.get(\"paper_id\", \"Unknown_Paper\")\n",
    "            h = 0.5 * (b_s + dist_to_sim(dist))\n",
    "            # refresh best hybrid and chunk counts\n",
    "            best = max(h, paper_hybrid_best.get(pid, 0.0))\n",
    "            cnt = paper_chunk_counts.get(pid, 0) + 1\n",
    "            paper_norm_score_exp[pid] = max(paper_norm_score_exp.get(pid, 0.0), best / (1.0 + np.log1p(cnt)))\n",
    "\n",
    "        # Re-rank and take top-K\n",
    "        ranked_papers = sorted(paper_norm_score_exp.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_paper_ids = [pid for pid, _ in ranked_papers[:TOP_K_PAPERS]]\n",
    "    else:\n",
    "        print(\"[AGENT] No new terms produced; using original ranking.\")\n",
    "except Exception as e:\n",
    "    print(f\"[AGENT] Expansion skipped due to error: {e}\")\n",
    "\n",
    "print(\"[PHASE 1] Selecting top papers based on normalized per-paper score\",\n",
    "      \"(best_chunk_score penalized by chunk_count; higher = better).\")\n",
    "\n",
    "# Debug: Print ranking summary\n",
    "print(f\"Selected top {len(top_paper_ids)} papers for Phase 2 evaluation:\")\n",
    "for pid in top_paper_ids:\n",
    "    distances = paper_distances.get(pid, [])\n",
    "    if not distances:\n",
    "        print(f\" - {pid}: (no distance stats available from retrieval passes)\")\n",
    "        continue\n",
    "    print(f\" - {pid}: best(min)={min(distances):.4f}, avg={np.mean(distances):.4f}, chunks={len(distances)}\")\n",
    "\n",
    "# Percentile summary (of best distances per paper)\n",
    "best_dists = [min(dlist) for dlist in paper_distances.values()]\n",
    "\n",
    "if best_dists:\n",
    "    percentiles = [25, 50, 75, 90]\n",
    "    print(\"\\n[PHASE 1] Distance Percentile Summary (lower = more similar):\")\n",
    "    for p in percentiles:\n",
    "        val = np.percentile(best_dists, p)\n",
    "        print(f\"  {p}th percentile: {val:.4f}\")\n",
    "    print(f\"  Min (best): {min(best_dists):.4f}\")\n",
    "    print(f\"  Max (worst): {max(best_dists):.4f}\")\n",
    "else:\n",
    "    print(\"[PHASE 1] No distances available to compute percentiles.\")\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 2: FULL-CHUNK RETRIEVAL WITH CAP + EARLY SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print(\"[PHASE 2] Retrieving all chunks (bulk) and filtering for top papers...\")\n",
    "\n",
    "# Retrieve many chunks at once\n",
    "# NOTE: FAISS returns (doc, distance). Smaller distance = better.\n",
    "all_chunks = vectorstore.similarity_search_with_score(topic, k=9999)\n",
    "\n",
    "paper_chunk_data = {}  # {paper_id: [(distance, text), ...]}\n",
    "\n",
    "for doc, distance in all_chunks:\n",
    "    pid = doc.metadata.get(\"paper_id\", \"Unknown_Paper\")\n",
    "    if pid in top_paper_ids:\n",
    "        paper_chunk_data.setdefault(pid, []).append((distance, doc.page_content))\n",
    "\n",
    "# Build per-paper text with cap and optional early summaries\n",
    "paper_chunks = {}\n",
    "paper_chunk_counts = {}\n",
    "\n",
    "for pid, chunks in paper_chunk_data.items():\n",
    "    # Sort by ascending distance (most similar first)\n",
    "    sorted_chunks = sorted(chunks, key=lambda x: x[0])\n",
    "    capped = sorted_chunks[:CHUNK_CAP_PER_PAPER]\n",
    "    paper_chunk_counts[pid] = len(sorted_chunks)\n",
    "\n",
    "    if len(sorted_chunks) > LONG_PAPER_THRESHOLD:\n",
    "        print(f\"[EARLY SUMMARY] Paper '{pid}' exceeds {LONG_PAPER_THRESHOLD} chunks → summarizing chunks.\")\n",
    "        chunk_summaries = []\n",
    "        for _, chunk_text in capped:\n",
    "            summary = summarize_text(chunk_text)\n",
    "            chunk_summaries.append(summary)\n",
    "        paper_chunks[pid] = \"\\n\".join(chunk_summaries)\n",
    "    else:\n",
    "        paper_chunks[pid] = \"\\n\".join([text for _, text in capped])\n",
    "\n",
    "aggregated_papers = list(paper_chunks.items())\n",
    "print(f\"[PHASE 2] Aggregated {len(aggregated_papers)} papers for full relevance evaluation.\")\n",
    "\n",
    "print(\"[PHASE 2] Chunk count per selected paper (post-aggregation with cap):\")\n",
    "for pid, total in paper_chunk_counts.items():\n",
    "    print(f\" - {pid}: {min(total, CHUNK_CAP_PER_PAPER)} chunks used (original {total})\")\n",
    "\n",
    "# Optional warning for overrepresentation (kept)\n",
    "WARNING_THRESHOLD_PERCENT = 25\n",
    "total_chunks_used = sum(min(count, CHUNK_CAP_PER_PAPER) for count in paper_chunk_counts.values())\n",
    "for pid, count in paper_chunk_counts.items():\n",
    "    used = min(count, CHUNK_CAP_PER_PAPER)\n",
    "    percent = (used / total_chunks_used) * 100 if total_chunks_used else 0\n",
    "    if percent > WARNING_THRESHOLD_PERCENT:\n",
    "        print(f\"*** WARNING: Paper '{pid}' contributes {percent:.1f}% of post-cap chunks ({used}/{total_chunks_used}). ***\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# MAIN LOOP (updated to parse summary-only JSON)\n",
    "# ------------------------------------------------------------\n",
    "documents = []\n",
    "irrelevant_docs_list = []\n",
    "progress_cnt = 1\n",
    "relevant_papers_count = 0\n",
    "irrelevant_papers_count = 0\n",
    "total_files = len(aggregated_papers)\n",
    "\n",
    "for batch_start in range(0, total_files, BATCH_SIZE):\n",
    "    batch = aggregated_papers[batch_start: batch_start + BATCH_SIZE]\n",
    "    print(f\"\\nProcessing batch {batch_start//BATCH_SIZE + 1} \"\n",
    "          f\"({len(batch)} papers) out of {total_files} total papers...\")\n",
    "\n",
    "    for paper_id, paper_text in batch:\n",
    "        try:\n",
    "            # Token budget check\n",
    "            token_estimate = len(encoding.encode(paper_text))\n",
    "            if token_estimate > TOKEN_LIMIT_BEFORE_SUMMARY:\n",
    "                print(f\"[TOKEN GUARD] Paper '{paper_id}' estimated {token_estimate} tokens → auto-summarizing.\")\n",
    "                paper_text = summarize_text(paper_text)\n",
    "\n",
    "            # Build final prompt (few-shot prefix + paper text appended to base prompt)\n",
    "            available_tokens = MAX_TOKENS - len(encoding.encode(prompt_with_examples))\n",
    "            truncated_text = encoding.decode(encoding.encode(paper_text)[:available_tokens])\n",
    "            full_prompt = prompt_with_examples + truncated_text\n",
    "\n",
    "            # Cost tracking\n",
    "            token_count = len(encoding.encode(full_prompt))\n",
    "            total_input_tokens += token_count\n",
    "            total_output_tokens += int(token_count * 0.1)\n",
    "            total_cost_usd = (\n",
    "                (total_input_tokens / 1000) * INPUT_COST_PER_1K +\n",
    "                (total_output_tokens / 1000) * OUTPUT_COST_PER_1K\n",
    "            )\n",
    "            print(f\"[Token Count] {paper_id}: {token_count} tokens \"\n",
    "                  f\"(Estimated running cost: ${total_cost_usd:.4f})\")\n",
    "\n",
    "            # Call LLM\n",
    "            response = invoke_with_budget_guardrail(\n",
    "                llm,\n",
    "                full_prompt,\n",
    "                model_name=\"gpt-4o-mini\",\n",
    "                trace_info={\n",
    "                    \"paper_id\": paper_id,\n",
    "                    \"phase\": \"relevance_eval\",\n",
    "                    \"batch\": batch_start // BATCH_SIZE + 1,\n",
    "                    \"iteration\": progress_cnt,\n",
    "                },\n",
    "            )\n",
    "\n",
    "            progress_cnt += 1\n",
    "\n",
    "            # --- Parse STRICT summary-only JSON per instructor spec ---\n",
    "            parsed = {}\n",
    "            try:\n",
    "                parsed = json.loads(response.content)\n",
    "            except json.JSONDecodeError:\n",
    "                m = re.search(r\"\\{.*\\}\", response.content, re.DOTALL)\n",
    "                parsed = json.loads(m.group(0)) if m else {\"summary\": \"PAPER NOT RELATED TO TOPIC\"}\n",
    "\n",
    "            summary_text = (parsed or {}).get(\"summary\", \"\").strip()\n",
    "\n",
    "            # --- LLM relevance re-check (0–100%) ---\n",
    "            try:\n",
    "                judge_prompt = f\"\"\"\n",
    "            Rate relevance 0–100 using this rubric:\n",
    "            - Coverage of the NOFO's primary topic (40%)\n",
    "            - Alignment with these criteria (60% total), score each 0–100 and weight equally:\n",
    "            {relevance_prompt_a}\n",
    "\n",
    "            Return ONLY a single integer (0–100). If evidence is weak or speculative, score lower.\n",
    "\n",
    "            TOPIC:\n",
    "            {topic}\n",
    "\n",
    "            PAPER SUMMARY:\n",
    "            {summary_text}\n",
    "\n",
    "            \"\"\"\n",
    "                judge_resp = invoke_with_budget_guardrail(\n",
    "                    llm, judge_prompt, model_name=\"gpt-4o-mini\",\n",
    "                    trace_info={\"phase\": \"relevance_recheck\", \"paper_id\": paper_id}\n",
    "                )\n",
    "                m = re.search(r\"\\b(\\d{1,3})\\b\", (judge_resp.content or \"\"))\n",
    "                llm_conf = max(0, min(100, int(m.group(1)))) if m else 50\n",
    "                # Map numeric judge score to an INTERNAL label (not part of model output)\n",
    "                if llm_conf >= 85:\n",
    "                    internal_conf_label = \"high\"\n",
    "                elif llm_conf >= 70:\n",
    "                    internal_conf_label = \"medium\"\n",
    "                else:\n",
    "                    internal_conf_label = \"low\"\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[PHASE 2] LLM re-check failed; defaulting to 50%. Error: {e}\")\n",
    "                llm_conf = 50\n",
    "\n",
    "            # Trim likely false positives\n",
    "            if llm_conf < LLM_RECHECK_MIN_CONFIDENCE:\n",
    "                irrelevant_papers_count += 1\n",
    "                irrelevant_docs_list.append(paper_id)\n",
    "                continue\n",
    "\n",
    "            # Decision: relevant vs not\n",
    "            if summary_text.upper() == \"PAPER NOT RELATED TO TOPIC\" or not summary_text:\n",
    "                irrelevant_papers_count += 1\n",
    "                irrelevant_docs_list.append(paper_id)\n",
    "                continue\n",
    "\n",
    "            # Store result (confidence fields now N/A; we keep placeholders for log schema compatibility)\n",
    "            documents.append({\n",
    "                'title': paper_id,\n",
    "                'file_path': \"(from FAISS index)\",\n",
    "                'llm_reasoning': json.dumps({\"summary\": summary_text}, ensure_ascii=False),  # OUTPUT stays single-field JSON\n",
    "                'model_confidence': None,      # kept for schema compatibility\n",
    "                'rule_confidence': None,\n",
    "                'confidence_discrepancy': None,\n",
    "                'flagged_for_review': False,\n",
    "                'internal_relevance_confidence': internal_conf_label  # INTERNAL ONLY; not part of returned JSON\n",
    "            })\n",
    "\n",
    "            relevant_papers_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!!! Error processing {paper_id}: {str(e)}\")\n",
    "\n",
    "    print(f\"Batch {batch_start//BATCH_SIZE + 1} complete. Sleeping {BATCH_DELAY} seconds...\")\n",
    "    time.sleep(BATCH_DELAY)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SUMMARY OUTPUT (kept; confidence fields will be None)\n",
    "# ------------------------------------------------------------\n",
    "print(\"=\" * 50)\n",
    "print(f\"Relevant Papers: {relevant_papers_count}/{total_files}\")\n",
    "print(f\"Irrelevant Papers: {irrelevant_papers_count}/{total_files}\")\n",
    "print(f\"Estimated Total Input Tokens: {total_input_tokens}\")\n",
    "print(f\"Estimated Total Output Tokens: {total_output_tokens}\")\n",
    "print(f\"Estimated Total Cost: ${total_cost_usd:.4f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nList of relevant papers:\")\n",
    "for doc in documents:\n",
    "    print(f\"\\nTitle: {doc['title']}\")\n",
    "    print(f\"Reasoning (summary only; truncated): {doc['llm_reasoning'][:300]}...\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# LOGGING (kept; now logs only summary JSON in 'reasoning')\n",
    "# ------------------------------------------------------------\n",
    "relevant_docs_with_reasoning = [\n",
    "    {\n",
    "        \"title\": d['title'],\n",
    "        \"reasoning\": d['llm_reasoning'],  # contains {\"summary\": \"...\"} per spec\n",
    "        \"model_confidence\": d['model_confidence'],\n",
    "        \"rule_confidence\": d['rule_confidence'],\n",
    "        \"confidence_discrepancy\": d['confidence_discrepancy'],\n",
    "        \"flagged_for_review\": d['flagged_for_review']\n",
    "    }\n",
    "    for d in documents\n",
    "]\n",
    "\n",
    "log_prompt_iteration(\n",
    "    json_path=\"prompt_evaluation_log_cleaned.json\",\n",
    "    prompt=prompt_with_examples,\n",
    "    relevant_docs_with_reasoning=relevant_docs_with_reasoning,\n",
    "    irrelevant_docs=irrelevant_docs_list\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggested interim step from Claude -- Need to think through and also add agentic step here\n",
    "# Use this AFTER collecting relevant papers\n",
    "proposal_alignment_prompt = \"\"\"\n",
    "Given this collection of relevant research papers and the NOFO requirements, \n",
    "evaluate how each paper could contribute to a fundable proposal:\n",
    "\n",
    "ALIGNMENT DIMENSIONS:\n",
    "1. Methodological Alignment\n",
    "   - Does it provide validated research methods?\n",
    "   - Clinical trial designs or evaluation frameworks?\n",
    "   \n",
    "2. Theoretical Contribution\n",
    "   - Relevant theoretical frameworks?\n",
    "   - Evidence base for intervention design?\n",
    "   \n",
    "3. Practical Application\n",
    "   - Direct implementation pathways?\n",
    "   - Technology solutions or components?\n",
    "   \n",
    "4. Gap Identification\n",
    "   - What problems does it highlight?\n",
    "   - What opportunities for innovation?\n",
    "\n",
    "[Continue with specific NIMH priorities and NOFO requirements...]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNdBg6Iei7VJ"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Step 3: Proposal Ideation Based on Filtered Research - [4 marks]**\n",
    "> **Use the filtered papers, to generate ideas for the Reseach Proposal.**\n",
    "---\n",
    "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt, 1 mark for the Generating Idea and 1 mark for fetching file path of chosen idea along with successful completion of this section, including debugging or modifying the code if necessary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN5c3WhIEpzL"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "**TASK:** Write an Prompt which can be used to generate 5 ideas for the Research Proposal, each idea should consist:\n",
    "\n",
    "1. **Idea X:** [Concise Title of the Project Idea]  \\n\n",
    "2. **Description:** [Brief and targeted description summarizing the objectives, innovative elements, scientific rationale, and anticipated impact.]  \\n\n",
    "3. **Citation:** [Author(s), Year or Paper Title]  \\n\n",
    "4. **NOFO Alignment:** [List two or more specific NOFO requirements that this idea directly addresses]  \\n\n",
    "5. **File Path of the Research Paper:** [Exact file path, ending in .pdf]\n",
    "\n",
    "- Use the Delimiter `---` for defining the structure of the sample outputs in the prompt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLgOVonjveNM"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "#### Generating 5 Ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NoHQ6HH1kiD4"
   },
   "outputs": [],
   "source": [
    "# Note to self: Be sure to add additional details from page linked in the NOFO pdf\n",
    "# Also need to include constraints, e.g., \"Digital health test beds that leverage well-established \n",
    "# digital health platforms to optimize evidence-based digital mental health interventions\"\n",
    "\n",
    "# Suggested criteria from Claude:\n",
    "\n",
    "\n",
    "gen_idea_prompt = f\"\"\"\n",
    "\n",
    "\n",
    "<WRITE YOUR PROMPT HERE>\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-oCJeXkcBKd"
   },
   "outputs": [],
   "source": [
    "ideas = invoke_with_budget_guardrail(\n",
    "    llm,\n",
    "    gen_idea_prompt,\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    trace_info={\n",
    "        \"paper_id\": paper_id,\n",
    "        \"phase\": \"relevance_eval\",\n",
    "        \"batch\": batch_start // BATCH_SIZE + 1,\n",
    "        \"iteration\": progress_cnt,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952
    },
    "id": "TQpW_7cKco8Q",
    "outputId": "6b59af58-d64c-4804-f5a6-bfe89bb023dd"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(ideas.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For consideration if extracted text is not clean enough\n",
    "# Add post-extraction GPT-enabled noise removal step\n",
    "# to remove additional noise from chunks\n",
    "\n",
    "# Too resource intensive for full data set. Add later if needed.\n",
    "\n",
    "# import json\n",
    "# from openai import OpenAI\n",
    "\n",
    "# # Initialize OpenAI client\n",
    "# client = OpenAI()\n",
    "\n",
    "# def semantic_clean_text(raw_text):\n",
    "#     prompt = f\"\"\"\n",
    "# You are a document cleaner. Extract ONLY the main body text from the following academic or technical document:\n",
    "# - Remove page numbers, headers/footers\n",
    "# - Remove title page, author affiliations, figure/table captions\n",
    "# - Remove references/bibliography sections\n",
    "# - Keep abstracts, introductions, main sections, and conclusions\n",
    "\n",
    "# Document:\n",
    "# \\\"\\\"\\\"{raw_text}\\\"\\\"\\\"\n",
    "\n",
    "# Return only the cleaned text.\n",
    "# \"\"\"\n",
    "#     response = client.responses.create(\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         input=prompt,\n",
    "#         max_output_tokens=4000\n",
    "#     )\n",
    "#     return response.output_text\n",
    "\n",
    "# # --- Ingest cleaned + chunked data and post-process with GPT ---\n",
    "# input_path = \"data/cleaned_chunked_papers.json\"\n",
    "# output_path = \"data/cleaned_gpt.json\"\n",
    "\n",
    "# # Load chunked data\n",
    "# with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     chunked_data = json.load(f)\n",
    "\n",
    "# # Prepare list for GPT-processed results\n",
    "# gpt_cleaned_data = []\n",
    "\n",
    "# # Loop through each document\n",
    "# for record in chunked_data:\n",
    "#     doc_id = record[\"id\"]\n",
    "#     gpt_chunks = []\n",
    "\n",
    "#     print(f\"Post-processing (GPT cleanup): {doc_id}\")\n",
    "\n",
    "#     # Apply GPT cleaning to each chunk\n",
    "#     for chunk in record[\"chunks\"]:\n",
    "#         cleaned_chunk = semantic_clean_text(chunk)\n",
    "#         gpt_chunks.append(cleaned_chunk)\n",
    "\n",
    "#     # Store result\n",
    "#     gpt_cleaned_data.append({\n",
    "#         \"id\": doc_id,\n",
    "#         \"chunks\": gpt_chunks\n",
    "#     })\n",
    "\n",
    "# # Save GPT-cleaned data\n",
    "# with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(gpt_cleaned_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# print(f\"Saved GPT post-processed chunks to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLD4_7trvhKL"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "#### Choosing 1 Idea and fetching details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3ulgD6_dkrJ"
   },
   "outputs": [],
   "source": [
    "# Modify the idea_number for choosing the different idea\n",
    "idea_number = 5   # change the number if you wish to choose and generate the research proposal for another idea\n",
    "chosen_idea = ideas.content.split(\"---\")[idea_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DcV8QY1irIyH",
    "outputId": "d71e7380-e5d3-49d8-cd93-3dceedd57cf8"
   },
   "outputs": [],
   "source": [
    "# Import required libraries for core functionality\n",
    "import re\n",
    "\n",
    "# Use a regular expression to find the file path of the research paper\n",
    "\n",
    "pattern = r\"File Path of the Research Paper:\\*\\*\\s*(.+?)\\n\"\n",
    "# If you are unable to extract the file path successfully using this pattern, use the `ChatGPT` or any other LLM to find the pattern that works for you, simply provide the LLM the sample response of your whole ideas and ask the LLM to generate the regex patterm for extracting the \"File Path of the Research Paper\"\n",
    "\n",
    "match = re.search(pattern, chosen_idea)\n",
    "\n",
    "if match:\n",
    "  idea_generated_from_research_paper = match.group(1).strip()\n",
    "  print(\"Filepath : \", idea_generated_from_research_paper)\n",
    "else:\n",
    "  print(\"File Path of the Research Paper not found in the chosen idea.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51quLkMgi7S5"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Step 4: Proposal Blueprint Preparation - [3 Marks]**\n",
    "\n",
    "> **Select appropriate research ideas for the proposal and supply 'Sample Research Proposals' as templates to the LLM to support the generation of the final proposal.**\n",
    "---   \n",
    "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt and 1 mark for the successful completion of this section, including debugging or modifying the code if necessary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJhO1BFHC7cE"
   },
   "source": [
    "**TASK:** Write an Prompt which can be used to generate the Research Proposal.\n",
    "\n",
    "The prompt should be able to craft a research proposal based on the sample research proposal template, using one of the ideas generated above. The proposal should include references to the actual research papers from which the ideas are derived and should align well with the NOFO documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMOe-9_AgKvN"
   },
   "outputs": [],
   "source": [
    "# Here we need to add the full papers instead of the summary\n",
    "# Load PDF files and extract content using PyPDFLoader\n",
    "chosen_idea_rp = PyPDFLoader(idea_generated_from_research_paper, mode=\"single\").load()\n",
    "\n",
    "# Loading the sample research proposal template\n",
    "# Load PDF files and extract content using PyPDFLoader\n",
    "research_proposal_template = PyPDFLoader(\" <Path of Research Proposal Template> \", mode=\"single\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e6c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pypdf import PdfReader\n",
    "import camelot\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import tiktoken\n",
    "\n",
    "# --- Tokenization setup ---\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "MAX_TOKENS = 127500          # total model context window\n",
    "EXTRACTION_BUDGET = 100000   # reserve ~20% for prompts/response\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Count tokens using tiktoken encoding.\"\"\"\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# --- Load matching papers from JSON log ---\n",
    "def load_matched_papers(json_path, pdf_folder=\"content\"):\n",
    "    \"\"\"\n",
    "    Extract list of relevant document file paths from the latest JSON iteration.\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Take the last iteration's relevant_documents\n",
    "    last_iteration = data[-1]\n",
    "    relevant_docs = last_iteration.get(\"relevant_documents\", [])\n",
    "    \n",
    "    # Build file paths for each relevant doc (assumes they exist in pdf_folder)\n",
    "    file_paths = []\n",
    "    for doc in relevant_docs:\n",
    "        title = doc[\"title\"]\n",
    "        pdf_path = os.path.join(pdf_folder, title)\n",
    "        if os.path.exists(pdf_path):\n",
    "            file_paths.append(pdf_path)\n",
    "        else:\n",
    "            print(f\"Warning: {pdf_path} not found. Skipping.\")\n",
    "    return file_paths\n",
    "\n",
    "# --- Stage 1 & 2: Text + Table extraction ---\n",
    "def extract_text_and_tables(file_path, token_budget):\n",
    "    \"\"\"Extract text and tables within token budget.\"\"\"\n",
    "    content = \"\"\n",
    "    token_count = 0\n",
    "\n",
    "    # Stage 1: PyPDF text extraction\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text() or \"\"\n",
    "            token_count += count_tokens(page_text)\n",
    "            if token_count > token_budget:\n",
    "                print(f\"Token budget reached during text extraction: {file_path}\")\n",
    "                break\n",
    "            content += page_text\n",
    "    except Exception as e:\n",
    "        print(f\"PyPDF extraction failed: {e}\")\n",
    "\n",
    "    # Stage 2: Table extraction (Camelot)\n",
    "    # try:\n",
    "    #     tables = camelot.read_pdf(file_path, pages='all')\n",
    "    #     for table in tables:\n",
    "    #         table_text = \"\\n[Table Extracted]\\n\" + table.df.to_string()\n",
    "    #         token_count += count_tokens(table_text)\n",
    "    #         if token_count > token_budget:\n",
    "    #             print(f\"Token budget reached during table extraction: {file_path}\")\n",
    "    #             break\n",
    "    #         content += table_text\n",
    "    # except Exception:\n",
    "    #     pass\n",
    "\n",
    "    return content, token_count\n",
    "\n",
    "# --- Stage 3: OCR extraction ---\n",
    "# def extract_ocr(file_path, token_budget, current_tokens=0):\n",
    "#     \"\"\"Extract OCR text (figures/scanned pages) within remaining token budget.\"\"\"\n",
    "#     content = \"\"\n",
    "#     token_count = current_tokens\n",
    "\n",
    "#     try:\n",
    "#         images = convert_from_path(file_path)\n",
    "#         for image in images:\n",
    "#             ocr_text = pytesseract.image_to_string(image)\n",
    "#             token_count += count_tokens(ocr_text)\n",
    "#             if token_count > token_budget:\n",
    "#                 print(f\"Token budget reached during OCR extraction: {file_path}\")\n",
    "#                 break\n",
    "#             content += \"\\n[OCR Extracted]\\n\" + ocr_text\n",
    "#     except Exception:\n",
    "#         pass\n",
    "\n",
    "    return content\n",
    "\n",
    "# --- Process all matched papers ---\n",
    "def process_matched_papers(json_path, pdf_folder=\"content\"):\n",
    "    \"\"\"\n",
    "    Load matched papers from JSON and process them using multi-stage extraction:\n",
    "    Pass 1: Text + Tables\n",
    "    Pass 2: OCR (Figures)\n",
    "    Returns dict mapping filename -> combined extracted content.\n",
    "    \"\"\"\n",
    "    matched_files = load_matched_papers(json_path, pdf_folder)\n",
    "    text_table_data = {}\n",
    "    token_usage = {}\n",
    "\n",
    "    for file_path in matched_files:\n",
    "        print(f\"Extracting text/tables: {os.path.basename(file_path)}\")\n",
    "        content, tokens_used = extract_text_and_tables(file_path, EXTRACTION_BUDGET)\n",
    "        text_table_data[os.path.basename(file_path)] = content\n",
    "        token_usage[os.path.basename(file_path)] = tokens_used\n",
    "\n",
    "    # Return text_table_data directly\n",
    "    return text_table_data\n",
    "\n",
    "    # Pass 2: Extract OCR for all files (if budget allows)\n",
    "    # for file_path in matched_files:\n",
    "    #     filename = os.path.basename(file_path)\n",
    "    #     remaining_budget = EXTRACTION_BUDGET - token_usage.get(filename, 0)\n",
    "    #     if remaining_budget > 0:\n",
    "    #         print(f\"Extracting OCR: {filename} (remaining budget: {remaining_budget})\")\n",
    "    #         ocr_content = extract_ocr(file_path, EXTRACTION_BUDGET, token_usage[filename])\n",
    "    #         results[filename] = text_table_data[filename] + ocr_content\n",
    "    #     else:\n",
    "    #         print(f\"Skipping OCR for {filename} (no remaining token budget)\")\n",
    "    #         results[filename] = text_table_data[filename]\n",
    "\n",
    "# Example usage:\n",
    "# matched_content = process_matched_papers(\"/mnt/data/prompt_evaluation_log_cleaned.json\", pdf_folder=\"../content\")\n",
    "# print(matched_content.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_content = process_matched_papers(\"prompt_evaluation_log_cleaned.json\", pdf_folder=\"data/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matched_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OGcodZLo7VE"
   },
   "outputs": [],
   "source": [
    "research_proposal_template_prompt = f\"\"\"\n",
    "\n",
    "\n",
    "<WRITE YOUR PROMPT HERE>\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gXIZE0jkieg"
   },
   "outputs": [],
   "source": [
    "research_plan = invoke_with_budget_guardrail(\n",
    "    llm,\n",
    "    research_proposal_template_prompt,\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    trace_info={\n",
    "        \"paper_id\": paper_id,\n",
    "        \"phase\": \"relevance_eval\",\n",
    "        \"batch\": batch_start // BATCH_SIZE + 1,\n",
    "        \"iteration\": progress_cnt,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1UFS-Vxlkib0",
    "outputId": "2761571a-d2a0-421c-b429-252cc78ddd41"
   },
   "outputs": [],
   "source": [
    "display(Markdown(research_plan.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkuiPncysdCa"
   },
   "outputs": [],
   "source": [
    "# @title **Optional Part - Creating a PDF of the Research Proposal**\n",
    "# The code in this cell block is used for printing out the output in the PDF format\n",
    "from markdown_pdf import MarkdownPdf, Section\n",
    "\n",
    "pdf = MarkdownPdf()\n",
    "pdf.add_section(Section(research_plan.content))\n",
    "pdf.save(\"Reseach Proposal First Draft.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77t_iYgni7QV"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Step 5: Proposal Evaluation Against NOFO Criteria - [3 Marks]**\n",
    "> **Use the LLM to evaluate the generated proposal (LLM-as-Judge) and assess its alignment with the NOFO criteria.**\n",
    "   \n",
    "\n",
    "---\n",
    "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt and 1 mark for the successful completion of this section, including debugging or modifying the code if necessary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXWK_mZewlim"
   },
   "source": [
    "**TASK:** Write an Prompt which can be used to evaluate the Research Proposal based on:\n",
    "1. **Innovation**\n",
    "2. **Significance**\n",
    "3. **Approach**\n",
    "4. **Investigator Expertise**\n",
    "\n",
    "- Ask the LLM to rate on each of the criteria from **1 (Poor)** to **5 (Excellent)**\n",
    "- Ask the LLM to provide the resonse in the json format\n",
    "```JSON\n",
    "name: Innovation\n",
    "    justification: \"<Justification>\"\n",
    "    score: <1-5>\n",
    "    strengths: \"<Strength 1>\"\n",
    "    weaknesses: \"<Weakness 1>\"\n",
    "    recommendations: \"<Recommendation 1>\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ax5H703ZhZ7y"
   },
   "outputs": [],
   "source": [
    "evaluation_prompt = f'''\n",
    "\n",
    "\n",
    "<WRITE YOUR PROMPT HERE>\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aZox8iNhZ5g"
   },
   "outputs": [],
   "source": [
    "# Call the LLM with the prepared prompt and truncated paper content\n",
    "eval_response = invoke_with_budget_guardrail(\n",
    "    llm,\n",
    "    evaluation_prompt,\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    trace_info={\n",
    "        \"paper_id\": paper_id,\n",
    "        \"phase\": \"relevance_eval\",\n",
    "        \"batch\": batch_start // BATCH_SIZE + 1,\n",
    "        \"iteration\": progress_cnt,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCx7_am-hZ3H"
   },
   "outputs": [],
   "source": [
    "# Import required libraries for core functionality\n",
    "import json\n",
    "json_resp = json.loads(eval_response.content[7:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vwAFtxolhZpD",
    "outputId": "a5d2b346-065a-428a-8c5d-a38cd57a90ae"
   },
   "outputs": [],
   "source": [
    "for key, value in json_resp.items():\n",
    "  print(f\"---\\n{key}:\")\n",
    "  if isinstance(value, list):\n",
    "    for item in value:\n",
    "      for k, v in item.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "      print(\"=\"*50)\n",
    "  elif isinstance(value, dict):\n",
    "    for k, v in value.items():\n",
    "      print(f\"  {k}: {v}\")\n",
    "  else:\n",
    "    print(f\"  {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fomQFyZAi7N4"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Step 6: Human Review and Refinement of Proposal**\n",
    "> **Perform Human Evaluation of the generated Proposal. Edit or Modify the proposal as necessary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HWwfHOXHmMOu",
    "outputId": "87320d16-f244-429e-e5cb-215e5eae01e7"
   },
   "outputs": [],
   "source": [
    "display(Markdown(research_plan.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRbWRHAJ_KXz"
   },
   "source": [
    "# **Step 7: Summary and Recommendation - [2 Marks]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jC6F4JezA840"
   },
   "source": [
    "Based on the projects, learners are expected to share their observations, key learnings, and insights related to this business use case, including the challenges they encountered.\n",
    "\n",
    "Additionally, they should recommend or explain any changes that could improve the project, along with suggesting additional steps that could be taken for further enhancement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8_WFIYIB12b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c868c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Enhanced PDF Processing (Commenting original PyPDF-only approach) ---\n",
    "# Original starter code (commented for traceability):\n",
    "# Load PDF files and extract content using PyPDFLoader\n",
    "# docs = PyPDFLoader(file_path, mode=\"single\").load()\n",
    "\n",
    "# New Implementation: Multi-stage parsing (PyPDF → Camelot/Tabula → OCR fallback)\n",
    "# Purpose: Capture text, tables, and figures from diverse PDF formats (Mermaid C node, Rubric Step 2).\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "# Import required libraries for core functionality\n",
    "import camelot\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def process_pdf_multistage(file_path):\n",
    "    \"\"\"\n",
    "    Multi-stage pipeline for extracting text, tables, and figures from PDFs.\n",
    "    Stages:\n",
    "    1. PyPDF (text)\n",
    "    2. Camelot/Tabula (tables)\n",
    "    3. OCR (scanned pages/figures)\n",
    "    \"\"\"\n",
    "    content = \"\"\n",
    "\n",
    "    # Stage 1: PyPDF text extraction\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        for page in reader.pages:\n",
    "            content += page.extract_text() or \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"PyPDF extraction failed: {e}\")\n",
    "\n",
    "    # Stage 2: Table extraction (Camelot)\n",
    "    try:\n",
    "        tables = camelot.read_pdf(file_path, pages='all')\n",
    "        for table in tables:\n",
    "            content += \"\\n[Table Extracted]\\n\" + table.df.to_string()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Stage 3: OCR fallback for scanned pages or figures\n",
    "    try:\n",
    "        images = convert_from_path(file_path)\n",
    "        for image in images:\n",
    "            text = pytesseract.image_to_string(image)\n",
    "            content += \"\\n[OCR Extracted]\\n\" + text\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6527505",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Hybrid Retrieval (BM25 + Embeddings) ---\n",
    "# Original code used either BM25 OR embeddings; this combines both (Mermaid D node, Rubric Step 2).\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def hybrid_retrieval_setup(docs_text):\n",
    "    \"\"\"\n",
    "    Creates BM25 and embedding indexes for hybrid search.\n",
    "    \"\"\"\n",
    "    # BM25 Index\n",
    "    tokenized_corpus = [doc.split(\" \") for doc in docs_text]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    # Embedding Index\n",
    "    embed_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectorstore = Chroma.from_texts(docs_text, embed_model)\n",
    "\n",
    "    return bm25, vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aabb353",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Agentic Components (Research Analyst, Proposal Writer, Compliance Checker) ---\n",
    "# Implements multi-agent workflow (Mermaid E subgraph, Rubric Step 3-4).\n",
    "\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "def analyze_papers(query):\n",
    "    return \"Synthesis of relevant papers\"\n",
    "\n",
    "def check_compliance(proposal):\n",
    "    return \"Compliance report\"\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"Research Analyst\", func=analyze_papers, description=\"Synthesizes relevant papers.\"),\n",
    "    Tool(name=\"Compliance Checker\", func=check_compliance, description=\"Ensures NOFO alignment.\")\n",
    "]\n",
    "\n",
    "# Initialize agent with zero-shot reasoning and tools\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a917953",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Agentic Components (Research Analyst, Proposal Writer, Compliance Checker) ---\n",
    "# Implements multi-agent workflow (Mermaid E subgraph, Rubric Step 3-4).\n",
    "\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "def analyze_papers(query):\n",
    "    return \"Synthesis of relevant papers\"\n",
    "\n",
    "def check_compliance(proposal):\n",
    "    return \"Compliance report\"\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"Research Analyst\", func=analyze_papers, description=\"Synthesizes relevant papers.\"),\n",
    "    Tool(name=\"Compliance Checker\", func=check_compliance, description=\"Ensures NOFO alignment.\")\n",
    "]\n",
    "\n",
    "# Initialize agent with zero-shot reasoning and tools\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5141a498",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Multi-Criteria Evaluation with Guardrails ---\n",
    "# Original evaluation only scored NIH criteria; now adds guardrail flags (Mermaid G node, Rubric Step 5).\n",
    "\n",
    "evaluation_prompt = f\"\"\"\n",
    "Evaluate the proposal on:\n",
    "1. Innovation\n",
    "2. Significance\n",
    "3. Approach\n",
    "4. Investigator Expertise\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"criteria\": [\n",
    "    {{\n",
    "      \"name\": \"Innovation\",\n",
    "      \"score\": 1-5,\n",
    "      \"strengths\": \"...\",\n",
    "      \"weaknesses\": \"...\",\n",
    "      \"recommendations\": \"...\"\n",
    "    }},\n",
    "    ...\n",
    "  ],\n",
    "  \"overall_score\": 1-5,\n",
    "  \"guardrail_flags\": [\"hallucination risk\", \"compliance gap\"]\n",
    "}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b907ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Caching Intermediate Steps ---\n",
    "# Saves embeddings, filtered papers, and draft proposals for reuse (Mermaid J node, Rubric Step 7).\n",
    "\n",
    "# Import required libraries for core functionality\n",
    "import pickle\n",
    "\n",
    "def save_checkpoint(data, name):\n",
    "    with open(f\"checkpoint_{name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_checkpoint(name):\n",
    "    try:\n",
    "        with open(f\"checkpoint_{name}.pkl\", \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7667a6",
   "metadata": {},
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "\n",
    "# Quick Reference: Few-Shot + Agentic Enhancements\n",
    "\n",
    "This section provides details about the few-shot pool, semantic versioning, and agentic conflict resolver integrated into this workflow.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "**Semantic Versioning**\n",
    "- Automatically increments version numbers (`v2-fewshot`, `v3-agentic`) based on features used.\n",
    "- Few-shot only → `-fewshot`\n",
    "- Few-shot + agentic resolver → `-agentic`\n",
    "\n",
    "**Few-Shot Pool**\n",
    "- Derived from cleaned log (`prompt_evaluation_log_cleaned.json`).\n",
    "- Filters examples with ≥80% hybrid confidence.\n",
    "- Balances relevant/irrelevant examples 50/50 and ensures diversity.\n",
    "\n",
    "**Agentic Conflict Resolver**\n",
    "- Activates when model vs. rule confidence differs by >20%.\n",
    "- Produces reconciled decision and rationale logged under `agentic_resolution`.\n",
    "\n",
    "**Enhanced Logging Fields**\n",
    "- `decision_source`: hybrid (model + rule)\n",
    "- `hybrid_confidence`: average of model and rule confidence\n",
    "- `agentic_resolution`: reconciliation result (if applicable)\n",
    "- `prompt_version`: auto-generated semantic version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da55b749",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# VERSION TRACKING + FEW-SHOT REBUILDER + AGENTIC RESOLVER\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Function: Determine the next semantic version string for the prompt\n",
    "def get_next_prompt_version(log_path, agentic_enabled=False):\n",
    "    \"\"\"\n",
    "    Determine next semantic version based on last logged version.\n",
    "    Increments number, adds suffix based on features used.\n",
    "    \"\"\"\n",
    "# Import required libraries for core functionality\n",
    "    import os, json, re\n",
    "    version_num = 1\n",
    "    if os.path.exists(log_path):\n",
    "        with open(log_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                data = []\n",
    "        # Extract last version number\n",
    "        for entry in reversed(data):\n",
    "            if \"prompt_version\" in entry:\n",
    "                match = re.match(r\"v(\\d+)\", entry[\"prompt_version\"])\n",
    "                if match:\n",
    "                    version_num = int(match.group(1)) + 1\n",
    "                break\n",
    "\n",
    "    suffix = \"-agentic\" if agentic_enabled else \"-fewshot\"\n",
    "    return f\"v{version_num}{suffix}\"\n",
    "\n",
    "\n",
    "# Function: Build balanced high-confidence few-shot example pool from the log\n",
    "def rebuild_few_shot_pool(cleaned_log_path, min_conf=80, max_examples=4):\n",
    "    \"\"\"\n",
    "    Build balanced high-confidence few-shot pool from cleaned log.\n",
    "    Balances relevant and irrelevant, ensures diversity.\n",
    "    \"\"\"\n",
    "# Import required libraries for core functionality\n",
    "    import json, random\n",
    "    with open(cleaned_log_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    relevant, irrelevant = [], []\n",
    "    for iteration in data:\n",
    "        for doc in iteration.get(\"relevant_documents\", []):\n",
    "            hybrid_conf = max(doc.get(\"model_confidence\", 0), doc.get(\"rule_confidence\", 0))\n",
    "            if hybrid_conf >= min_conf:\n",
    "                relevant.append((doc[\"title\"], doc[\"reasoning\"]))\n",
    "        for doc in iteration.get(\"irrelevant_documents\", []):\n",
    "            irrelevant.append((doc, \"PAPER NOT RELATED TO TOPIC\"))\n",
    "\n",
    "    # Shuffle and balance\n",
    "    half = max_examples // 2\n",
    "    random.shuffle(relevant)\n",
    "    random.shuffle(irrelevant)\n",
    "    return relevant[:half] + irrelevant[:half]\n",
    "\n",
    "\n",
    "# Function: Resolve discrepancies between model and rule confidences using agentic logic\n",
    "def agentic_conflict_resolver(doc_title, reasoning_json, model_conf, rule_conf):\n",
    "    \"\"\"\n",
    "    Agentic layer to reconcile conflicts:\n",
    "    - Triggered when discrepancy exceeds threshold\n",
    "    - Returns reconciled decision and rationale\n",
    "    \"\"\"\n",
    "    rationale = []\n",
    "    if abs(model_conf - rule_conf) > 20:\n",
    "        if rule_conf > model_conf:\n",
    "            final_decision = \"RELEVANT\" if rule_conf >= 50 else \"PAPER NOT RELATED TO TOPIC\"\n",
    "            rationale.append(\"Rule confidence higher; prioritizing deterministic criteria.\")\n",
    "        else:\n",
    "            final_decision = \"RELEVANT\" if model_conf >= 50 else \"PAPER NOT RELATED TO TOPIC\"\n",
    "            rationale.append(\"Model confidence higher; prioritizing LLM interpretation.\")\n",
    "    else:\n",
    "        final_decision = \"RELEVANT\" if (model_conf + rule_conf) / 2 >= 50 else \"PAPER NOT RELATED TO TOPIC\"\n",
    "        rationale.append(\"Confidences close; hybrid average used for decision.\")\n",
    "\n",
    "    return {\n",
    "        \"final_decision\": final_decision,\n",
    "        \"rationale\": \" \".join(rationale)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16955fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# ENHANCED LOGGING WITH SEMANTIC VERSIONING AND AGENTIC RESOLUTION\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Ensure this cell is run AFTER document processing and building relevant_docs_with_reasoning\n",
    "\n",
    "# Define constants for few-shot\n",
    "# Define configuration for few-shot example retrieval (number of examples)\n",
    "FEW_SHOT_MAX_EXAMPLES = 4\n",
    "# Minimum confidence threshold for including examples in few-shot prompting\n",
    "MIN_CONFIDENCE_FOR_FEWSHOT = 70\n",
    "# Path to the cleaned JSON log file where prompt evaluation iterations are stored\n",
    "LOG_PATH = \"prompt_evaluation_log_cleaned.json\"\n",
    "\n",
    "# Determine prompt version\n",
    "# Path to the cleaned JSON log file where prompt evaluation iterations are stored\n",
    "current_version = get_next_prompt_version(LOG_PATH, agentic_enabled=any(doc.get('flagged_for_review', False) for doc in relevant_docs_with_reasoning))\n",
    "\n",
    "# Add decision source and hybrid confidence\n",
    "for doc in relevant_docs_with_reasoning:\n",
    "    doc[\"decision_source\"] = \"hybrid\"\n",
    "    doc[\"hybrid_confidence\"] = (doc[\"model_confidence\"] + doc[\"rule_confidence\"]) / 2\n",
    "\n",
    "# Add agentic resolution for flagged docs\n",
    "for doc in relevant_docs_with_reasoning:\n",
    "    if doc.get(\"flagged_for_review\"):\n",
    "        resolution = agentic_conflict_resolver(\n",
    "            doc_title=doc[\"title\"],\n",
    "            reasoning_json=doc[\"reasoning\"],\n",
    "            model_conf=doc[\"model_confidence\"],\n",
    "            rule_conf=doc[\"rule_confidence\"]\n",
    "        )\n",
    "        doc[\"agentic_resolution\"] = resolution\n",
    "\n",
    "# Append prompt_version to log\n",
    "# Path to the cleaned JSON log file where prompt evaluation iterations are stored\n",
    "with open(LOG_PATH, \"r+\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    if data:\n",
    "        data[-1][\"prompt_version\"] = current_version\n",
    "    f.seek(0)\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    f.truncate()\n",
    "\n",
    "print(f\"Logged with prompt version: {current_version}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional enhancements proposed by Claude\n",
    "\n",
    "Your flowchart shows a well-structured approach to the RFP response generation system. Here are several improvements I'd recommend to enhance the robustness and effectiveness of your solution:\n",
    "\n",
    "1. Enhanced RFP Requirements Extraction\n",
    "After step B, add a sub-process for:\n",
    "\n",
    "Requirement Categorization: Classify requirements into mandatory vs. optional, technical vs. administrative\n",
    "Scoring Rubric Extraction: Specifically parse how proposals will be evaluated\n",
    "Budget Constraints Analysis: Extract funding limits and cost-effectiveness criteria\n",
    "Timeline Extraction: Identify key dates and milestone requirements\n",
    "\n",
    "2. Improved Paper Processing Pipeline\n",
    "Between steps C and D, consider adding:\n",
    "\n",
    "Citation Network Analysis: Map relationships between papers to identify influential work\n",
    "Method/Innovation Extraction: Specifically extract methodologies and novel approaches\n",
    "Results/Outcomes Extraction: Capture quantitative results and impact metrics\n",
    "Quality Assessment: Add a paper quality scoring mechanism (impact factor, recency, relevance)\n",
    "\n",
    "3. Enhanced Retrieval and Ranking\n",
    "Expand step D with:\n",
    "\n",
    "Multi-Query Generation: Generate multiple search queries from different RFP aspects\n",
    "Cross-Reference Validation: Verify that selected papers actually support proposed innovations\n",
    "Diversity Scoring: Ensure selected papers cover different aspects of the RFP\n",
    "Gap Analysis: Identify what the RFP asks for that isn't well-covered in existing research\n",
    "\n",
    "4. Strengthened Agentic Architecture\n",
    "Add these specialized agents to your existing three:\n",
    "\n",
    "Innovation Synthesizer Agent: Combines findings from multiple papers into novel approaches\n",
    "Budget Estimator Agent: Ensures proposals are financially realistic\n",
    "Risk Assessment Agent: Identifies potential implementation challenges\n",
    "Competitive Analysis Agent: Positions your proposal against likely competitors\n",
    "\n",
    "5. Improved Evaluation and Refinement\n",
    "Enhance the evaluation loop (G-I) with:\n",
    "\n",
    "Specific Weakness Detection: Not just overall score, but identify specific weak sections\n",
    "Competitive Benchmarking: Compare against successful past proposals if available\n",
    "Consistency Checking: Ensure all sections align and support each other\n",
    "Technical Feasibility Validation: Verify proposed solutions are implementable\n",
    "\n",
    "6. Additional Process Improvements\n",
    "Consider these architectural enhancements:\n",
    "flowchart LR\n",
    "    subgraph \"Knowledge Management\"\n",
    "        KB1[Domain Ontology]\n",
    "        KB2[Success Patterns DB]\n",
    "        KB3[Common Pitfalls DB]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Feedback Loops\"\n",
    "        FL1[Real-time Agent Collaboration]\n",
    "        FL2[Iterative Improvement Tracking]\n",
    "        FL3[Version Control System]\n",
    "    end\n",
    "7. Quality Assurance Additions\n",
    "\n",
    "Plagiarism Detection: Ensure generated content is original\n",
    "Fact Verification: Cross-check claims against source papers\n",
    "Readability Analysis: Ensure proposal meets target audience expectations\n",
    "Compliance Validation: Automated checks against all RFP requirements\n",
    "\n",
    "8. Output Enhancement\n",
    "For the final deliverables (step N), consider generating:\n",
    "\n",
    "Executive Summary: One-page overview for quick review\n",
    "Technical Appendix: Detailed methodology descriptions\n",
    "Budget Justification: Line-by-line cost explanations\n",
    "Risk Mitigation Plan: Addressing identified challenges\n",
    "Evaluation Metrics: How success will be measured\n",
    "\n",
    "9. Monitoring and Logging\n",
    "Add throughout the pipeline:\n",
    "\n",
    "Decision Logging: Track why papers were selected/rejected\n",
    "Agent Reasoning Traces: Understand how proposals were generated\n",
    "Performance Metrics: Time taken, resources used, quality scores\n",
    "Error Handling: Graceful degradation if components fail\n",
    "\n",
    "10. Advanced Features\n",
    "Consider these stretch goals:\n",
    "\n",
    "Multi-RFP Learning: Learn from multiple RFPs to improve over time\n",
    "Collaborative Filtering: If multiple users, learn from collective behavior\n",
    "Adaptive Prompting: Adjust prompts based on intermediate results\n",
    "Uncertainty Quantification: Flag areas where the system is less confident\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3cxtywSY4tq0",
    "ZG1Ah1eDi7aK",
    "cXHRa9IlMycZ"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
