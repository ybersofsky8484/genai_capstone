{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1942eb2c",
   "metadata": {},
   "source": [
    "\n",
    "# Notebook Map: Relevance Evaluation Pipeline with Few-Shot + Agentic Enhancements\n",
    "\n",
    "This table of contents provides a structured overview of the notebook, describing each section's purpose and how it fits into the workflow.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Quick Reference\n",
    "- Overview of semantic versioning, few-shot prompting, and agentic conflict resolver.\n",
    "\n",
    "## 2. Imports and Configuration\n",
    "- Load required libraries and define configuration constants (e.g., few-shot parameters, log paths).\n",
    "\n",
    "## 3. Core Utility Functions\n",
    "- `verify_decision`: Ensures model decisions are consistent.\n",
    "- `calculate_rule_confidence`: Computes rule-based confidence from criteria.\n",
    "- `get_next_prompt_version`: Auto-increments semantic prompt version.\n",
    "- `rebuild_few_shot_pool`: Builds balanced few-shot example set from log.\n",
    "- `agentic_conflict_resolver`: Resolves discrepancies between model and rule evaluations.\n",
    "\n",
    "## 4. Data Preparation\n",
    "- Load PDF research papers from `data/raw`.\n",
    "- Truncate text to fit LLM context window.\n",
    "\n",
    "## 5. Few-Shot Prompt Building\n",
    "- Retrieve high-confidence examples from log.\n",
    "- Prepend examples to base relevance prompt.\n",
    "\n",
    "## 6. Main Evaluation Loop\n",
    "- Iterate through PDFs.\n",
    "- Evaluate relevance using LLM.\n",
    "- Apply rule-based scoring and hybrid confidence calculation.\n",
    "- Flag documents for review when model vs. rule confidence diverges.\n",
    "\n",
    "## 7. Logging and Versioning\n",
    "- Append results to `prompt_evaluation_log.json`.\n",
    "- Add `prompt_version`, `decision_source`, and `agentic_resolution` where applicable.\n",
    "\n",
    "## 8. Visualization\n",
    "- Display confidence distribution, relevance drift, and flagged discrepancy trends.\n",
    "\n",
    "## 9. Enhancements (Appended)\n",
    "- Additional functions and logging improvements appended at the end for optional use.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRJ1_wsbi7cZ"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "<font size=10>**End-Term / Final Project**</font>\n",
    "\n",
    "<font size=6>**AI for Research Proposal Automation**</font>\n",
    "\n",
    "### **Business Problem - Create an AI system which will help you writing the research proposal aligning with the NOFO Document**\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsZ53h43336R"
   },
   "source": [
    "Meet Dr. Ian McCulloh, a seasoned research advisor and a leading voice in interdisciplinary science. Over the years, his lab has explored everything from AI for counterterrorism to social network analysis in neuroscience. His publication portfolio is vast, rich, and... chaotic.\n",
    "\n",
    "When the National Institute of Mental Health released a new NOFO (Notice of Funding Opportunity) seeking innovative digital health solutions for mental health equity, Dr. Ian saw an opportunity. But there was a problem: despite his extensive work, none of his existing research was directly aligned with digital mental health interventions. And with NIH deadlines looming, manually identifying relevant angles and generating a competitive proposal would be a massive lift.\n",
    "\n",
    "Dr. Ian wished for a smart assistant—one that could digest his past work, interpret the NOFO’s intent, spark new research directions, and even help draft proposal sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATYzMPf1333q"
   },
   "source": [
    "**The Challenge:**\n",
    "\n",
    "Organizations and researchers often maintain large archives of publications and prior work. When responding to competitive grants—especially highly specific ones like NIH NOFOs—it becomes extremely difficult and time-consuming to:\n",
    "\n",
    "1. Align past work with a new funding call.\n",
    "2. Extract relevant expertise from unrelated projects.\n",
    "3. Ideate novel, fundable research proposals tailored to complex criteria.\n",
    "4. Generate high-quality text for grant submission that satisfies technical and scientific review criteria.\n",
    "\n",
    "The manual effort to sift through dense research documents, match them to nuanced funding criteria, and write compelling, compliant proposals is labor-intensive, inconsistent, and prone to missed opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsDECO7z5eMJ"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "### **The Case Study Approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syX7mfDi5eb8"
   },
   "source": [
    "**Objective**\n",
    "1. Develop a generative AI-powered system using LLMs to automate and optimize the creation of NIH research proposals.\n",
    "2. The tool will identify relevant prior research, generate aligned project ideas, and draft high-quality proposal content tailored to specific NOFO requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given workflow:**\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Read NOFO Document] --> B[Analyze Research Papers]\n",
    "    B --> C[Filter Papers by Topic]\n",
    "    C --> D[Generate Research Ideas]\n",
    "    D --> E[Upload ideas to LLM]\n",
    "    E --> F[Generate Proposal]\n",
    "    F --> G[LLM Evaluation]\n",
    "    G --> H{Meets criteria?}\n",
    "    H -- NO --> F\n",
    "    H -- YES --> I[Human Review]\n",
    "    I --> J{Approved?}\n",
    "    J -- NO --> F\n",
    "    J -- YES --> K[Final Proposal]\n",
    "```\n",
    "\n",
    "**Enhanced workflow based on conversations with ChatGPT and Claude:**\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Read NOFO Document] --> B[Extract Key Requirements & Evaluation Criteria]\n",
    "    B --> C[Multi-Stage Paper Processing<br>(PyPDF → OCR)]\n",
    "    C --> C1[Table Extraction]\n",
    "    C --> C2[Figure Extraction (OCR + Captioning)]\n",
    "    C1 --> D\n",
    "    C2 --> D\n",
    "    D[Hybrid Indexing & Filtering<br>(BM25 + Embeddings + Metadata)]\n",
    "    D --> E[Agentic Research Synthesis<br>(Research Analyst + Proposal Writer + Compliance Checker)]\n",
    "    E --> F[Generate Proposal Blueprint + Draft]\n",
    "    F --> G[Multi-Criteria Evaluation<br/>(RAG + LLM-as-Judge + Guardrails)]\n",
    "    G --> H{Score ≥ Threshold?}\n",
    "    H -- NO --> I[Targeted Refinement Loop<br/>(Weakness-Specific Prompts)]\n",
    "    I --> F\n",
    "    H -- YES --> J[Caching + Checkpointing of Results]\n",
    "    J --> K[Human Review Interface]\n",
    "    K --> L{Approved?}\n",
    "    L -- NO --> M[Capture Feedback & Return to Refinement]\n",
    "    M --> F\n",
    "    L -- YES --> N[Final Proposal + Deliverables]\n",
    "    \n",
    "    subgraph \"Agentic Components\"\n",
    "        E1[Research Analyst Agent]\n",
    "        E2[Proposal Writer Agent]\n",
    "        E3[Compliance Checker Agent]\n",
    "        E1 <--> E2\n",
    "        E2 <--> E3\n",
    "        E3 <--> E1\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cxtywSY4tq0"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Setup - [2 Marks]**\n",
    "---\n",
    "<font color=Red>**Note:**</font> *1 marks is awarded for the Embedding Model configuration and 1 mark for the LLM Configuration.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "b52EI78ZiY1X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# @title Run this cell => Restart the session => Start executing the below cells **(DO NOT EXECUTE THIS CELL AGAIN)**\n",
    "\n",
    "!pip install -q langchain==0.3.21 \\\n",
    "                huggingface_hub==0.29.3 \\\n",
    "                openai==1.68.2 \\\n",
    "                chromadb==0.6.3 \\\n",
    "                langchain-community==0.3.20 \\\n",
    "                langchain_openai==0.3.10 \\\n",
    "                lark==1.2.2\\\n",
    "                rank_bm25==0.2.2\\\n",
    "                numpy==2.2.4 \\\n",
    "                scipy==1.15.2 \\\n",
    "                scikit-learn==1.6.1 \\\n",
    "                transformers==4.50.0 \\\n",
    "                pypdf==5.4.0 \\\n",
    "                markdown-pdf==1.7 \\\n",
    "                tiktoken==0.9.0 \\\n",
    "                sentence_transformers==4.0.0 \\\n",
    "                torch==2.6.0 \\\n",
    "                pypdf \\\n",
    "                camelot-py \\\n",
    "                opencv-python \\\n",
    "                pytesseract \\\n",
    "                pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r ../requirements.txt\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for core functionality\n",
    "import os\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "base_url = os.getenv(\"OPENAI_BASE_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for core functionality\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# @title Defining the LLM Model - Use `gpt-4o-mini` Model\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# FEW-SHOT AND LOGGING CONFIG\n",
    "# ------------------------------------------------------------\n",
    "# These constants control how many examples are retrieved and the minimum confidence threshold.\n",
    "# Modify here if you want more or fewer few-shot examples or to change the confidence cutoff.\n",
    "FEW_SHOT_MAX_EXAMPLES = 4         # Total examples (balanced between relevant/irrelevant if possible)\n",
    "# Minimum confidence threshold for including examples in few-shot prompting\n",
    "MIN_CONFIDENCE_FOR_FEWSHOT = 70   # Minimum hybrid confidence (%) to consider for few-shot retrieval\n",
    "\n",
    "# JSON log path\n",
    "# Path to the cleaned JSON log file where prompt evaluation iterations are stored\n",
    "LOG_PATH = \"prompt_evaluation_log_cleaned.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned PDF saved to: ../data/NOFO_cleaned.pdf\n",
      "Cleaning annotations for: cycon-final-draft.pdf\n",
      "Cleaned PDF saved to: data/raw/cycon-final-draft_cleaned.pdf\n",
      "Cleaning annotations for: Chat GPT Bias final w copyright.pdf\n",
      "Cleaned PDF saved to: data/raw/Chat GPT Bias final w copyright_cleaned.pdf\n",
      "Cleaning annotations for: Genetic_Algorithms_for_Prompt_Optimization.pdf\n",
      "Cleaned PDF saved to: data/raw/Genetic_Algorithms_for_Prompt_Optimization_cleaned.pdf\n",
      "Cleaning annotations for: DIVERSE_LLM_Dataset___IEEE_Big_Data.pdf\n",
      "Cleaned PDF saved to: data/raw/DIVERSE_LLM_Dataset___IEEE_Big_Data_cleaned.pdf\n",
      "Cleaning annotations for: Hashtag_Revival.pdf\n",
      "Cleaned PDF saved to: data/raw/Hashtag_Revival_cleaned.pdf\n",
      "Cleaning annotations for: FBI_Recruit_Hire_Final.pdf\n",
      "Cleaned PDF saved to: data/raw/FBI_Recruit_Hire_Final_cleaned.pdf\n",
      "Cleaning annotations for: Benson_MA491_NLP.pdf\n",
      "Cleaned PDF saved to: data/raw/Benson_MA491_NLP_cleaned.pdf\n",
      "Cleaning annotations for: Extreme Cohesion Darknet 20190815.pdf\n",
      "Cleaned PDF saved to: data/raw/Extreme Cohesion Darknet 20190815_cleaned.pdf\n",
      "Cleaning annotations for: Encyclopedia of SNA - R Packages.pdf\n",
      "Cleaned PDF saved to: data/raw/Encyclopedia of SNA - R Packages_cleaned.pdf\n",
      "Cleaning annotations for: RES2D.pdf\n",
      "Cleaned PDF saved to: data/raw/RES2D_cleaned.pdf\n",
      "Cleaning annotations for: ClassifiersCrowdSource.pdf\n",
      "Cleaned PDF saved to: data/raw/ClassifiersCrowdSource_cleaned.pdf\n",
      "Cleaning annotations for: FSS-19_paper_137.pdf\n",
      "Cleaned PDF saved to: data/raw/FSS-19_paper_137_cleaned.pdf\n",
      "Cleaning annotations for: Kidney_Behavioral.pdf\n",
      "Cleaned PDF saved to: data/raw/Kidney_Behavioral_cleaned.pdf\n",
      "Cleaning annotations for: Sim of Decon.pdf\n",
      "Cleaned PDF saved to: data/raw/Sim of Decon_cleaned.pdf\n",
      "Cleaning annotations for: Cross_Platform_Information_Spread_During_the_January_6th_Capitol_Riots.pdf\n",
      "Cleaned PDF saved to: data/raw/Cross_Platform_Information_Spread_During_the_January_6th_Capitol_Riots_cleaned.pdf\n",
      "Cleaning annotations for: Political_Networks_Conference.pdf\n",
      "Cleaned PDF saved to: data/raw/Political_Networks_Conference_cleaned.pdf\n",
      "Cleaning annotations for: On the Science of Networks.pdf\n",
      "Cleaned PDF saved to: data/raw/On the Science of Networks_cleaned.pdf\n",
      "Cleaning annotations for: Simmelian-Gamma-LDA.pdf\n",
      "Cleaned PDF saved to: data/raw/Simmelian-Gamma-LDA_cleaned.pdf\n",
      "Cleaning annotations for: BotBuster___AAAI.pdf\n",
      "Cleaned PDF saved to: data/raw/BotBuster___AAAI_cleaned.pdf\n",
      "Cleaning annotations for: Planning for AI Sustainment A Methodology for Maintenance and Cost Management_V5.pdf\n",
      "Cleaned PDF saved to: data/raw/Planning for AI Sustainment A Methodology for Maintenance and Cost Management_V5_cleaned.pdf\n",
      "Cleaning annotations for: Symbolic Generative AI 20231012.pdf\n",
      "Cleaned PDF saved to: data/raw/Symbolic Generative AI 20231012_cleaned.pdf\n",
      "Cleaning annotations for: 23-US-DHS-001.pdf\n",
      "Cleaned PDF saved to: data/raw/23-US-DHS-001_cleaned.pdf\n",
      "Cleaning annotations for: 2024_ICWSM_Data_Challenge__Post_API_Data_Collection.pdf\n",
      "Cleaned PDF saved to: data/raw/2024_ICWSM_Data_Challenge__Post_API_Data_Collection_cleaned.pdf\n",
      "Cleaning annotations for: Misinformation_Simulation.pdf\n",
      "Cleaned PDF saved to: data/raw/Misinformation_Simulation_cleaned.pdf\n",
      "Cleaning annotations for: Supply Chain Excellence.pdf\n",
      "Cleaned PDF saved to: data/raw/Supply Chain Excellence_cleaned.pdf\n",
      "Cleaning annotations for: 2021_EPJ_MVMCInfoOps.pdf\n",
      "Cleaned PDF saved to: data/raw/2021_EPJ_MVMCInfoOps_cleaned.pdf\n",
      "Cleaning annotations for: Network Simulation Models.pdf\n",
      "Cleaned PDF saved to: data/raw/Network Simulation Models_cleaned.pdf\n",
      "Cleaning annotations for: ALL18.pdf\n",
      "Cleaned PDF saved to: data/raw/ALL18_cleaned.pdf\n",
      "Cleaning annotations for: ONA-in-R.pdf\n",
      "Cleaned PDF saved to: data/raw/ONA-in-R_cleaned.pdf\n",
      "Cleaning annotations for: Helene_and_Milton_ACM.pdf\n",
      "Cleaned PDF saved to: data/raw/Helene_and_Milton_ACM_cleaned.pdf\n",
      "Cleaning annotations for: Unobtrusive Email.pdf\n",
      "Cleaned PDF saved to: data/raw/Unobtrusive Email_cleaned.pdf\n",
      "Cleaning annotations for: docnet.pdf\n",
      "Cleaned PDF saved to: data/raw/docnet_cleaned.pdf\n",
      "Cleaning annotations for: Utility Seeking in Complex Social Systems.pdf\n",
      "Cleaned PDF saved to: data/raw/Utility Seeking in Complex Social Systems_cleaned.pdf\n",
      "Cleaning annotations for: Lessons from Advising in Afghanistan.pdf\n",
      "Cleaned PDF saved to: data/raw/Lessons from Advising in Afghanistan_cleaned.pdf\n",
      "Cleaning annotations for: MOOC 20190828.pdf\n",
      "Cleaned PDF saved to: data/raw/MOOC 20190828_cleaned.pdf\n",
      "Cleaning annotations for: WEIRD.pdf\n",
      "Cleaned PDF saved to: data/raw/WEIRD_cleaned.pdf\n",
      "Cleaning annotations for: IkeNet.pdf\n",
      "Cleaned PDF saved to: data/raw/IkeNet_cleaned.pdf\n",
      "Cleaning annotations for: EmergencyResponseAI.pdf\n",
      "Cleaned PDF saved to: data/raw/EmergencyResponseAI_cleaned.pdf\n",
      "Cleaning annotations for: Quantifying_Information_Advantage.pdf\n",
      "Cleaned PDF saved to: data/raw/Quantifying_Information_Advantage_cleaned.pdf\n",
      "Cleaning annotations for: Confidence_Chaining.pdf\n",
      "Cleaned PDF saved to: data/raw/Confidence_Chaining_cleaned.pdf\n",
      "Cleaning annotations for: RatingsVRankings.pdf\n",
      "Cleaned PDF saved to: data/raw/RatingsVRankings_cleaned.pdf\n",
      "Cleaning annotations for: Analysis_of_Malware_Communities_Using_Multi_Modal_Features.pdf\n",
      "Cleaned PDF saved to: data/raw/Analysis_of_Malware_Communities_Using_Multi_Modal_Features_cleaned.pdf\n",
      "Cleaning annotations for: jfq-110_46-53_Cruickshank.pdf\n",
      "Cleaned PDF saved to: data/raw/jfq-110_46-53_Cruickshank_cleaned.pdf\n",
      "Cleaning annotations for: MLTEing_Models_for_NIER_at_ICSE_2023.pdf\n",
      "Cleaned PDF saved to: data/raw/MLTEing_Models_for_NIER_at_ICSE_2023_cleaned.pdf\n",
      "Cleaning annotations for: SocNetAlQaeda.pdf\n",
      "Cleaned PDF saved to: data/raw/SocNetAlQaeda_cleaned.pdf\n",
      "Cleaning annotations for: Leadership of Data Annotation 20180304v2.pdf\n",
      "Cleaned PDF saved to: data/raw/Leadership of Data Annotation 20180304v2_cleaned.pdf\n",
      "Cleaning annotations for: Parler_Disinformation_Challenge___CMOT_Extended.pdf\n",
      "Cleaned PDF saved to: data/raw/Parler_Disinformation_Challenge___CMOT_Extended_cleaned.pdf\n",
      "Cleaning annotations for: ICWSM_2025_Political_Bias.pdf\n",
      "Cleaned PDF saved to: data/raw/ICWSM_2025_Political_Bias_cleaned.pdf\n",
      "Cleaning annotations for: HIV.pdf\n",
      "Cleaned PDF saved to: data/raw/HIV_cleaned.pdf\n",
      "Cleaning annotations for: Limit Velocity.pdf\n",
      "Cleaned PDF saved to: data/raw/Limit Velocity_cleaned.pdf\n",
      "Cleaning annotations for: Spectral Analysis SNA.pdf\n",
      "Cleaned PDF saved to: data/raw/Spectral Analysis SNA_cleaned.pdf\n",
      "Cleaning annotations for: LLM_Confidence_Metrics.pdf\n",
      "Cleaned PDF saved to: data/raw/LLM_Confidence_Metrics_cleaned.pdf\n",
      "Cleaning annotations for: Food Addiction 20231222 v3.pdf\n",
      "Cleaned PDF saved to: data/raw/Food Addiction 20231222 v3_cleaned.pdf\n",
      "Cleaning annotations for: NBA Performance.pdf\n",
      "Cleaned PDF saved to: data/raw/NBA Performance_cleaned.pdf\n",
      "Cleaning annotations for: MIPB-CDA.pdf\n",
      "Cleaned PDF saved to: data/raw/MIPB-CDA_cleaned.pdf\n",
      "Cleaning annotations for: Vol33Iss1_INSNApdf.pdf\n",
      "Cleaned PDF saved to: data/raw/Vol33Iss1_INSNApdf_cleaned.pdf\n",
      "Cleaning annotations for: NeuroCogInfluence.pdf\n",
      "Cleaned PDF saved to: data/raw/NeuroCogInfluence_cleaned.pdf\n",
      "Cleaning annotations for: Frontiers COVID.pdf\n",
      "Cleaned PDF saved to: data/raw/Frontiers COVID_cleaned.pdf\n",
      "Cleaning annotations for: Evolution_of_Terrorism_PNAS.pdf\n",
      "Cleaned PDF saved to: data/raw/Evolution_of_Terrorism_PNAS_cleaned.pdf\n",
      "Cleaning annotations for: Text Analysis Using Automated Language Translators.pdf\n",
      "Cleaned PDF saved to: data/raw/Text Analysis Using Automated Language Translators_cleaned.pdf\n",
      "Cleaning annotations for: Vulnerable_Code_Detection.pdf\n",
      "Cleaned PDF saved to: data/raw/Vulnerable_Code_Detection_cleaned.pdf\n",
      "Cleaning annotations for: Dormant Bots 20190814.pdf\n",
      "Cleaned PDF saved to: data/raw/Dormant Bots 20190814_cleaned.pdf\n",
      "Cleaning annotations for: NAP Behavioral Sci Intel.pdf\n",
      "Cleaned PDF saved to: data/raw/NAP Behavioral Sci Intel_cleaned.pdf\n",
      "Cleaning annotations for: YouTube-COVID.pdf\n",
      "Cleaned PDF saved to: data/raw/YouTube-COVID_cleaned.pdf\n",
      "Cleaning annotations for: Organizational risk using network analysis.pdf\n",
      "Cleaned PDF saved to: data/raw/Organizational risk using network analysis_cleaned.pdf\n",
      "Cleaning annotations for: k-truss.pdf\n",
      "Cleaned PDF saved to: data/raw/k-truss_cleaned.pdf\n",
      "Cleaning annotations for: Political Party Cohesion.pdf\n",
      "Cleaned PDF saved to: data/raw/Political Party Cohesion_cleaned.pdf\n",
      "Cleaning annotations for: Sailer McCulloh Soc Net and Spatial Config.pdf\n",
      "Cleaned PDF saved to: data/raw/Sailer McCulloh Soc Net and Spatial Config_cleaned.pdf\n",
      "Cleaning annotations for: CUSUM Parameterization.pdf\n",
      "Cleaned PDF saved to: data/raw/CUSUM Parameterization_cleaned.pdf\n",
      "Cleaning annotations for: Multi_view_Clustering_for_Social_Based_Data.pdf\n",
      "Cleaned PDF saved to: data/raw/Multi_view_Clustering_for_Social_Based_Data_cleaned.pdf\n",
      "Cleaning annotations for: ONA-using-igraph.pdf\n",
      "Cleaned PDF saved to: data/raw/ONA-using-igraph_cleaned.pdf\n",
      "Cleaning annotations for: improving-decision-support-for-organ-transplant.pdf\n",
      "Cleaned PDF saved to: data/raw/improving-decision-support-for-organ-transplant_cleaned.pdf\n",
      "Cleaning annotations for: Tweets-to-touchdowns.pdf\n",
      "Cleaned PDF saved to: data/raw/Tweets-to-touchdowns_cleaned.pdf\n",
      "Cleaning annotations for: Social_Det_COVID_Mortality.pdf\n",
      "Cleaned PDF saved to: data/raw/Social_Det_COVID_Mortality_cleaned.pdf\n",
      "Cleaning annotations for: White Paper Brain Gaze.pdf\n",
      "Cleaned PDF saved to: data/raw/White Paper Brain Gaze_cleaned.pdf\n",
      "Cleaning annotations for: McCullohCarleyJOSS.pdf\n",
      "Cleaned PDF saved to: data/raw/McCullohCarleyJOSS_cleaned.pdf\n",
      "Cleaning annotations for: Course Info Security.pdf\n",
      "Cleaned PDF saved to: data/raw/Course Info Security_cleaned.pdf\n",
      "Cleaning annotations for: A_Complex_Network_Approach_to_Find_Latent_Terorrist_Communities.pdf\n",
      "Cleaned PDF saved to: data/raw/A_Complex_Network_Approach_to_Find_Latent_Terorrist_Communities_cleaned.pdf\n",
      "Cleaning annotations for: Knowing the Terrain.pdf\n",
      "Cleaned PDF saved to: data/raw/Knowing the Terrain_cleaned.pdf\n",
      "Cleaning annotations for: Social_Network_Probability_Mechanics.pdf\n",
      "Cleaned PDF saved to: data/raw/Social_Network_Probability_Mechanics_cleaned.pdf\n",
      "Cleaning annotations for: The ABCs of AI-Enabled Intelligence Analysis - War on the Rocks.pdf\n",
      "Cleaned PDF saved to: data/raw/The ABCs of AI-Enabled Intelligence Analysis - War on the Rocks_cleaned.pdf\n",
      "Cleaning annotations for: SocNetChgDet.pdf\n",
      "Cleaned PDF saved to: data/raw/SocNetChgDet_cleaned.pdf\n",
      "Cleaning annotations for: Take_boards.pdf\n",
      "Cleaned PDF saved to: data/raw/Take_boards_cleaned.pdf\n",
      "Cleaning annotations for: Arrow White Paper DExTra.pdf\n",
      "Cleaned PDF saved to: data/raw/Arrow White Paper DExTra_cleaned.pdf\n",
      "Cleaning annotations for: TrainingSetSize.pdf\n",
      "Cleaned PDF saved to: data/raw/TrainingSetSize_cleaned.pdf\n",
      "Cleaning annotations for: CausalOrgInorgContent.pdf\n",
      "Cleaned PDF saved to: data/raw/CausalOrgInorgContent_cleaned.pdf\n",
      "Cleaning annotations for: Overcoming_Social_Media_API_Restrictions__Building_an_Effective_Web_Scraper.pdf\n",
      "Cleaned PDF saved to: data/raw/Overcoming_Social_Media_API_Restrictions__Building_an_Effective_Web_Scraper_cleaned.pdf\n",
      "Cleaning annotations for: LLM_UQ.pdf\n",
      "Cleaned PDF saved to: data/raw/LLM_UQ_cleaned.pdf\n",
      "Cleaning annotations for: Kent2022_Chapter_MicroscopicMarkovChainApproach.pdf\n",
      "Cleaned PDF saved to: data/raw/Kent2022_Chapter_MicroscopicMarkovChainApproach_cleaned.pdf\n",
      "Cleaning annotations for: Acquiring Maintainable AI_Enable Systems_Final.pdf\n",
      "Cleaned PDF saved to: data/raw/Acquiring Maintainable AI_Enable Systems_Final_cleaned.pdf\n",
      "Cleaning annotations for: Characterizing_Communities_of_Hashtag_Usage_on_Twitter_During_the_2020_COVID_19_Pandemic.pdf\n",
      "Cleaned PDF saved to: data/raw/Characterizing_Communities_of_Hashtag_Usage_on_Twitter_During_the_2020_COVID_19_Pandemic_cleaned.pdf\n",
      "Cleaning annotations for: SecurityPrivAIML.pdf\n",
      "Cleaned PDF saved to: data/raw/SecurityPrivAIML_cleaned.pdf\n",
      "Cleaning annotations for: AAAI IAA CV.pdf\n",
      "Cleaned PDF saved to: data/raw/AAAI IAA CV_cleaned.pdf\n",
      "Cleaning annotations for: LSA email.pdf\n",
      "Cleaned PDF saved to: data/raw/LSA email_cleaned.pdf\n",
      "Cleaning annotations for: Clustering_Analysis_of_Website_Usage_on_Twitter_during_the_COVID_19_Pandemic.pdf\n",
      "Cleaned PDF saved to: data/raw/Clustering_Analysis_of_Website_Usage_on_Twitter_during_the_COVID_19_Pandemic_cleaned.pdf\n",
      "Cleaning annotations for: Designed Networks.pdf\n",
      "Cleaned PDF saved to: data/raw/Designed Networks_cleaned.pdf\n",
      "Cleaning annotations for: COVID Bayesian Data Aug.pdf\n",
      "Cleaned PDF saved to: data/raw/COVID Bayesian Data Aug_cleaned.pdf\n",
      "Cleaning annotations for: Cohort_Optimization_Methods_SNAMS_2021_working_draft (4).pdf\n",
      "Cleaned PDF saved to: data/raw/Cohort_Optimization_Methods_SNAMS_2021_working_draft (4)_cleaned.pdf\n",
      "Cleaning annotations for: SM Customer Feedback_FAB_2019_rev3.pdf\n",
      "Cleaned PDF saved to: data/raw/SM Customer Feedback_FAB_2019_rev3_cleaned.pdf\n",
      "Cleaning annotations for: IkekNet1.pdf\n",
      "Cleaned PDF saved to: data/raw/IkekNet1_cleaned.pdf\n",
      "Cleaning annotations for: Social Media Mental Health Final.pdf\n",
      "Cleaned PDF saved to: data/raw/Social Media Mental Health Final_cleaned.pdf\n",
      "Cleaning annotations for: Reforming Sectarian Beliefs.pdf\n",
      "Cleaned PDF saved to: data/raw/Reforming Sectarian Beliefs_cleaned.pdf\n",
      "Cleaning annotations for: Savas.pdf\n",
      "Cleaned PDF saved to: data/raw/Savas_cleaned.pdf\n",
      "Cleaning annotations for: Data_Education__Emerging_Challenges_and_Opportunities.pdf\n",
      "Cleaned PDF saved to: data/raw/Data_Education__Emerging_Challenges_and_Opportunities_cleaned.pdf\n",
      "Cleaning annotations for: Dissertation.pdf\n",
      "Cleaned PDF saved to: data/raw/Dissertation_cleaned.pdf\n",
      "Cleaning annotations for: Leveraging_AI_to_Improve_Viral_Information_Detection_in_Online_Discourse.pdf\n",
      "Cleaned PDF saved to: data/raw/Leveraging_AI_to_Improve_Viral_Information_Detection_in_Online_Discourse_cleaned.pdf\n",
      "Cleaning annotations for: ICWSM___Use_of_Large_Language_Models_for_Stance_Classification.pdf\n",
      "Cleaned PDF saved to: data/raw/ICWSM___Use_of_Large_Language_Models_for_Stance_Classification_cleaned.pdf\n",
      "Cleaning annotations for: NeuroSynchrony.pdf\n",
      "Cleaned PDF saved to: data/raw/NeuroSynchrony_cleaned.pdf\n",
      "Cleaning annotations for: Multi_Agent_Systems_for_Frame_Detection.pdf\n",
      "Cleaned PDF saved to: data/raw/Multi_Agent_Systems_for_Frame_Detection_cleaned.pdf\n",
      "Cleaning annotations for: Review of R Packages_20161026.pdf\n",
      "Cleaned PDF saved to: data/raw/Review of R Packages_20161026_cleaned.pdf\n",
      "Cleaning annotations for: Lead-Azide.pdf\n",
      "Cleaned PDF saved to: data/raw/Lead-Azide_cleaned.pdf\n",
      "Cleaning annotations for: LongNetViewerORA.pdf\n",
      "Cleaned PDF saved to: data/raw/LongNetViewerORA_cleaned.pdf\n",
      "All research PDFs cleaned and saved in data/raw/\n"
     ]
    }
   ],
   "source": [
    "# PDF Cleaning Step: Remove non-visual annotations (comments, links, form fields)\n",
    "# Keeps images, diagrams, and visible callouts intact\n",
    "\n",
    "# Import required libraries for core functionality\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def clean_pdf_annotations(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Strips non-visual annotations (comments, form fields, links) from a PDF\n",
    "    while preserving visible images and diagrams.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(input_path)\n",
    "\n",
    "    for page in doc:\n",
    "        # Iterate over all annotations (not images)\n",
    "        annot = page.first_annot\n",
    "        while annot:\n",
    "            next_annot = annot.next  # store reference to next annotation\n",
    "            # Remove annotation object (highlights, comments, links)\n",
    "            page.delete_annot(annot)\n",
    "            annot = next_annot\n",
    "\n",
    "    # Save cleaned PDF\n",
    "    doc.save(output_path, garbage=4, deflate=True)\n",
    "    doc.close()\n",
    "\n",
    "# Clean NOFO file\n",
    "input_pdf = \"../data/NOFO.pdf\"\n",
    "cleaned_pdf = \"../data/NOFO_cleaned.pdf\"\n",
    "clean_pdf_annotations(input_pdf, cleaned_pdf)\n",
    "print(f\"Cleaned PDF saved to: {cleaned_pdf}\")\n",
    "\n",
    "# Get de-annotated NOFO doc content using PyPDFLoader for evaluation step\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "pdf_file = \"../data/NOFO_cleaned.pdf\"\n",
    "pdf_loader = PyPDFLoader(pdf_file);\n",
    "NOFO_pdf = pdf_loader.load()\n",
    "\n",
    "# Prepare output folder for de-annotated research papers\n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "\n",
    "# Set variables for de-annotating the research paper PDF collection \n",
    "source_dir = \"../content\"\n",
    "output_dir = \"data/raw\"\n",
    "\n",
    "# Loop through content folder, de-annotate each PDF, and save to a 'clean' output directory\n",
    "for file_name in os.listdir(source_dir):\n",
    "    if file_name.lower().endswith(\".pdf\"):\n",
    "        input_pdf = os.path.join(source_dir, file_name)\n",
    "        cleaned_pdf = os.path.join(output_dir, file_name.replace(\".pdf\", \"_cleaned.pdf\"))\n",
    "        print(f\"Cleaning annotations for: {file_name}\")\n",
    "        clean_pdf_annotations(input_pdf, cleaned_pdf)\n",
    "        print(f\"Cleaned PDF saved to: {cleaned_pdf}\")\n",
    "print(\"All research PDFs cleaned and saved in data/raw/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split cleaned text into 3000-token chunks with overlap for RAG.\n",
    "\n",
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "def chunk_text(text, chunk_size=3000, overlap=200):\n",
    "    tokens = encoding.encode(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        chunk_tokens = tokens[i:i+chunk_size]\n",
    "        chunks.append(encoding.decode(chunk_tokens))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean extracted text by:\n",
    "# - removing headers/footers\n",
    "# - removing noise\n",
    "# - fixing multi-column layout issues\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_extracted_text(text):\n",
    "    \"\"\"Remove noise (page numbers, headers, footers), merge hyphenated words,\n",
    "    and flatten potential two-column layouts.\"\"\"\n",
    "    # Remove common noise patterns\n",
    "    text = re.sub(r'\\bPage \\d+\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\d+ of \\d+', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove repeated headers/footers (heuristic: lines repeated >3 times)\n",
    "    lines = text.split('\\n')\n",
    "    freq = {}\n",
    "    for line in lines:\n",
    "        freq[line] = freq.get(line, 0) + 1\n",
    "    lines = [line for line in lines if freq[line] <= 3]\n",
    "\n",
    "    # Merge hyphenated words and normalize whitespace\n",
    "    text = re.sub(r'-\\n', '', '\\n'.join(lines))\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "    # Merge two-column text by pairing lines\n",
    "    merged_lines = []\n",
    "    lines = text.split('\\n')\n",
    "    for i in range(0, len(lines), 2):\n",
    "        if i+1 < len(lines):\n",
    "            merged_lines.append(lines[i] + \" \" + lines[i+1])\n",
    "        else:\n",
    "            merged_lines.append(lines[i])\n",
    "    return \"\\n\".join(merged_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract PDF text from cleaned research paper pdfs\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def process_pdf_multistage(file_path):\n",
    "    content = \"\"\n",
    "\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text() or \"\"\n",
    "            content += page_text\n",
    "    except Exception as e:\n",
    "        print(f\"PyPDF extraction failed: {e}\")\n",
    "\n",
    "    # Clean text\n",
    "    cleaned_text = clean_extracted_text(content)\n",
    "\n",
    "    # Chunk text\n",
    "    chunks = chunk_text(cleaned_text)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: data/raw/AAAI IAA CV_cleaned.pdf\n",
      "Processing: data/raw/Sim of Decon_cleaned.pdf\n",
      "Processing: data/raw/BotBuster___AAAI_cleaned.pdf\n",
      "Processing: data/raw/Political_Networks_Conference_cleaned.pdf\n",
      "Processing: data/raw/EmergencyResponseAI_cleaned.pdf\n",
      "Processing: data/raw/FSS-19_paper_137_cleaned.pdf\n",
      "Processing: data/raw/DIVERSE_LLM_Dataset___IEEE_Big_Data_cleaned.pdf\n",
      "Processing: data/raw/Clustering_Analysis_of_Website_Usage_on_Twitter_during_the_COVID_19_Pandemic_cleaned.pdf\n",
      "Processing: data/raw/Cohort_Optimization_Methods_SNAMS_2021_working_draft (4)_cleaned.pdf\n",
      "Processing: data/raw/Lead-Azide_cleaned.pdf\n",
      "Processing: data/raw/Knowing the Terrain_cleaned.pdf\n",
      "Processing: data/raw/Leadership of Data Annotation 20180304v2_cleaned.pdf\n",
      "Processing: data/raw/A_Complex_Network_Approach_to_Find_Latent_Terorrist_Communities_cleaned.pdf\n",
      "Processing: data/raw/Designed Networks_cleaned.pdf\n",
      "Processing: data/raw/Organizational risk using network analysis_cleaned.pdf\n",
      "Processing: data/raw/LongNetViewerORA_cleaned.pdf\n",
      "Processing: data/raw/Unobtrusive Email_cleaned.pdf\n",
      "Processing: data/raw/SM Customer Feedback_FAB_2019_rev3_cleaned.pdf\n",
      "Processing: data/raw/Benson_MA491_NLP_cleaned.pdf\n",
      "Processing: data/raw/SecurityPrivAIML_cleaned.pdf\n",
      "Processing: data/raw/SocNetAlQaeda_cleaned.pdf\n",
      "Processing: data/raw/COVID Bayesian Data Aug_cleaned.pdf\n",
      "Processing: data/raw/Savas_cleaned.pdf\n",
      "Processing: data/raw/Lessons from Advising in Afghanistan_cleaned.pdf\n",
      "Processing: data/raw/Confidence_Chaining_cleaned.pdf\n",
      "Processing: data/raw/Limit Velocity_cleaned.pdf\n",
      "Processing: data/raw/Text Analysis Using Automated Language Translators_cleaned.pdf\n",
      "Processing: data/raw/Multi_Agent_Systems_for_Frame_Detection_cleaned.pdf\n",
      "Processing: data/raw/IkekNet1_cleaned.pdf\n",
      "Processing: data/raw/Dissertation_cleaned.pdf\n",
      "Processing: data/raw/LLM_UQ_cleaned.pdf\n",
      "Processing: data/raw/Parler_Disinformation_Challenge___CMOT_Extended_cleaned.pdf\n",
      "Processing: data/raw/LSA email_cleaned.pdf\n",
      "Processing: data/raw/Political Party Cohesion_cleaned.pdf\n",
      "Processing: data/raw/Spectral Analysis SNA_cleaned.pdf\n",
      "Processing: data/raw/NeuroCogInfluence_cleaned.pdf\n",
      "Processing: data/raw/Simmelian-Gamma-LDA_cleaned.pdf\n",
      "Processing: data/raw/MIPB-CDA_cleaned.pdf\n",
      "Processing: data/raw/CausalOrgInorgContent_cleaned.pdf\n",
      "Processing: data/raw/Genetic_Algorithms_for_Prompt_Optimization_cleaned.pdf\n",
      "Processing: data/raw/ALL18_cleaned.pdf\n",
      "Processing: data/raw/LLM_Confidence_Metrics_cleaned.pdf\n",
      "Processing: data/raw/FBI_Recruit_Hire_Final_cleaned.pdf\n",
      "Processing: data/raw/ICWSM_2025_Political_Bias_cleaned.pdf\n",
      "Processing: data/raw/Review of R Packages_20161026_cleaned.pdf\n",
      "Processing: data/raw/Evolution_of_Terrorism_PNAS_cleaned.pdf\n",
      "Processing: data/raw/Planning for AI Sustainment A Methodology for Maintenance and Cost Management_V5_cleaned.pdf\n",
      "Processing: data/raw/Social Media Mental Health Final_cleaned.pdf\n",
      "Processing: data/raw/Cross_Platform_Information_Spread_During_the_January_6th_Capitol_Riots_cleaned.pdf\n",
      "Processing: data/raw/Multi_view_Clustering_for_Social_Based_Data_cleaned.pdf\n",
      "Processing: data/raw/Extreme Cohesion Darknet 20190815_cleaned.pdf\n",
      "Processing: data/raw/Social_Det_COVID_Mortality_cleaned.pdf\n",
      "Processing: data/raw/White Paper Brain Gaze_cleaned.pdf\n",
      "Processing: data/raw/Encyclopedia of SNA - R Packages_cleaned.pdf\n",
      "Processing: data/raw/CUSUM Parameterization_cleaned.pdf\n",
      "Processing: data/raw/Tweets-to-touchdowns_cleaned.pdf\n",
      "Processing: data/raw/SocNetChgDet_cleaned.pdf\n",
      "Processing: data/raw/23-US-DHS-001_cleaned.pdf\n",
      "Processing: data/raw/Analysis_of_Malware_Communities_Using_Multi_Modal_Features_cleaned.pdf\n",
      "Processing: data/raw/Acquiring Maintainable AI_Enable Systems_Final_cleaned.pdf\n",
      "Processing: data/raw/Food Addiction 20231222 v3_cleaned.pdf\n",
      "Processing: data/raw/HIV_cleaned.pdf\n",
      "Processing: data/raw/TrainingSetSize_cleaned.pdf\n",
      "Processing: data/raw/WEIRD_cleaned.pdf\n",
      "Processing: data/raw/Dormant Bots 20190814_cleaned.pdf\n",
      "Processing: data/raw/The ABCs of AI-Enabled Intelligence Analysis - War on the Rocks_cleaned.pdf\n",
      "Processing: data/raw/improving-decision-support-for-organ-transplant_cleaned.pdf\n",
      "Processing: data/raw/Leveraging_AI_to_Improve_Viral_Information_Detection_in_Online_Discourse_cleaned.pdf\n",
      "Processing: data/raw/Vulnerable_Code_Detection_cleaned.pdf\n",
      "Processing: data/raw/Overcoming_Social_Media_API_Restrictions__Building_an_Effective_Web_Scraper_cleaned.pdf\n",
      "Processing: data/raw/Supply Chain Excellence_cleaned.pdf\n",
      "Processing: data/raw/Reforming Sectarian Beliefs_cleaned.pdf\n",
      "Processing: data/raw/ICWSM___Use_of_Large_Language_Models_for_Stance_Classification_cleaned.pdf\n",
      "Processing: data/raw/Course Info Security_cleaned.pdf\n",
      "Processing: data/raw/Take_boards_cleaned.pdf\n",
      "Processing: data/raw/Hashtag_Revival_cleaned.pdf\n",
      "Processing: data/raw/Misinformation_Simulation_cleaned.pdf\n",
      "Processing: data/raw/Network Simulation Models_cleaned.pdf\n",
      "Processing: data/raw/cycon-final-draft_cleaned.pdf\n",
      "Processing: data/raw/NAP Behavioral Sci Intel_cleaned.pdf\n",
      "Processing: data/raw/Utility Seeking in Complex Social Systems_cleaned.pdf\n",
      "Processing: data/raw/k-truss_cleaned.pdf\n",
      "Processing: data/raw/Arrow White Paper DExTra_cleaned.pdf\n",
      "Processing: data/raw/Helene_and_Milton_ACM_cleaned.pdf\n",
      "Processing: data/raw/Data_Education__Emerging_Challenges_and_Opportunities_cleaned.pdf\n",
      "Processing: data/raw/RatingsVRankings_cleaned.pdf\n",
      "Processing: data/raw/RES2D_cleaned.pdf\n",
      "Processing: data/raw/McCullohCarleyJOSS_cleaned.pdf\n",
      "Processing: data/raw/ONA-using-igraph_cleaned.pdf\n",
      "Processing: data/raw/Vol33Iss1_INSNApdf_cleaned.pdf\n",
      "Processing: data/raw/Kidney_Behavioral_cleaned.pdf\n",
      "Processing: data/raw/ClassifiersCrowdSource_cleaned.pdf\n",
      "Processing: data/raw/MLTEing_Models_for_NIER_at_ICSE_2023_cleaned.pdf\n",
      "Processing: data/raw/Quantifying_Information_Advantage_cleaned.pdf\n",
      "Processing: data/raw/NeuroSynchrony_cleaned.pdf\n",
      "Processing: data/raw/Frontiers COVID_cleaned.pdf\n",
      "Processing: data/raw/Kent2022_Chapter_MicroscopicMarkovChainApproach_cleaned.pdf\n",
      "Processing: data/raw/jfq-110_46-53_Cruickshank_cleaned.pdf\n",
      "Processing: data/raw/Sailer McCulloh Soc Net and Spatial Config_cleaned.pdf\n",
      "Processing: data/raw/MOOC 20190828_cleaned.pdf\n",
      "Processing: data/raw/2021_EPJ_MVMCInfoOps_cleaned.pdf\n",
      "Processing: data/raw/ONA-in-R_cleaned.pdf\n",
      "Processing: data/raw/docnet_cleaned.pdf\n",
      "Processing: data/raw/Characterizing_Communities_of_Hashtag_Usage_on_Twitter_During_the_2020_COVID_19_Pandemic_cleaned.pdf\n",
      "Processing: data/raw/Symbolic Generative AI 20231012_cleaned.pdf\n",
      "Processing: data/raw/NBA Performance_cleaned.pdf\n",
      "Processing: data/raw/Chat GPT Bias final w copyright_cleaned.pdf\n",
      "Processing: data/raw/IkeNet_cleaned.pdf\n",
      "Processing: data/raw/On the Science of Networks_cleaned.pdf\n",
      "Processing: data/raw/Social_Network_Probability_Mechanics_cleaned.pdf\n",
      "Processing: data/raw/2024_ICWSM_Data_Challenge__Post_API_Data_Collection_cleaned.pdf\n",
      "Processing: data/raw/YouTube-COVID_cleaned.pdf\n",
      "Saved cleaned + chunked text for 112 PDFs to data/cleaned_chunked_papers.json\n"
     ]
    }
   ],
   "source": [
    "# Extract, clean, chunk, and store raw chunks for all research paper PDFs\n",
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "# Folder containing PDFs\n",
    "pdf_folder = \"data/raw\"\n",
    "\n",
    "# Output JSON file\n",
    "output_json_path = \"data/cleaned_chunked_papers.json\"\n",
    "\n",
    "# List to store results (each as a dict with id + chunks)\n",
    "all_chunks = []\n",
    "\n",
    "# Loop through all PDF files in the folder\n",
    "for pdf_path in glob(os.path.join(pdf_folder, \"*.pdf\")):\n",
    "    doc_name = os.path.basename(pdf_path)  # Use filename as ID\n",
    "    print(f\"Processing: {pdf_path}\")\n",
    "    try:\n",
    "        chunks = process_pdf_multistage(pdf_path)  # uses your existing function\n",
    "        all_chunks.append({\n",
    "            \"id\": doc_name,\n",
    "            \"chunks\": chunks\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {e}\")\n",
    "\n",
    "# Save results to JSON\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved cleaned + chunked text for {len(all_chunks)} PDFs to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add post-extraction GPT-enabled noise removal step\n",
    "\n",
    "# Too resource intensive for full data set. Add later if needed.\n",
    "\n",
    "# import json\n",
    "# from openai import OpenAI\n",
    "\n",
    "# # Initialize OpenAI client\n",
    "# client = OpenAI()\n",
    "\n",
    "# def semantic_clean_text(raw_text):\n",
    "#     prompt = f\"\"\"\n",
    "# You are a document cleaner. Extract ONLY the main body text from the following academic or technical document:\n",
    "# - Remove page numbers, headers/footers\n",
    "# - Remove title page, author affiliations, figure/table captions\n",
    "# - Remove references/bibliography sections\n",
    "# - Keep abstracts, introductions, main sections, and conclusions\n",
    "\n",
    "# Document:\n",
    "# \\\"\\\"\\\"{raw_text}\\\"\\\"\\\"\n",
    "\n",
    "# Return only the cleaned text.\n",
    "# \"\"\"\n",
    "#     response = client.responses.create(\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         input=prompt,\n",
    "#         max_output_tokens=4000\n",
    "#     )\n",
    "#     return response.output_text\n",
    "\n",
    "# # --- Ingest cleaned + chunked data and post-process with GPT ---\n",
    "# input_path = \"data/cleaned_chunked_papers.json\"\n",
    "# output_path = \"data/cleaned_gpt.json\"\n",
    "\n",
    "# # Load chunked data\n",
    "# with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     chunked_data = json.load(f)\n",
    "\n",
    "# # Prepare list for GPT-processed results\n",
    "# gpt_cleaned_data = []\n",
    "\n",
    "# # Loop through each document\n",
    "# for record in chunked_data:\n",
    "#     doc_id = record[\"id\"]\n",
    "#     gpt_chunks = []\n",
    "\n",
    "#     print(f\"Post-processing (GPT cleanup): {doc_id}\")\n",
    "\n",
    "#     # Apply GPT cleaning to each chunk\n",
    "#     for chunk in record[\"chunks\"]:\n",
    "#         cleaned_chunk = semantic_clean_text(chunk)\n",
    "#         gpt_chunks.append(cleaned_chunk)\n",
    "\n",
    "#     # Store result\n",
    "#     gpt_cleaned_data.append({\n",
    "#         \"id\": doc_id,\n",
    "#         \"chunks\": gpt_chunks\n",
    "#     })\n",
    "\n",
    "# # Save GPT-cleaned data\n",
    "# with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(gpt_cleaned_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# print(f\"Saved GPT post-processed chunks to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Chunk Scoring: BM25 + Cosine Similarity ---\n",
    "# Hybrid scoring method combining BM25 ranking and embedding similarity.\n",
    "\n",
    "# Placeholder: implement BM25 for tokenized chunks\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def score_chunks(query, chunks):\n",
    "    # BM25 scoring\n",
    "    tokenized_chunks = [chunk.split() for chunk in chunks]\n",
    "    bm25 = BM25Okapi(tokenized_chunks)\n",
    "    bm25_scores = bm25.get_scores(query.split())\n",
    "\n",
    "    # Embedding cosine similarity\n",
    "    query_emb = model.encode([query])[0]\n",
    "    chunk_embs = model.encode(chunks)\n",
    "    cosine_scores = [np.dot(query_emb, ce) / (np.linalg.norm(query_emb) * np.linalg.norm(ce))\n",
    "                     for ce in chunk_embs]\n",
    "\n",
    "    # Combine (simple average or weighted)\n",
    "    combined_scores = [(bm + cos) / 2 for bm, cos in zip(bm25_scores, cosine_scores)]\n",
    "    return combined_scores\n",
    "\n",
    "\n",
    "# --- Retrieval of Relevant Chunks ---\n",
    "# Fetch top-N chunks by hybrid score for downstream RAG generation.\n",
    "\n",
    "# def retrieve_relevant_chunks(query, chunks, top_k=5):\n",
    "#     scores = score_chunks(query, chunks)\n",
    "#     ranked = sorted(zip(chunks, scores), key=lambda x: x[1], reverse=True)\n",
    "# #     return ranked[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOFO Match Evaluation Log Normaizing Function\n",
    "\n",
    "def clean_json_log(json_path):\n",
    "    \"\"\"\n",
    "    Cleans and normalizes older log entries for consistent structure:\n",
    "    - Ensures relevant docs have confidence and discrepancy fields\n",
    "    - Wraps irrelevant docs as list of titles (if stored as raw strings)\n",
    "    \"\"\"\n",
    "# Import required libraries for core functionality\n",
    "    import os, json\n",
    "    if not os.path.exists(json_path):\n",
    "        return\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Warning: JSON log corrupted. Starting fresh.\")\n",
    "            return\n",
    "\n",
    "    updated = False\n",
    "    for iteration in data:\n",
    "        # Normalize relevant_documents\n",
    "        normalized_relevant = []\n",
    "        for doc in iteration.get(\"relevant_documents\", []):\n",
    "            # Convert string to dict if old format\n",
    "            if isinstance(doc, str):\n",
    "                normalized_relevant.append({\n",
    "                    \"title\": doc,\n",
    "                    \"reasoning\": \"\",\n",
    "                    \"model_confidence\": 0,\n",
    "                    \"rule_confidence\": 0,\n",
    "                    \"confidence_discrepancy\": 0,\n",
    "                    \"flagged_for_review\": False\n",
    "                })\n",
    "                updated = True\n",
    "            else:\n",
    "                # Ensure all expected fields exist\n",
    "                doc.setdefault(\"model_confidence\", 0)\n",
    "                doc.setdefault(\"rule_confidence\", 0)\n",
    "                doc.setdefault(\"confidence_discrepancy\", 0)\n",
    "                doc.setdefault(\"flagged_for_review\", False)\n",
    "                normalized_relevant.append(doc)\n",
    "\n",
    "        iteration[\"relevant_documents\"] = normalized_relevant\n",
    "\n",
    "        # Normalize irrelevant_documents to always be list of strings\n",
    "        if isinstance(iteration.get(\"irrelevant_documents\"), dict):\n",
    "            iteration[\"irrelevant_documents\"] = list(iteration[\"irrelevant_documents\"].keys())\n",
    "            updated = True\n",
    "\n",
    "    # Save back only if modifications were made\n",
    "    if updated:\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        print(\"JSON log cleaned and normalized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4f08eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"prompt_evaluation_log_cleaned.json\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# FEW-SHOT FALLBACK EXAMPLES\n",
    "# ------------------------------------------------------------\n",
    "FALLBACK_EXAMPLES = [\n",
    "    (\n",
    "        \"Digital CBT for Adolescents\",\n",
    "        \"\"\"{\n",
    "  \"criteria_results\": {\n",
    "    \"domain_relevance\": \"YES - focuses on mental health digital interventions\",\n",
    "    \"methodological_alignment\": \"YES - randomized controlled trial design\",\n",
    "    \"theoretical_connection\": \"NO - lacks explicit framework reference\",\n",
    "    \"practical_application\": \"YES - informs deployment in youth settings\"\n",
    "  },\n",
    "  \"decision\": \"RELEVANT\",\n",
    "  \"confidence\": \"85\",\n",
    "  \"summary\": \"This study evaluates a mobile CBT app for adolescents, showing significant reduction in anxiety and depression symptoms compared to control. It highlights engagement strategies relevant to NOFO objectives.\"\n",
    "}\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"Oncology Drug Delivery Review\",\n",
    "        \"\"\"{\n",
    "  \"criteria_results\": {\n",
    "    \"domain_relevance\": \"NO - focuses on oncology drug mechanisms\",\n",
    "    \"methodological_alignment\": \"NO\",\n",
    "    \"theoretical_connection\": \"NO\",\n",
    "    \"practical_application\": \"NO\"\n",
    "  },\n",
    "  \"decision\": \"PAPER NOT RELATED TO TOPIC\",\n",
    "  \"confidence\": \"0\",\n",
    "  \"summary\": null\n",
    "}\"\"\"\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dd2e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# FEW-SHOT RETRIEVAL + PROMPT BUILDER\n",
    "# ------------------------------------------------------------\n",
    "def get_few_shot_examples(\n",
    "    json_path,\n",
    "# Define configuration for few-shot example retrieval (number of examples)\n",
    "    max_examples=FEW_SHOT_MAX_EXAMPLES,\n",
    "# Minimum confidence threshold for including examples in few-shot prompting\n",
    "    min_confidence=MIN_CONFIDENCE_FOR_FEWSHOT\n",
    "):\n",
    "    \"\"\"\n",
    "    Pulls balanced high-confidence examples from log or uses fallback if none found.\n",
    "    \"\"\"\n",
    "# Import required libraries for core functionality\n",
    "    import os, json, random\n",
    "\n",
    "    examples = []\n",
    "\n",
    "    # Attempt to pull from log\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                data = []\n",
    "\n",
    "        relevant_examples, irrelevant_examples = [], []\n",
    "\n",
    "        for iteration in data:\n",
    "            for doc in iteration.get(\"relevant_documents\", []):\n",
    "                hybrid_conf = max(doc.get(\"model_confidence\", 0), doc.get(\"rule_confidence\", 0))\n",
    "                if hybrid_conf >= min_confidence:\n",
    "                    relevant_examples.append((doc[\"title\"], doc[\"reasoning\"]))\n",
    "\n",
    "            for doc in iteration.get(\"irrelevant_documents\", []):\n",
    "                irrelevant_examples.append((doc, \"PAPER NOT RELATED TO TOPIC\"))\n",
    "\n",
    "        # Balance relevant and irrelevant (half and half)\n",
    "        half = max_examples // 2\n",
    "        random.shuffle(relevant_examples)\n",
    "        random.shuffle(irrelevant_examples)\n",
    "        selected_relevant = relevant_examples[:half]\n",
    "        selected_irrelevant = irrelevant_examples[:half]\n",
    "        examples = selected_relevant + selected_irrelevant\n",
    "\n",
    "    # Fallback if no examples found\n",
    "    if not examples:\n",
    "        print(\"No high-confidence examples found. Using fallback seed examples.\")\n",
    "        examples = FALLBACK_EXAMPLES[:max_examples]\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def build_prompt_with_examples(topic, base_prompt, examples):\n",
    "    \"\"\"\n",
    "    Prepend few-shot examples (from log or fallback) to the base prompt.\n",
    "    \"\"\"\n",
    "    if not examples:\n",
    "        return base_prompt\n",
    "\n",
    "    examples_str = \"\\n\\n\".join(\n",
    "        [f\"Example ({title}):\\n{reasoning}\" for title, reasoning in examples]\n",
    "    )\n",
    "\n",
    "    return f\"\"\"\n",
    "You are a research grant specialist evaluating research papers for relevance to NIH NOFO objectives: {topic}.\n",
    "\n",
    "Below are examples of prior evaluations for context:\n",
    "{examples_str}\n",
    "\n",
    "Now evaluate the following paper using the same structure and logic:\n",
    "\n",
    "{base_prompt}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZG1Ah1eDi7aK"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Step 1: Topic Extraction - [3 Marks]**\n",
    "\n",
    "> **Read the NOFO doc and identify the topic for which the funding is to be given.**\n",
    "---\n",
    "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt and 1 mark for the successful completion of this section, including debugging or modifying the code if necessary.*\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkW_lO_CHSpc"
   },
   "source": [
    "**TASK:** Write an LLM prompt to extract the Topic for what the funding is been provided, from the NOFO document, Ask the LLM to respond back with the topic name only and nothing else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DcEpaUYNM_W"
   },
   "outputs": [],
   "source": [
    "# Topic extraction prompt\n",
    "topic_extraction_prompt = f\"\"\"\n",
    "You are a research grant specialist with expertise in analyzing NIH funding announcements and extracting key research priorities.\n",
    "\n",
    "Your task: Analyze this NOFO document from the National Institute of Mental Health (NIMH) to identify the PRIMARY funding topic.\n",
    "\n",
    "The document may describe multiple research areas, objectives, and priorities. Extract the single overarching topic that encompasses the main focus of this funding opportunity.\n",
    "\n",
    "Return ONLY the primary topic in 3-8 words. No explanations, descriptions, or additional text.\n",
    "\n",
    "Document:\n",
    "{NOFO_pdf[0].page_content}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "WtmCEbaKN9aW",
    "outputId": "7f9d4f38-5e38-4c9c-f70a-3976dfb8c881"
   },
   "outputs": [],
   "source": [
    "# Finding the topic for which the Funding is been given\n",
    "topic_extraction = llm.invoke(topic_extraction_prompt)\n",
    "topic = topic_extraction.content\n",
    "topic\n",
    "\n",
    "# Note: Multiple iterations of the above prompt yielded 'Digital mental health interventions' from both Open AI and Claude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXHRa9IlMycZ"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Step 2: Research Paper Relevance Assessment - [3 Marks]**\n",
    "> **Analyze all the Research Papers and filter out the research papers based on the topic of NOFO**\n",
    "---\n",
    "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt and 1 mark for the successful completion of this section, including debugging or modifying the code if necessary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kWc0LaCGPo3"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "**TASK:** Write an Prompt which can be used to analyze the relevance of the provided research paper in relation to the topic outlined in the NOFO (Notice of Funding Opportunity) document. Determine whether the research aligns with the goals, objectives, and funding criteria specified in the NOFO. Additionally, assess whether the research paper can be used to support or develop a viable project idea that fits within the scope of the funding opportunity.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Note:** If the paper does **not** significantly relate to the topic—by domain, method, theory, or application ask the LLM to return: **\"PAPER NOT RELATED TO TOPIC\"**\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Ask the LLM to respond in the below specified structure:\n",
    "\n",
    "```\n",
    "### Output Format:\n",
    "\"summary\": \"<summary of the paper under 300 words, or return: PAPER NOT RELATED TO TOPIC>\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3LPwoNeyXC7"
   },
   "outputs": [],
   "source": [
    "relevance_prompt_a = f\"\"\"\n",
    "You are a research grant specialist evaluating research papers for relevance to NIH NOFO objectives: {topic}.\n",
    "\n",
    "Evaluate the paper step-by-step against these criteria:\n",
    "1. Domain relevance (mental health, digital health, intervention effectiveness)\n",
    "2. Methodological alignment (clinical trials, user engagement studies, technology development)\n",
    "3. Theoretical connection (frameworks, evidence, insights for intervention design/implementation)\n",
    "4. Practical application (supports development or testing of digital mental health interventions)\n",
    "\n",
    "Instructions:\n",
    "- For EACH criterion, respond YES or NO and justify briefly.\n",
    "- A paper is RELEVANT if at least ONE criterion is YES.\n",
    "- Assign a confidence score (0–100%) to the RELEVANT decision, based on how strongly the paper meets the criteria (higher = more confident relevance).\n",
    "- If RELEVANT: provide a <300-word summary focused on digital mental health intervention insights.\n",
    "- If NOT RELEVANT: return exactly \"PAPER NOT RELATED TO TOPIC\".\n",
    "\n",
    "Output format (JSON):\n",
    "{{\n",
    "  \"criteria_results\": {{\n",
    "    \"domain_relevance\": \"YES/NO - justification\",\n",
    "    \"methodological_alignment\": \"YES/NO - justification\",\n",
    "    \"theoretical_connection\": \"YES/NO - justification\",\n",
    "    \"practical_application\": \"YES/NO - justification\"\n",
    "  }},\n",
    "  \"decision\": \"RELEVANT\" or \"PAPER NOT RELATED TO TOPIC\",\n",
    "  \"confidence\": \"<integer between 0 and 100>\",\n",
    "  \"summary\": \"<summary text or null>\"\n",
    "}}\n",
    "\n",
    "### Paper content:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_with_examples(topic, base_prompt, examples):\n",
    "    \"\"\"\n",
    "    Assemble few-shot prompt:\n",
    "    - Inserts prior examples (formatted) before evaluation instructions\n",
    "    \"\"\"\n",
    "    examples_str = \"\\n\\n\".join(\n",
    "        [f\"Example ({title}):\\n{reasoning}\" for title, reasoning in examples]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a research grant specialist evaluating research papers for relevance to NIH NOFO objectives: {topic}.\n",
    "\n",
    "Below are examples of prior evaluations for context:\n",
    "{examples_str}\n",
    "\n",
    "Now evaluate the following paper using the same structure and logic:\n",
    "\n",
    "{base_prompt}\n",
    "\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Few-shot setup ---\n",
    "# Path to the cleaned JSON log file where prompt evaluation iterations are stored\n",
    "clean_json_log(LOG_PATH)\n",
    "# Path to the cleaned JSON log file where prompt evaluation iterations are stored\n",
    "few_shot_examples = get_few_shot_examples(LOG_PATH)\n",
    "prompt_with_examples = build_prompt_with_examples(topic, relevance_prompt_a, few_shot_examples)\n",
    "\n",
    "# Import required libraries for core functionality\n",
    "import os\n",
    "# Import required libraries for core functionality\n",
    "import json\n",
    "# Import required libraries for core functionality\n",
    "import random\n",
    "# Import required libraries for core functionality\n",
    "import tiktoken\n",
    "from datetime import datetime\n",
    "# Import required libraries for core functionality\n",
    "import re\n",
    "# Import required libraries for core functionality\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CONFIGURATION SECTION\n",
    "# ------------------------------------------------------------\n",
    "# These variables let you control behavior without editing main logic.\n",
    "\n",
    "TEST_MODE = True                  # If True, process only a subset of files for quick iteration\n",
    "TEST_SAMPLE_SIZE = 60          # How many files to evaluate in test mode\n",
    "STRATIFY = True                   # If True, attempt stratified sampling (balanced categories)\n",
    "DISCREPANCY_THRESHOLD = 20        # Flag difference (%) between model vs rule confidence for review\n",
    "\n",
    "# Prior classification data (if available) can guide stratified sampling\n",
    "# e.g., after first run, categorize known relevant/irrelevant papers\n",
    "prior_classification = {\n",
    "    \"relevant\": [],   # fill with filenames identified as relevant\n",
    "    \"irrelevant\": [], # fill with filenames identified as irrelevant\n",
    "    \"unknown\": []     # files not yet evaluated or borderline\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# LOGGING FUNCTION\n",
    "# ------------------------------------------------------------\n",
    "def log_prompt_iteration(\n",
    "    json_path,\n",
    "    prompt,\n",
    "    relevant_docs_with_reasoning,\n",
    "    irrelevant_docs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Append this iteration's results (prompt + classified documents) to a master JSON log.\n",
    "\n",
    "    Rationale:\n",
    "    - Allows longitudinal analysis of prompt versions and performance trends\n",
    "    - Facilitates reproducibility for future audit or review\n",
    "    \"\"\"\n",
    "    iteration_id = len(json.load(open(json_path))) + 1 if os.path.exists(json_path) else 1\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    entry = {\n",
    "        \"iteration_id\": iteration_id,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"prompt\": prompt,\n",
    "        \"relevant_documents\": relevant_docs_with_reasoning,\n",
    "        \"irrelevant_documents\": irrelevant_docs\n",
    "    }\n",
    "\n",
    "    # Load existing log or create a new one\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                data = []\n",
    "    else:\n",
    "        data = []\n",
    "\n",
    "    data.append(entry)\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Logged iteration {iteration_id} to {json_path}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SELF-CHECK FUNCTION\n",
    "# ------------------------------------------------------------\n",
    "# Function: Self-check verification of LLM decision consistency\n",
    "def verify_decision(llm, reasoning_output):\n",
    "    \"\"\"\n",
    "    Performs a secondary verification pass using the model itself:\n",
    "    - Inputs the full reasoning text\n",
    "    - Asks for binary 'YES' or 'NO' confirmation of relevance\n",
    "\n",
    "    Rationale:\n",
    "    - Adds a lightweight consistency check\n",
    "    - Reduces false positives where reasoning contradicts final label\n",
    "    \"\"\"\n",
    "    verification_prompt = f\"\"\"\n",
    "You are verifying the relevance decision based on the following evaluation:\n",
    "\n",
    "{reasoning_output}\n",
    "\n",
    "Only answer with 'YES' if the decision should be considered relevant, or 'NO' if not relevant.\n",
    "    \"\"\"\n",
    "# Call the LLM with the prepared prompt and truncated paper content\n",
    "    verification_response = llm.invoke(verification_prompt)\n",
    "    return \"YES\" in verification_response.content.upper()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# RULE-DERIVED CONFIDENCE FUNCTION\n",
    "# ------------------------------------------------------------\n",
    "# Function: Compute rule-based confidence score based on YES criteria count\n",
    "def calculate_rule_confidence(criteria_results):\n",
    "    \"\"\"\n",
    "    Computes deterministic confidence score based on count of YES criteria.\n",
    "\n",
    "    Mapping (transparent to stakeholders):\n",
    "    - 0 YES = 0%\n",
    "    - 1 YES = 50%\n",
    "    - 2 YES = 70%\n",
    "    - 3 YES = 85%\n",
    "    - 4 YES = 95%\n",
    "\n",
    "    Rationale:\n",
    "    - Provides reproducible baseline independent of model's self-estimation\n",
    "    - Useful for auditing or hybrid scoring strategies\n",
    "    \"\"\"\n",
    "    yes_count = sum(1 for v in criteria_results.values() if v.upper().startswith(\"YES\"))\n",
    "    if yes_count == 0:\n",
    "        return 0\n",
    "    elif yes_count == 1:\n",
    "        return 50\n",
    "    elif yes_count == 2:\n",
    "        return 70\n",
    "    elif yes_count == 3:\n",
    "        return 85\n",
    "    else:\n",
    "        return 95\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# STRATIFIED FILE SAMPLING FUNCTION\n",
    "# ------------------------------------------------------------\n",
    "def get_files_to_process(path):\n",
    "    \"\"\"\n",
    "    Builds file list for processing:\n",
    "    - Uses full dataset if TEST_MODE = False\n",
    "    - Otherwise randomly samples TEST_SAMPLE_SIZE\n",
    "    - If STRATIFY = True and prior classifications exist, balances sample\n",
    "      across relevant/irrelevant/unknown groups\n",
    "\n",
    "    Rationale:\n",
    "    - Rapid iterations on representative subsets improve prompt tuning speed\n",
    "    - Stratification ensures diverse coverage (avoids subset bias)\n",
    "    \"\"\"\n",
    "    all_files = [f for f in os.listdir(path) if f.endswith('.pdf')]\n",
    "\n",
    "    if not TEST_MODE:\n",
    "        return all_files\n",
    "\n",
    "    if STRATIFY and any(prior_classification.values()):\n",
    "        files_to_process = []\n",
    "        groups = ['relevant', 'irrelevant', 'unknown']\n",
    "        quota = max(1, TEST_SAMPLE_SIZE // len(groups))\n",
    "\n",
    "        for group in groups:\n",
    "            pool = [f for f in all_files if f in prior_classification[group]]\n",
    "            if pool:\n",
    "                files_to_process.extend(random.sample(pool, min(quota, len(pool))))\n",
    "\n",
    "        # Fill remaining slots randomly if stratified pool is too small\n",
    "        remaining = TEST_SAMPLE_SIZE - len(files_to_process)\n",
    "        if remaining > 0:\n",
    "            leftover_pool = list(set(all_files) - set(files_to_process))\n",
    "            files_to_process.extend(random.sample(leftover_pool, min(remaining, len(leftover_pool))))\n",
    "    else:\n",
    "        files_to_process = random.sample(all_files, min(TEST_SAMPLE_SIZE, len(all_files)))\n",
    "\n",
    "    return files_to_process\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# MAIN LOOP: CLASSIFY DOCUMENTS\n",
    "# ------------------------------------------------------------\n",
    "path = \"data/raw\"\n",
    "files_to_process = get_files_to_process(path)\n",
    "\n",
    "documents = []            # Stores relevant docs with reasoning + confidences\n",
    "irrelevant_docs_list = []  # Stores filenames for irrelevant docs\n",
    "total_files = len(files_to_process)\n",
    "\n",
    "# Configure tokenization for the specified LLM to manage context window limits\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "MAX_TOKENS = 300000\n",
    "\n",
    "# MAX_TOKENS = 127500\n",
    "# Purpose:\n",
    "# Sets an internal limit you use for chunking or retrieval pipelines to ensure total token usage stays within the model’s maximum context window (≈128k tokens for GPT‑4o‑mini).\n",
    "# Impact on your project:\n",
    "\n",
    "# Document Chunking: Controls how big each chunk (or combined chunks) can be before sending them to the LLM.\n",
    "# Hybrid Indexing: Influences how much content from BM25 + embeddings can be merged into a single query.\n",
    "# Agent Orchestration: Each agent’s context window (Research, Proposal Writer, Compliance) will be limited by this ceiling.\n",
    "# Proposal Generation: Prevents oversized prompts when combining NOFO sections, extracted tables/figures, and retrieved research papers.\n",
    "# Refinement Loops: Ensures each pass fits in context even as drafts get longer (important for iterative proposals).\n",
    "\n",
    "# 3. Why It Matters in Your Flow\n",
    "# Your pipeline does multi-modal RAG (text + tables + figure captions). Without careful token counting:\n",
    "# Combined context from NOFO + papers + tables could easily exceed the limit.\n",
    "# Hybrid retrieval (BM25 + embeddings) may bring in redundant or overlapping chunks unless you budget tokens.\n",
    "# Setting MAX_TOKENS just below the model’s hard limit (127.5k vs. 128k) is a safety margin—avoids hard errors during synthesis or evaluation.\n",
    "\n",
    "progress_cnt = 1\n",
    "relevant_papers_count = 0\n",
    "irrelevant_papers_count = 0\n",
    "\n",
    "for filename in files_to_process:\n",
    "    file_path = os.path.join(path, filename)\n",
    "\n",
    "    try:\n",
    "        # -------------------------\n",
    "        # Load PDF and prepare text\n",
    "        # -------------------------\n",
    "# Load PDF files and extract content using PyPDFLoader\n",
    "        docs = PyPDFLoader(file_path, mode=\"single\").load()\n",
    "        pages = docs[0].page_content\n",
    "\n",
    "        # Token management: truncate paper text to fit model context window\n",
    "        available_tokens = MAX_TOKENS - len(encoding.encode(prompt_with_examples))\n",
    "        truncated_pages = encoding.decode(encoding.encode(pages)[:available_tokens])\n",
    "        full_prompt = prompt_with_examples + truncated_pages\n",
    "\n",
    "        # -------------------------\n",
    "        # Primary LLM evaluation\n",
    "        # -------------------------\n",
    "# Call the LLM with the prepared prompt and truncated paper content\n",
    "        response = llm.invoke(full_prompt)\n",
    "# Provide progress feedback to user on processing status\n",
    "        print(f\"Successfully processed: {progress_cnt}/{total_files}\")\n",
    "        progress_cnt += 1\n",
    "\n",
    "        # -------------------------\n",
    "        # Self-check verification\n",
    "        # -------------------------\n",
    "        is_relevant = verify_decision(llm, response.content)\n",
    "\n",
    "        if not is_relevant or \"PAPER NOT RELATED TO TOPIC\" in response.content:\n",
    "            irrelevant_papers_count += 1\n",
    "            irrelevant_docs_list.append(filename)\n",
    "            continue\n",
    "\n",
    "        # -------------------------\n",
    "        # Parse JSON-like output\n",
    "        # -------------------------\n",
    "        try:\n",
    "            parsed_json = json.loads(response.content)\n",
    "        except json.JSONDecodeError:\n",
    "            # Fallback regex parse if model wraps JSON in text\n",
    "            json_match = re.search(r\"\\{.*\\}\", response.content, re.DOTALL)\n",
    "            parsed_json = json.loads(json_match.group(0)) if json_match else {}\n",
    "\n",
    "        # -------------------------\n",
    "        # Extract confidences\n",
    "        # -------------------------\n",
    "        # Model-estimated confidence (direct from LLM)\n",
    "        model_confidence = int(parsed_json.get(\"confidence\", 0)) if parsed_json else None\n",
    "\n",
    "        # Rule-derived confidence (count of YES answers)\n",
    "        rule_confidence = 0\n",
    "        if \"criteria_results\" in parsed_json:\n",
    "            rule_confidence = calculate_rule_confidence(parsed_json[\"criteria_results\"])\n",
    "\n",
    "        # Discrepancy between two confidences\n",
    "        discrepancy = None\n",
    "        flagged = False\n",
    "        if model_confidence is not None:\n",
    "            discrepancy = abs(model_confidence - rule_confidence)\n",
    "            flagged = discrepancy > DISCREPANCY_THRESHOLD  # auto-flag if > threshold\n",
    "\n",
    "        # Store structured result\n",
    "        documents.append({\n",
    "            'title': filename,\n",
    "            'file_path': file_path,\n",
    "            'llm_reasoning': response.content,\n",
    "            'model_confidence': model_confidence,\n",
    "            'rule_confidence': rule_confidence,\n",
    "            'confidence_discrepancy': discrepancy,\n",
    "            'flagged_for_review': flagged\n",
    "        })\n",
    "        relevant_papers_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!!! Error processing {filename}: {str(e)}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SUMMARY OUTPUT\n",
    "# ------------------------------------------------------------\n",
    "print(\"=\" * 50)\n",
    "print(f\"Relevant Papers: {relevant_papers_count}/{total_files}\")\n",
    "print(f\"Irrelevant Papers: {irrelevant_papers_count}/{total_files}\")\n",
    "\n",
    "print(\"\\nList of relevant papers:\")\n",
    "for doc in documents:\n",
    "    print(f\"\\nTitle: {doc['title']}\")\n",
    "    print(f\"Model Confidence: {doc['model_confidence']}\")\n",
    "    print(f\"Rule Confidence: {doc['rule_confidence']}\")\n",
    "    print(f\"Discrepancy: {doc['confidence_discrepancy']} (Flagged: {doc['flagged_for_review']})\")\n",
    "    print(f\"Reasoning (truncated): {doc['llm_reasoning'][:500]}...\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# LOGGING: MASTER + FLAGGED\n",
    "# ------------------------------------------------------------\n",
    "# Prepare relevant docs with reasoning for main log\n",
    "relevant_docs_with_reasoning = [\n",
    "    {\n",
    "        \"title\": doc['title'],\n",
    "        \"reasoning\": doc['llm_reasoning'],\n",
    "        \"model_confidence\": doc['model_confidence'],\n",
    "        \"rule_confidence\": doc['rule_confidence'],\n",
    "        \"confidence_discrepancy\": doc['confidence_discrepancy'],\n",
    "        \"flagged_for_review\": doc['flagged_for_review']\n",
    "    }\n",
    "    for doc in documents\n",
    "]\n",
    "\n",
    "# Log all results\n",
    "log_prompt_iteration(\n",
    "    json_path=\"prompt_evaluation_log_cleaned.json\",\n",
    "    prompt=prompt_with_examples,\n",
    "    relevant_docs_with_reasoning=relevant_docs_with_reasoning,\n",
    "    irrelevant_docs=irrelevant_docs_list,\n",
    ")\n",
    "\n",
    "# Save flagged docs separately for manual review queue\n",
    "flagged_docs = [doc for doc in relevant_docs_with_reasoning if doc[\"flagged_for_review\"]]\n",
    "if flagged_docs:\n",
    "    with open(\"flagged_for_review.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(flagged_docs, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Saved {len(flagged_docs)} flagged documents to flagged_for_review.json\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# VISUALIZATION\n",
    "# ------------------------------------------------------------\n",
    "# Compare model vs rule confidence distributions\n",
    "# model_conf = [doc['model_confidence'] for doc in documents if doc['model_confidence'] is not None]\n",
    "# rule_conf = [doc['rule_confidence'] for doc in documents]\n",
    "\n",
    "# if model_conf and rule_conf:\n",
    "#     # Histogram: distribution comparison\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "#     plt.figure(figsize=(6, 4))\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "#     plt.hist(model_conf, bins=10, alpha=0.5, label=\"Model Confidence\")\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "#     plt.hist(rule_conf, bins=10, alpha=0.5, label=\"Rule Confidence\")\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "#     plt.legend()\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "#     plt.title(\"Confidence Distribution\")\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "#     plt.xlabel(\"Confidence (%)\")\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "#     plt.ylabel(\"Count\")\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "#     plt.show()\n",
    "\n",
    "#     # Scatterplot: identify discrepancies visually\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     colors = [\"red\" if doc['flagged_for_review'] else \"blue\" for doc in documents]\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "#     plt.scatter(rule_conf, model_conf, c=colors, alpha=0.6)\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "#     plt.axline((0, 0), slope=1, color=\"gray\", linestyle=\"--\")  # perfect agreement line\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "#     plt.title(\"Model vs Rule Confidence (Flagged in Red)\")\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "#     plt.xlabel(\"Rule-derived Confidence (%)\")\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "#     plt.ylabel(\"Model-estimated Confidence (%)\")\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# AUTO-UPDATE PRIOR CLASSIFICATION FOR STRATIFIED SAMPLING\n",
    "# ------------------------------------------------------------\n",
    "# After each run, update prior_classification with filenames categorized this iteration.\n",
    "\n",
    "for doc in relevant_docs_with_reasoning:\n",
    "    if doc[\"title\"] not in prior_classification[\"relevant\"]:\n",
    "        prior_classification[\"relevant\"].append(doc[\"title\"])\n",
    "\n",
    "for doc in irrelevant_docs_list:\n",
    "    if doc not in prior_classification[\"irrelevant\"]:\n",
    "        prior_classification[\"irrelevant\"].append(doc)\n",
    "\n",
    "# Remove duplicates if any\n",
    "for key in prior_classification:\n",
    "    prior_classification[key] = list(set(prior_classification[key]))\n",
    "\n",
    "print(\"Updated prior_classification for future stratified sampling.\")\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SUMMARY OF PRIOR CLASSIFICATION (TRACKED IN LOG)\n",
    "# ------------------------------------------------------------\n",
    "summary_counts = {\n",
    "    \"relevant_count\": len(prior_classification[\"relevant\"]),\n",
    "    \"irrelevant_count\": len(prior_classification[\"irrelevant\"]),\n",
    "    \"unknown_count\": len(prior_classification[\"unknown\"])\n",
    "}\n",
    "\n",
    "# Append summary to last log entry\n",
    "# Path to the cleaned JSON log file where prompt evaluation iterations are stored\n",
    "with open(LOG_PATH, \"r+\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    data[-1][\"prior_classification_summary\"] = summary_counts\n",
    "    f.seek(0)\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    f.truncate()\n",
    "\n",
    "print(f\"Classification summary appended to log: {summary_counts}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# VISUALIZATION: EVOLUTION OF CLASSIFICATIONS OVER TIME\n",
    "# ------------------------------------------------------------\n",
    "# Create trend lines for relevant/irrelevant/unknown counts across iterations\n",
    "# iterations = list(range(1, len(data)+1))\n",
    "# relevant_counts = [entry.get(\"prior_classification_summary\", {}).get(\"relevant_count\", 0) for entry in data]\n",
    "# irrelevant_counts = [entry.get(\"prior_classification_summary\", {}).get(\"irrelevant_count\", 0) for entry in data]\n",
    "# unknown_counts = [entry.get(\"prior_classification_summary\", {}).get(\"unknown_count\", 0) for entry in data]\n",
    "\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "# plt.plot(iterations, relevant_counts, label=\"Relevant\", marker='o')\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "# plt.plot(iterations, irrelevant_counts, label=\"Irrelevant\", marker='o')\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "# plt.plot(iterations, unknown_counts, label=\"Unknown\", marker='o')\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "# plt.title(\"Classification Evolution Over Iterations\")\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "# plt.xlabel(\"Iteration\")\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "# plt.ylabel(\"Count\")\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "# plt.legend()\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "# plt.grid(True)\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ALERT: FLAGGED DISCREPANCIES TREND ANALYSIS\n",
    "# ------------------------------------------------------------\n",
    "# Count flagged docs per iteration\n",
    "flagged_counts = [len([doc for doc in entry.get(\"relevant_documents\", []) if doc.get(\"flagged_for_review\")]) for entry in data]\n",
    "\n",
    "# Define threshold (e.g., >30% flagged across last 3 iterations)\n",
    "if len(flagged_counts) >= 3:\n",
    "    recent_flagged = flagged_counts[-3:]\n",
    "    total_recent = [len(entry.get(\"relevant_documents\", [])) for entry in data[-3:]]\n",
    "    flagged_ratios = [fc/tr if tr > 0 else 0 for fc, tr in zip(recent_flagged, total_recent)]\n",
    "\n",
    "    avg_flagged_ratio = sum(flagged_ratios) / len(flagged_ratios)\n",
    "\n",
    "    if avg_flagged_ratio > 0.3:\n",
    "        print(f\"WARNING: High flagged discrepancy rate detected over last 3 iterations: {avg_flagged_ratio*100:.1f}%\")\n",
    "    else:\n",
    "        print(f\"Flagged discrepancy rate stable: {avg_flagged_ratio*100:.1f}% over last 3 iterations.\")\n",
    "else:\n",
    "    print(\"Not enough iterations yet for flagged discrepancy trend analysis.\")\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ALERT: FLAGGED DISCREPANCIES TREND ANALYSIS + LOGGING\n",
    "# ------------------------------------------------------------\n",
    "# Count flagged docs per iteration\n",
    "flagged_counts = [len([doc for doc in entry.get(\"relevant_documents\", []) if doc.get(\"flagged_for_review\")]) for entry in data]\n",
    "\n",
    "alert_status = \"No alert\"\n",
    "if len(flagged_counts) >= 3:\n",
    "    recent_flagged = flagged_counts[-3:]\n",
    "    total_recent = [len(entry.get(\"relevant_documents\", [])) for entry in data[-3:]]\n",
    "    flagged_ratios = [fc/tr if tr > 0 else 0 for fc, tr in zip(recent_flagged, total_recent)]\n",
    "\n",
    "    avg_flagged_ratio = sum(flagged_ratios) / len(flagged_ratios)\n",
    "\n",
    "    if avg_flagged_ratio > 0.3:\n",
    "        alert_status = f\"WARNING: High flagged discrepancy rate: {avg_flagged_ratio*100:.1f}% over last 3 iterations\"\n",
    "        print(alert_status)\n",
    "    else:\n",
    "        alert_status = f\"Stable flagged discrepancy rate: {avg_flagged_ratio*100:.1f}% over last 3 iterations\"\n",
    "        print(alert_status)\n",
    "else:\n",
    "    alert_status = \"Not enough iterations for trend analysis\"\n",
    "    print(alert_status)\n",
    "\n",
    "# Log alert status into JSON (append to last entry)\n",
    "# Path to the cleaned JSON log file where prompt evaluation iterations are stored\n",
    "with open(LOG_PATH, \"r+\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    data[-1][\"flagged_discrepancy_alert\"] = alert_status\n",
    "    f.seek(0)\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    f.truncate()\n",
    "\n",
    "cleaned_log = clean_and_normalize_log(LOG_PATH)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# VISUALIZATION: FLAGGED DISCREPANCY DRIFT OVER TIME\n",
    "# ------------------------------------------------------------\n",
    "# Compute ratio of flagged relevant docs per iteration\n",
    "# flagged_ratios_all = []\n",
    "# for entry in data:\n",
    "#     relevant_docs = entry.get(\"relevant_documents\", [])\n",
    "#     total = len(relevant_docs)\n",
    "#     flagged = len([doc for doc in relevant_docs if doc.get(\"flagged_for_review\")])\n",
    "#     ratio = (flagged / total) if total > 0 else 0\n",
    "#     flagged_ratios_all.append(ratio)\n",
    "\n",
    "# # Plot drift over time\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "# plt.figure(figsize=(8, 4))\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "# plt.plot(iterations, [r*100 for r in flagged_ratios_all], marker='o', color='red')\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "# plt.title(\"Flagged Discrepancy Ratio Over Time\")\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "# plt.xlabel(\"Iteration\")\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "# plt.ylabel(\"Flagged Ratio (%)\")\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "# plt.grid(True)\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "# plt.axhline(30, color='gray', linestyle='--', label='Alert Threshold (30%)')\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "# plt.legend()\n",
    "# # Visualization: Generate plots for confidence distributions or trends\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf33cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pypdf import PdfReader\n",
    "import tiktoken\n",
    "\n",
    "# --- Tokenization setup ---\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "EXTRACTION_BUDGET = 300000  # ~20% buffer below max context window\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# --- Load matching papers from JSON log ---\n",
    "def load_matched_papers(json_path, pdf_folder=\"data/raw\"):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    last_iteration = data[-1]\n",
    "    relevant_docs = last_iteration.get(\"relevant_documents\", [])\n",
    "    \n",
    "    file_paths = []\n",
    "    for doc in relevant_docs:\n",
    "        title = doc[\"title\"]\n",
    "        pdf_path = os.path.join(pdf_folder, title)\n",
    "        if os.path.exists(pdf_path):\n",
    "            file_paths.append(pdf_path)\n",
    "        else:\n",
    "            print(f\"Warning: {pdf_path} not found. Skipping.\")\n",
    "    return file_paths\n",
    "\n",
    "\n",
    "\n",
    "# --- Process all matched papers ---\n",
    "def process_matched_papers(json_path, pdf_folder=\"content\"):\n",
    "    matched_files = load_matched_papers(json_path, pdf_folder)\n",
    "    results = {}\n",
    "    for file_path in matched_files:\n",
    "        print(f\"Processing: {os.path.basename(file_path)}\")\n",
    "        results[os.path.basename(file_path)] = process_pdf_multistage(file_path)\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "matched_content = process_matched_papers(\"prompt_evaluation_log_cleaned.json\", pdf_folder=\"data/raw\")\n",
    "print(matched_content.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DjVMJE_wTaCc",
    "outputId": "515d6100-5139-4baf-be86-e3888fba6cdb"
   },
   "outputs": [],
   "source": [
    "for filename, content in matched_content.items():\n",
    "    print(f\"\\n--- {filename} ---\\n\")\n",
    "    print(content[:1000])  # print first 1000 chars to avoid overwhelming output\n",
    "    print(\"\\n-------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNdBg6Iei7VJ"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Step 3: Proposal Ideation Based on Filtered Research - [4 marks]**\n",
    "> **Use the filtered papers, to generate ideas for the Reseach Proposal.**\n",
    "---\n",
    "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt, 1 mark for the Generating Idea and 1 mark for fetching file path of chosen idea along with successful completion of this section, including debugging or modifying the code if necessary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN5c3WhIEpzL"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "**TASK:** Write an Prompt which can be used to generate 5 ideas for the Research Proposal, each idea should consist:\n",
    "\n",
    "1. **Idea X:** [Concise Title of the Project Idea]  \\n\n",
    "2. **Description:** [Brief and targeted description summarizing the objectives, innovative elements, scientific rationale, and anticipated impact.]  \\n\n",
    "3. **Citation:** [Author(s), Year or Paper Title]  \\n\n",
    "4. **NOFO Alignment:** [List two or more specific NOFO requirements that this idea directly addresses]  \\n\n",
    "5. **File Path of the Research Paper:** [Exact file path, ending in .pdf]\n",
    "\n",
    "- Use the Delimiter `---` for defining the structure of the sample outputs in the prompt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLgOVonjveNM"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "#### Generating 5 Ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NoHQ6HH1kiD4"
   },
   "outputs": [],
   "source": [
    "# Note to self: Be sure to add additional details from page linked in the NOFO pdf\n",
    "# Also need to include constraints, e.g., \"Digital health test beds that leverage well-established \n",
    "# digital health platforms to optimize evidence-based digital mental health interventions\"\n",
    "\n",
    "gen_idea_prompt = f\"\"\"\n",
    "\n",
    "\n",
    "<WRITE YOUR PROMPT HERE>\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-oCJeXkcBKd"
   },
   "outputs": [],
   "source": [
    "ideas = llm.invoke(gen_idea_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952
    },
    "id": "TQpW_7cKco8Q",
    "outputId": "6b59af58-d64c-4804-f5a6-bfe89bb023dd"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(ideas.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLD4_7trvhKL"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "#### Choosing 1 Idea and fetching details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3ulgD6_dkrJ"
   },
   "outputs": [],
   "source": [
    "# Modify the idea_number for choosing the different idea\n",
    "idea_number = 5   # change the number if you wish to choose and generate the research proposal for another idea\n",
    "chosen_idea = ideas.content.split(\"---\")[idea_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DcV8QY1irIyH",
    "outputId": "d71e7380-e5d3-49d8-cd93-3dceedd57cf8"
   },
   "outputs": [],
   "source": [
    "# Import required libraries for core functionality\n",
    "import re\n",
    "\n",
    "# Use a regular expression to find the file path of the research paper\n",
    "\n",
    "pattern = r\"File Path of the Research Paper:\\*\\*\\s*(.+?)\\n\"\n",
    "# If you are unable to extract the file path successfully using this pattern, use the `ChatGPT` or any other LLM to find the pattern that works for you, simply provide the LLM the sample response of your whole ideas and ask the LLM to generate the regex patterm for extracting the \"File Path of the Research Paper\"\n",
    "\n",
    "match = re.search(pattern, chosen_idea)\n",
    "\n",
    "if match:\n",
    "  idea_generated_from_research_paper = match.group(1).strip()\n",
    "  print(\"Filepath : \", idea_generated_from_research_paper)\n",
    "else:\n",
    "  print(\"File Path of the Research Paper not found in the chosen idea.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51quLkMgi7S5"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Step 4: Proposal Blueprint Preparation - [3 Marks]**\n",
    "\n",
    "> **Select appropriate research ideas for the proposal and supply 'Sample Research Proposals' as templates to the LLM to support the generation of the final proposal.**\n",
    "---   \n",
    "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt and 1 mark for the successful completion of this section, including debugging or modifying the code if necessary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJhO1BFHC7cE"
   },
   "source": [
    "**TASK:** Write an Prompt which can be used to generate the Research Proposal.\n",
    "\n",
    "The prompt should be able to craft a research proposal based on the sample research proposal template, using one of the ideas generated above. The proposal should include references to the actual research papers from which the ideas are derived and should align well with the NOFO documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMOe-9_AgKvN"
   },
   "outputs": [],
   "source": [
    "# Here we need to add the full papers instead of the summary\n",
    "# Load PDF files and extract content using PyPDFLoader\n",
    "chosen_idea_rp = PyPDFLoader(idea_generated_from_research_paper, mode=\"single\").load()\n",
    "\n",
    "# Loading the sample research proposal template\n",
    "# Load PDF files and extract content using PyPDFLoader\n",
    "research_proposal_template = PyPDFLoader(\" <Path of Research Proposal Template> \", mode=\"single\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e6c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pypdf import PdfReader\n",
    "import camelot\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import tiktoken\n",
    "\n",
    "# --- Tokenization setup ---\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "MAX_TOKENS = 127500          # total model context window\n",
    "EXTRACTION_BUDGET = 100000   # reserve ~20% for prompts/response\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Count tokens using tiktoken encoding.\"\"\"\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# --- Load matching papers from JSON log ---\n",
    "def load_matched_papers(json_path, pdf_folder=\"content\"):\n",
    "    \"\"\"\n",
    "    Extract list of relevant document file paths from the latest JSON iteration.\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Take the last iteration's relevant_documents\n",
    "    last_iteration = data[-1]\n",
    "    relevant_docs = last_iteration.get(\"relevant_documents\", [])\n",
    "    \n",
    "    # Build file paths for each relevant doc (assumes they exist in pdf_folder)\n",
    "    file_paths = []\n",
    "    for doc in relevant_docs:\n",
    "        title = doc[\"title\"]\n",
    "        pdf_path = os.path.join(pdf_folder, title)\n",
    "        if os.path.exists(pdf_path):\n",
    "            file_paths.append(pdf_path)\n",
    "        else:\n",
    "            print(f\"Warning: {pdf_path} not found. Skipping.\")\n",
    "    return file_paths\n",
    "\n",
    "# --- Stage 1 & 2: Text + Table extraction ---\n",
    "def extract_text_and_tables(file_path, token_budget):\n",
    "    \"\"\"Extract text and tables within token budget.\"\"\"\n",
    "    content = \"\"\n",
    "    token_count = 0\n",
    "\n",
    "    # Stage 1: PyPDF text extraction\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text() or \"\"\n",
    "            token_count += count_tokens(page_text)\n",
    "            if token_count > token_budget:\n",
    "                print(f\"Token budget reached during text extraction: {file_path}\")\n",
    "                break\n",
    "            content += page_text\n",
    "    except Exception as e:\n",
    "        print(f\"PyPDF extraction failed: {e}\")\n",
    "\n",
    "    # Stage 2: Table extraction (Camelot)\n",
    "    # try:\n",
    "    #     tables = camelot.read_pdf(file_path, pages='all')\n",
    "    #     for table in tables:\n",
    "    #         table_text = \"\\n[Table Extracted]\\n\" + table.df.to_string()\n",
    "    #         token_count += count_tokens(table_text)\n",
    "    #         if token_count > token_budget:\n",
    "    #             print(f\"Token budget reached during table extraction: {file_path}\")\n",
    "    #             break\n",
    "    #         content += table_text\n",
    "    # except Exception:\n",
    "    #     pass\n",
    "\n",
    "    return content, token_count\n",
    "\n",
    "# --- Stage 3: OCR extraction ---\n",
    "# def extract_ocr(file_path, token_budget, current_tokens=0):\n",
    "#     \"\"\"Extract OCR text (figures/scanned pages) within remaining token budget.\"\"\"\n",
    "#     content = \"\"\n",
    "#     token_count = current_tokens\n",
    "\n",
    "#     try:\n",
    "#         images = convert_from_path(file_path)\n",
    "#         for image in images:\n",
    "#             ocr_text = pytesseract.image_to_string(image)\n",
    "#             token_count += count_tokens(ocr_text)\n",
    "#             if token_count > token_budget:\n",
    "#                 print(f\"Token budget reached during OCR extraction: {file_path}\")\n",
    "#                 break\n",
    "#             content += \"\\n[OCR Extracted]\\n\" + ocr_text\n",
    "#     except Exception:\n",
    "#         pass\n",
    "\n",
    "    return content\n",
    "\n",
    "# --- Process all matched papers ---\n",
    "def process_matched_papers(json_path, pdf_folder=\"content\"):\n",
    "    \"\"\"\n",
    "    Load matched papers from JSON and process them using multi-stage extraction:\n",
    "    Pass 1: Text + Tables\n",
    "    Pass 2: OCR (Figures)\n",
    "    Returns dict mapping filename -> combined extracted content.\n",
    "    \"\"\"\n",
    "    matched_files = load_matched_papers(json_path, pdf_folder)\n",
    "    text_table_data = {}\n",
    "    token_usage = {}\n",
    "\n",
    "    for file_path in matched_files:\n",
    "        print(f\"Extracting text/tables: {os.path.basename(file_path)}\")\n",
    "        content, tokens_used = extract_text_and_tables(file_path, EXTRACTION_BUDGET)\n",
    "        text_table_data[os.path.basename(file_path)] = content\n",
    "        token_usage[os.path.basename(file_path)] = tokens_used\n",
    "\n",
    "    # Return text_table_data directly\n",
    "    return text_table_data\n",
    "\n",
    "    # Pass 2: Extract OCR for all files (if budget allows)\n",
    "    # for file_path in matched_files:\n",
    "    #     filename = os.path.basename(file_path)\n",
    "    #     remaining_budget = EXTRACTION_BUDGET - token_usage.get(filename, 0)\n",
    "    #     if remaining_budget > 0:\n",
    "    #         print(f\"Extracting OCR: {filename} (remaining budget: {remaining_budget})\")\n",
    "    #         ocr_content = extract_ocr(file_path, EXTRACTION_BUDGET, token_usage[filename])\n",
    "    #         results[filename] = text_table_data[filename] + ocr_content\n",
    "    #     else:\n",
    "    #         print(f\"Skipping OCR for {filename} (no remaining token budget)\")\n",
    "    #         results[filename] = text_table_data[filename]\n",
    "\n",
    "# Example usage:\n",
    "# matched_content = process_matched_papers(\"/mnt/data/prompt_evaluation_log_cleaned.json\", pdf_folder=\"../content\")\n",
    "# print(matched_content.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_content = process_matched_papers(\"prompt_evaluation_log_cleaned.json\", pdf_folder=\"data/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matched_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OGcodZLo7VE"
   },
   "outputs": [],
   "source": [
    "research_proposal_template_prompt = f\"\"\"\n",
    "\n",
    "\n",
    "<WRITE YOUR PROMPT HERE>\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gXIZE0jkieg"
   },
   "outputs": [],
   "source": [
    "research_plan = llm.invoke(research_proposal_template_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1UFS-Vxlkib0",
    "outputId": "2761571a-d2a0-421c-b429-252cc78ddd41"
   },
   "outputs": [],
   "source": [
    "display(Markdown(research_plan.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkuiPncysdCa"
   },
   "outputs": [],
   "source": [
    "# @title **Optional Part - Creating a PDF of the Research Proposal**\n",
    "# The code in this cell block is used for printing out the output in the PDF format\n",
    "from markdown_pdf import MarkdownPdf, Section\n",
    "\n",
    "pdf = MarkdownPdf()\n",
    "pdf.add_section(Section(research_plan.content))\n",
    "pdf.save(\"Reseach Proposal First Draft.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77t_iYgni7QV"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Step 5: Proposal Evaluation Against NOFO Criteria - [3 Marks]**\n",
    "> **Use the LLM to evaluate the generated proposal (LLM-as-Judge) and assess its alignment with the NOFO criteria.**\n",
    "   \n",
    "\n",
    "---\n",
    "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt and 1 mark for the successful completion of this section, including debugging or modifying the code if necessary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXWK_mZewlim"
   },
   "source": [
    "**TASK:** Write an Prompt which can be used to evaluate the Research Proposal based on:\n",
    "1. **Innovation**\n",
    "2. **Significance**\n",
    "3. **Approach**\n",
    "4. **Investigator Expertise**\n",
    "\n",
    "- Ask the LLM to rate on each of the criteria from **1 (Poor)** to **5 (Excellent)**\n",
    "- Ask the LLM to provide the resonse in the json format\n",
    "```JSON\n",
    "name: Innovation\n",
    "    justification: \"<Justification>\"\n",
    "    score: <1-5>\n",
    "    strengths: \"<Strength 1>\"\n",
    "    weaknesses: \"<Weakness 1>\"\n",
    "    recommendations: \"<Recommendation 1>\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ax5H703ZhZ7y"
   },
   "outputs": [],
   "source": [
    "evaluation_prompt = f'''\n",
    "\n",
    "\n",
    "<WRITE YOUR PROMPT HERE>\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aZox8iNhZ5g"
   },
   "outputs": [],
   "source": [
    "# Call the LLM with the prepared prompt and truncated paper content\n",
    "eval_response = llm.invoke(evaluation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCx7_am-hZ3H"
   },
   "outputs": [],
   "source": [
    "# Import required libraries for core functionality\n",
    "import json\n",
    "json_resp = json.loads(eval_response.content[7:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vwAFtxolhZpD",
    "outputId": "a5d2b346-065a-428a-8c5d-a38cd57a90ae"
   },
   "outputs": [],
   "source": [
    "for key, value in json_resp.items():\n",
    "  print(f\"---\\n{key}:\")\n",
    "  if isinstance(value, list):\n",
    "    for item in value:\n",
    "      for k, v in item.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "      print(\"=\"*50)\n",
    "  elif isinstance(value, dict):\n",
    "    for k, v in value.items():\n",
    "      print(f\"  {k}: {v}\")\n",
    "  else:\n",
    "    print(f\"  {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fomQFyZAi7N4"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Step 6: Human Review and Refinement of Proposal**\n",
    "> **Perform Human Evaluation of the generated Proposal. Edit or Modify the proposal as necessary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HWwfHOXHmMOu",
    "outputId": "87320d16-f244-429e-e5cb-215e5eae01e7"
   },
   "outputs": [],
   "source": [
    "display(Markdown(research_plan.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRbWRHAJ_KXz"
   },
   "source": [
    "# **Step 7: Summary and Recommendation - [2 Marks]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jC6F4JezA840"
   },
   "source": [
    "Based on the projects, learners are expected to share their observations, key learnings, and insights related to this business use case, including the challenges they encountered.\n",
    "\n",
    "Additionally, they should recommend or explain any changes that could improve the project, along with suggesting additional steps that could be taken for further enhancement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8_WFIYIB12b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c868c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Enhanced PDF Processing (Commenting original PyPDF-only approach) ---\n",
    "# Original starter code (commented for traceability):\n",
    "# Load PDF files and extract content using PyPDFLoader\n",
    "# docs = PyPDFLoader(file_path, mode=\"single\").load()\n",
    "\n",
    "# New Implementation: Multi-stage parsing (PyPDF → Camelot/Tabula → OCR fallback)\n",
    "# Purpose: Capture text, tables, and figures from diverse PDF formats (Mermaid C node, Rubric Step 2).\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "# Import required libraries for core functionality\n",
    "import camelot\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def process_pdf_multistage(file_path):\n",
    "    \"\"\"\n",
    "    Multi-stage pipeline for extracting text, tables, and figures from PDFs.\n",
    "    Stages:\n",
    "    1. PyPDF (text)\n",
    "    2. Camelot/Tabula (tables)\n",
    "    3. OCR (scanned pages/figures)\n",
    "    \"\"\"\n",
    "    content = \"\"\n",
    "\n",
    "    # Stage 1: PyPDF text extraction\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        for page in reader.pages:\n",
    "            content += page.extract_text() or \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"PyPDF extraction failed: {e}\")\n",
    "\n",
    "    # Stage 2: Table extraction (Camelot)\n",
    "    try:\n",
    "        tables = camelot.read_pdf(file_path, pages='all')\n",
    "        for table in tables:\n",
    "            content += \"\\n[Table Extracted]\\n\" + table.df.to_string()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Stage 3: OCR fallback for scanned pages or figures\n",
    "    try:\n",
    "        images = convert_from_path(file_path)\n",
    "        for image in images:\n",
    "            text = pytesseract.image_to_string(image)\n",
    "            content += \"\\n[OCR Extracted]\\n\" + text\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6527505",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Hybrid Retrieval (BM25 + Embeddings) ---\n",
    "# Original code used either BM25 OR embeddings; this combines both (Mermaid D node, Rubric Step 2).\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def hybrid_retrieval_setup(docs_text):\n",
    "    \"\"\"\n",
    "    Creates BM25 and embedding indexes for hybrid search.\n",
    "    \"\"\"\n",
    "    # BM25 Index\n",
    "    tokenized_corpus = [doc.split(\" \") for doc in docs_text]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    # Embedding Index\n",
    "    embed_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectorstore = Chroma.from_texts(docs_text, embed_model)\n",
    "\n",
    "    return bm25, vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aabb353",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Agentic Components (Research Analyst, Proposal Writer, Compliance Checker) ---\n",
    "# Implements multi-agent workflow (Mermaid E subgraph, Rubric Step 3-4).\n",
    "\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "def analyze_papers(query):\n",
    "    return \"Synthesis of relevant papers\"\n",
    "\n",
    "def check_compliance(proposal):\n",
    "    return \"Compliance report\"\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"Research Analyst\", func=analyze_papers, description=\"Synthesizes relevant papers.\"),\n",
    "    Tool(name=\"Compliance Checker\", func=check_compliance, description=\"Ensures NOFO alignment.\")\n",
    "]\n",
    "\n",
    "# Initialize agent with zero-shot reasoning and tools\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a917953",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Agentic Components (Research Analyst, Proposal Writer, Compliance Checker) ---\n",
    "# Implements multi-agent workflow (Mermaid E subgraph, Rubric Step 3-4).\n",
    "\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "def analyze_papers(query):\n",
    "    return \"Synthesis of relevant papers\"\n",
    "\n",
    "def check_compliance(proposal):\n",
    "    return \"Compliance report\"\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"Research Analyst\", func=analyze_papers, description=\"Synthesizes relevant papers.\"),\n",
    "    Tool(name=\"Compliance Checker\", func=check_compliance, description=\"Ensures NOFO alignment.\")\n",
    "]\n",
    "\n",
    "# Initialize agent with zero-shot reasoning and tools\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5141a498",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Multi-Criteria Evaluation with Guardrails ---\n",
    "# Original evaluation only scored NIH criteria; now adds guardrail flags (Mermaid G node, Rubric Step 5).\n",
    "\n",
    "evaluation_prompt = f\"\"\"\n",
    "Evaluate the proposal on:\n",
    "1. Innovation\n",
    "2. Significance\n",
    "3. Approach\n",
    "4. Investigator Expertise\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"criteria\": [\n",
    "    {{\n",
    "      \"name\": \"Innovation\",\n",
    "      \"score\": 1-5,\n",
    "      \"strengths\": \"...\",\n",
    "      \"weaknesses\": \"...\",\n",
    "      \"recommendations\": \"...\"\n",
    "    }},\n",
    "    ...\n",
    "  ],\n",
    "  \"overall_score\": 1-5,\n",
    "  \"guardrail_flags\": [\"hallucination risk\", \"compliance gap\"]\n",
    "}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b907ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Caching Intermediate Steps ---\n",
    "# Saves embeddings, filtered papers, and draft proposals for reuse (Mermaid J node, Rubric Step 7).\n",
    "\n",
    "# Import required libraries for core functionality\n",
    "import pickle\n",
    "\n",
    "def save_checkpoint(data, name):\n",
    "    with open(f\"checkpoint_{name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_checkpoint(name):\n",
    "    try:\n",
    "        with open(f\"checkpoint_{name}.pkl\", \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7667a6",
   "metadata": {},
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "\n",
    "# Quick Reference: Few-Shot + Agentic Enhancements\n",
    "\n",
    "This section provides details about the few-shot pool, semantic versioning, and agentic conflict resolver integrated into this workflow.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "**Semantic Versioning**\n",
    "- Automatically increments version numbers (`v2-fewshot`, `v3-agentic`) based on features used.\n",
    "- Few-shot only → `-fewshot`\n",
    "- Few-shot + agentic resolver → `-agentic`\n",
    "\n",
    "**Few-Shot Pool**\n",
    "- Derived from cleaned log (`prompt_evaluation_log_cleaned.json`).\n",
    "- Filters examples with ≥80% hybrid confidence.\n",
    "- Balances relevant/irrelevant examples 50/50 and ensures diversity.\n",
    "\n",
    "**Agentic Conflict Resolver**\n",
    "- Activates when model vs. rule confidence differs by >20%.\n",
    "- Produces reconciled decision and rationale logged under `agentic_resolution`.\n",
    "\n",
    "**Enhanced Logging Fields**\n",
    "- `decision_source`: hybrid (model + rule)\n",
    "- `hybrid_confidence`: average of model and rule confidence\n",
    "- `agentic_resolution`: reconciliation result (if applicable)\n",
    "- `prompt_version`: auto-generated semantic version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da55b749",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# VERSION TRACKING + FEW-SHOT REBUILDER + AGENTIC RESOLVER\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Function: Determine the next semantic version string for the prompt\n",
    "def get_next_prompt_version(log_path, agentic_enabled=False):\n",
    "    \"\"\"\n",
    "    Determine next semantic version based on last logged version.\n",
    "    Increments number, adds suffix based on features used.\n",
    "    \"\"\"\n",
    "# Import required libraries for core functionality\n",
    "    import os, json, re\n",
    "    version_num = 1\n",
    "    if os.path.exists(log_path):\n",
    "        with open(log_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                data = []\n",
    "        # Extract last version number\n",
    "        for entry in reversed(data):\n",
    "            if \"prompt_version\" in entry:\n",
    "                match = re.match(r\"v(\\d+)\", entry[\"prompt_version\"])\n",
    "                if match:\n",
    "                    version_num = int(match.group(1)) + 1\n",
    "                break\n",
    "\n",
    "    suffix = \"-agentic\" if agentic_enabled else \"-fewshot\"\n",
    "    return f\"v{version_num}{suffix}\"\n",
    "\n",
    "\n",
    "# Function: Build balanced high-confidence few-shot example pool from the log\n",
    "def rebuild_few_shot_pool(cleaned_log_path, min_conf=80, max_examples=4):\n",
    "    \"\"\"\n",
    "    Build balanced high-confidence few-shot pool from cleaned log.\n",
    "    Balances relevant and irrelevant, ensures diversity.\n",
    "    \"\"\"\n",
    "# Import required libraries for core functionality\n",
    "    import json, random\n",
    "    with open(cleaned_log_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    relevant, irrelevant = [], []\n",
    "    for iteration in data:\n",
    "        for doc in iteration.get(\"relevant_documents\", []):\n",
    "            hybrid_conf = max(doc.get(\"model_confidence\", 0), doc.get(\"rule_confidence\", 0))\n",
    "            if hybrid_conf >= min_conf:\n",
    "                relevant.append((doc[\"title\"], doc[\"reasoning\"]))\n",
    "        for doc in iteration.get(\"irrelevant_documents\", []):\n",
    "            irrelevant.append((doc, \"PAPER NOT RELATED TO TOPIC\"))\n",
    "\n",
    "    # Shuffle and balance\n",
    "    half = max_examples // 2\n",
    "    random.shuffle(relevant)\n",
    "    random.shuffle(irrelevant)\n",
    "    return relevant[:half] + irrelevant[:half]\n",
    "\n",
    "\n",
    "# Function: Resolve discrepancies between model and rule confidences using agentic logic\n",
    "def agentic_conflict_resolver(doc_title, reasoning_json, model_conf, rule_conf):\n",
    "    \"\"\"\n",
    "    Agentic layer to reconcile conflicts:\n",
    "    - Triggered when discrepancy exceeds threshold\n",
    "    - Returns reconciled decision and rationale\n",
    "    \"\"\"\n",
    "    rationale = []\n",
    "    if abs(model_conf - rule_conf) > 20:\n",
    "        if rule_conf > model_conf:\n",
    "            final_decision = \"RELEVANT\" if rule_conf >= 50 else \"PAPER NOT RELATED TO TOPIC\"\n",
    "            rationale.append(\"Rule confidence higher; prioritizing deterministic criteria.\")\n",
    "        else:\n",
    "            final_decision = \"RELEVANT\" if model_conf >= 50 else \"PAPER NOT RELATED TO TOPIC\"\n",
    "            rationale.append(\"Model confidence higher; prioritizing LLM interpretation.\")\n",
    "    else:\n",
    "        final_decision = \"RELEVANT\" if (model_conf + rule_conf) / 2 >= 50 else \"PAPER NOT RELATED TO TOPIC\"\n",
    "        rationale.append(\"Confidences close; hybrid average used for decision.\")\n",
    "\n",
    "    return {\n",
    "        \"final_decision\": final_decision,\n",
    "        \"rationale\": \" \".join(rationale)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16955fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# ENHANCED LOGGING WITH SEMANTIC VERSIONING AND AGENTIC RESOLUTION\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Ensure this cell is run AFTER document processing and building relevant_docs_with_reasoning\n",
    "\n",
    "# Define constants for few-shot\n",
    "# Define configuration for few-shot example retrieval (number of examples)\n",
    "FEW_SHOT_MAX_EXAMPLES = 4\n",
    "# Minimum confidence threshold for including examples in few-shot prompting\n",
    "MIN_CONFIDENCE_FOR_FEWSHOT = 70\n",
    "# Path to the cleaned JSON log file where prompt evaluation iterations are stored\n",
    "LOG_PATH = \"prompt_evaluation_log_cleaned.json\"\n",
    "\n",
    "# Determine prompt version\n",
    "# Path to the cleaned JSON log file where prompt evaluation iterations are stored\n",
    "current_version = get_next_prompt_version(LOG_PATH, agentic_enabled=any(doc.get('flagged_for_review', False) for doc in relevant_docs_with_reasoning))\n",
    "\n",
    "# Add decision source and hybrid confidence\n",
    "for doc in relevant_docs_with_reasoning:\n",
    "    doc[\"decision_source\"] = \"hybrid\"\n",
    "    doc[\"hybrid_confidence\"] = (doc[\"model_confidence\"] + doc[\"rule_confidence\"]) / 2\n",
    "\n",
    "# Add agentic resolution for flagged docs\n",
    "for doc in relevant_docs_with_reasoning:\n",
    "    if doc.get(\"flagged_for_review\"):\n",
    "        resolution = agentic_conflict_resolver(\n",
    "            doc_title=doc[\"title\"],\n",
    "            reasoning_json=doc[\"reasoning\"],\n",
    "            model_conf=doc[\"model_confidence\"],\n",
    "            rule_conf=doc[\"rule_confidence\"]\n",
    "        )\n",
    "        doc[\"agentic_resolution\"] = resolution\n",
    "\n",
    "# Append prompt_version to log\n",
    "# Path to the cleaned JSON log file where prompt evaluation iterations are stored\n",
    "with open(LOG_PATH, \"r+\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    if data:\n",
    "        data[-1][\"prompt_version\"] = current_version\n",
    "    f.seek(0)\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    f.truncate()\n",
    "\n",
    "print(f\"Logged with prompt version: {current_version}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional enhancements proposed by Claude\n",
    "\n",
    "Your flowchart shows a well-structured approach to the RFP response generation system. Here are several improvements I'd recommend to enhance the robustness and effectiveness of your solution:\n",
    "\n",
    "1. Enhanced RFP Requirements Extraction\n",
    "After step B, add a sub-process for:\n",
    "\n",
    "Requirement Categorization: Classify requirements into mandatory vs. optional, technical vs. administrative\n",
    "Scoring Rubric Extraction: Specifically parse how proposals will be evaluated\n",
    "Budget Constraints Analysis: Extract funding limits and cost-effectiveness criteria\n",
    "Timeline Extraction: Identify key dates and milestone requirements\n",
    "\n",
    "2. Improved Paper Processing Pipeline\n",
    "Between steps C and D, consider adding:\n",
    "\n",
    "Citation Network Analysis: Map relationships between papers to identify influential work\n",
    "Method/Innovation Extraction: Specifically extract methodologies and novel approaches\n",
    "Results/Outcomes Extraction: Capture quantitative results and impact metrics\n",
    "Quality Assessment: Add a paper quality scoring mechanism (impact factor, recency, relevance)\n",
    "\n",
    "3. Enhanced Retrieval and Ranking\n",
    "Expand step D with:\n",
    "\n",
    "Multi-Query Generation: Generate multiple search queries from different RFP aspects\n",
    "Cross-Reference Validation: Verify that selected papers actually support proposed innovations\n",
    "Diversity Scoring: Ensure selected papers cover different aspects of the RFP\n",
    "Gap Analysis: Identify what the RFP asks for that isn't well-covered in existing research\n",
    "\n",
    "4. Strengthened Agentic Architecture\n",
    "Add these specialized agents to your existing three:\n",
    "\n",
    "Innovation Synthesizer Agent: Combines findings from multiple papers into novel approaches\n",
    "Budget Estimator Agent: Ensures proposals are financially realistic\n",
    "Risk Assessment Agent: Identifies potential implementation challenges\n",
    "Competitive Analysis Agent: Positions your proposal against likely competitors\n",
    "\n",
    "5. Improved Evaluation and Refinement\n",
    "Enhance the evaluation loop (G-I) with:\n",
    "\n",
    "Specific Weakness Detection: Not just overall score, but identify specific weak sections\n",
    "Competitive Benchmarking: Compare against successful past proposals if available\n",
    "Consistency Checking: Ensure all sections align and support each other\n",
    "Technical Feasibility Validation: Verify proposed solutions are implementable\n",
    "\n",
    "6. Additional Process Improvements\n",
    "Consider these architectural enhancements:\n",
    "flowchart LR\n",
    "    subgraph \"Knowledge Management\"\n",
    "        KB1[Domain Ontology]\n",
    "        KB2[Success Patterns DB]\n",
    "        KB3[Common Pitfalls DB]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Feedback Loops\"\n",
    "        FL1[Real-time Agent Collaboration]\n",
    "        FL2[Iterative Improvement Tracking]\n",
    "        FL3[Version Control System]\n",
    "    end\n",
    "7. Quality Assurance Additions\n",
    "\n",
    "Plagiarism Detection: Ensure generated content is original\n",
    "Fact Verification: Cross-check claims against source papers\n",
    "Readability Analysis: Ensure proposal meets target audience expectations\n",
    "Compliance Validation: Automated checks against all RFP requirements\n",
    "\n",
    "8. Output Enhancement\n",
    "For the final deliverables (step N), consider generating:\n",
    "\n",
    "Executive Summary: One-page overview for quick review\n",
    "Technical Appendix: Detailed methodology descriptions\n",
    "Budget Justification: Line-by-line cost explanations\n",
    "Risk Mitigation Plan: Addressing identified challenges\n",
    "Evaluation Metrics: How success will be measured\n",
    "\n",
    "9. Monitoring and Logging\n",
    "Add throughout the pipeline:\n",
    "\n",
    "Decision Logging: Track why papers were selected/rejected\n",
    "Agent Reasoning Traces: Understand how proposals were generated\n",
    "Performance Metrics: Time taken, resources used, quality scores\n",
    "Error Handling: Graceful degradation if components fail\n",
    "\n",
    "10. Advanced Features\n",
    "Consider these stretch goals:\n",
    "\n",
    "Multi-RFP Learning: Learn from multiple RFPs to improve over time\n",
    "Collaborative Filtering: If multiple users, learn from collective behavior\n",
    "Adaptive Prompting: Adjust prompts based on intermediate results\n",
    "Uncertainty Quantification: Flag areas where the system is less confident\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3cxtywSY4tq0",
    "ZG1Ah1eDi7aK",
    "cXHRa9IlMycZ"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
