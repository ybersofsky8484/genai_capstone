{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1942eb2c",
   "metadata": {},
   "source": [
    "\n",
    "# Notebook Map: Relevance Evaluation Pipeline with Few-Shot + Agentic Enhancements\n",
    "\n",
    "This table of contents provides a structured overview of the notebook, describing each section's purpose and how it fits into the workflow.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Quick Reference\n",
    "- Overview of semantic versioning, few-shot prompting, and agentic conflict resolver.\n",
    "\n",
    "## 2. Imports and Configuration\n",
    "- Load required libraries and define configuration constants (e.g., few-shot parameters, log paths).\n",
    "\n",
    "## 3. Core Utility Functions\n",
    "- `verify_decision`: Ensures model decisions are consistent.\n",
    "- `calculate_rule_confidence`: Computes rule-based confidence from criteria.\n",
    "- `get_next_prompt_version`: Auto-increments semantic prompt version.\n",
    "- `rebuild_few_shot_pool`: Builds balanced few-shot example set from log.\n",
    "- `agentic_conflict_resolver`: Resolves discrepancies between model and rule evaluations.\n",
    "\n",
    "## 4. Data Preparation\n",
    "- Load PDF research papers from `data/raw`.\n",
    "- Truncate text to fit LLM context window.\n",
    "\n",
    "## 5. Few-Shot Prompt Building\n",
    "- Retrieve high-confidence examples from log.\n",
    "- Prepend examples to base relevance prompt.\n",
    "\n",
    "## 6. Main Evaluation Loop\n",
    "- Iterate through PDFs.\n",
    "- Evaluate relevance using LLM.\n",
    "- Apply rule-based scoring and hybrid confidence calculation.\n",
    "- Flag documents for review when model vs. rule confidence diverges.\n",
    "\n",
    "## 7. Logging and Versioning\n",
    "- Append results to `prompt_evaluation_log.json`.\n",
    "- Add `prompt_version`, `decision_source`, and `agentic_resolution` where applicable.\n",
    "\n",
    "## 8. Visualization\n",
    "- Display confidence distribution, relevance drift, and flagged discrepancy trends.\n",
    "\n",
    "## 9. Enhancements (Appended)\n",
    "- Additional functions and logging improvements appended at the end for optional use.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRJ1_wsbi7cZ"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "<font size=10>**End-Term / Final Project**</font>\n",
    "\n",
    "<font size=6>**AI for Research Proposal Automation**</font>\n",
    "\n",
    "### **Business Problem - Create an AI system which will help you writing the research proposal aligning with the NOFO Document**\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsZ53h43336R"
   },
   "source": [
    "Meet Dr. Ian McCulloh, a seasoned research advisor and a leading voice in interdisciplinary science. Over the years, his lab has explored everything from AI for counterterrorism to social network analysis in neuroscience. His publication portfolio is vast, rich, and... chaotic.\n",
    "\n",
    "When the National Institute of Mental Health released a new NOFO (Notice of Funding Opportunity) seeking innovative digital health solutions for mental health equity, Dr. Ian saw an opportunity. But there was a problem: despite his extensive work, none of his existing research was directly aligned with digital mental health interventions. And with NIH deadlines looming, manually identifying relevant angles and generating a competitive proposal would be a massive lift.\n",
    "\n",
    "Dr. Ian wished for a smart assistant—one that could digest his past work, interpret the NOFO’s intent, spark new research directions, and even help draft proposal sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATYzMPf1333q"
   },
   "source": [
    "**The Challenge:**\n",
    "\n",
    "Organizations and researchers often maintain large archives of publications and prior work. When responding to competitive grants—especially highly specific ones like NIH NOFOs—it becomes extremely difficult and time-consuming to:\n",
    "\n",
    "1. Align past work with a new funding call.\n",
    "2. Extract relevant expertise from unrelated projects.\n",
    "3. Ideate novel, fundable research proposals tailored to complex criteria.\n",
    "4. Generate high-quality text for grant submission that satisfies technical and scientific review criteria.\n",
    "\n",
    "The manual effort to sift through dense research documents, match them to nuanced funding criteria, and write compelling, compliant proposals is labor-intensive, inconsistent, and prone to missed opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsDECO7z5eMJ"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "### **The Case Study Approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syX7mfDi5eb8"
   },
   "source": [
    "**Objective**\n",
    "1. Develop a generative AI-powered system using LLMs to automate and optimize the creation of NIH research proposals.\n",
    "2. The tool will identify relevant prior research, generate aligned project ideas, and draft high-quality proposal content tailored to specific NOFO requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given workflow:**\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Read NOFO Document] --> B[Analyze Research Papers]\n",
    "    B --> C[Filter Papers by Topic]\n",
    "    C --> D[Generate Research Ideas]\n",
    "    D --> E[Upload ideas to LLM]\n",
    "    E --> F[Generate Proposal]\n",
    "    F --> G[LLM Evaluation]\n",
    "    G --> H{Meets criteria?}\n",
    "    H -- NO --> F\n",
    "    H -- YES --> I[Human Review]\n",
    "    I --> J{Approved?}\n",
    "    J -- NO --> F\n",
    "    J -- YES --> K[Final Proposal]\n",
    "```\n",
    "\n",
    "**Enhanced workflow based on conversations with ChatGPT and Claude:**\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Read NOFO Document] --> B[Extract Key Requirements & Evaluation Criteria]\n",
    "    B --> C[Multi-Stage Paper Processing<br>(PyPDF → OCR)]\n",
    "    C --> C1[Table Extraction]\n",
    "    C --> C2[Figure Extraction (OCR + Captioning)]\n",
    "    C1 --> D\n",
    "    C2 --> D\n",
    "    D[Hybrid Indexing & Filtering<br>(BM25 + Embeddings + Metadata)]\n",
    "    D --> E[Agentic Research Synthesis<br>(Research Analyst + Proposal Writer + Compliance Checker)]\n",
    "    E --> F[Generate Proposal Blueprint + Draft]\n",
    "    F --> G[Multi-Criteria Evaluation<br/>(RAG + LLM-as-Judge + Guardrails)]\n",
    "    G --> H{Score ≥ Threshold?}\n",
    "    H -- NO --> I[Targeted Refinement Loop<br/>(Weakness-Specific Prompts)]\n",
    "    I --> F\n",
    "    H -- YES --> J[Caching + Checkpointing of Results]\n",
    "    J --> K[Human Review Interface]\n",
    "    K --> L{Approved?}\n",
    "    L -- NO --> M[Capture Feedback & Return to Refinement]\n",
    "    M --> F\n",
    "    L -- YES --> N[Final Proposal + Deliverables]\n",
    "    \n",
    "    subgraph \"Agentic Components\"\n",
    "        E1[Research Analyst Agent]\n",
    "        E2[Proposal Writer Agent]\n",
    "        E3[Compliance Checker Agent]\n",
    "        E1 <--> E2\n",
    "        E2 <--> E3\n",
    "        E3 <--> E1\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cxtywSY4tq0"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Setup - [2 Marks]**\n",
    "---\n",
    "<font color=Red>**Note:**</font> *1 marks is awarded for the Embedding Model configuration and 1 mark for the LLM Configuration.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b52EI78ZiY1X"
   },
   "outputs": [],
   "source": [
    "# Install required packages with progress and output displayed\n",
    "\n",
    "# Encountered multiple conflicts between packages and with codespace. Ended up installing all packages via the .venv\n",
    "\n",
    "# DISPLAY FINAL REQUIREMENTS.TXT for final file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for core functionality\n",
    "import os\n",
    "import warnings\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "base_url = os.getenv(\"OPENAI_BASE_URL\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM Model - Use `gpt-4o-mini` Model\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\")  # optional; only if using non-default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# FEW-SHOT AND LOGGING CONFIG\n",
    "# ------------------------------------------------------------\n",
    "# These constants control how many examples are retrieved and the minimum confidence threshold.\n",
    "# Modify here if you want more or fewer few-shot examples or to change the confidence cutoff.\n",
    "FEW_SHOT_MAX_EXAMPLES = 4         # Total examples (balanced between relevant/irrelevant if possible)\n",
    "# Minimum confidence threshold for including examples in few-shot prompting\n",
    "MIN_CONFIDENCE_FOR_FEWSHOT = 70   # Minimum hybrid confidence (%) to consider for few-shot retrieval\n",
    "\n",
    "# JSON log path\n",
    "# Path to the cleaned JSON log file where prompt evaluation iterations are stored\n",
    "LOG_PATH = \"prompt_evaluation_log_cleaned.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned PDF saved to: ../data/NOFO_cleaned.pdf\n",
      "Cleaning annotations for: cycon-final-draft.pdf\n",
      "Cleaned PDF saved to: data/raw/cycon-final-draft_cleaned.pdf\n",
      "Cleaning annotations for: Chat GPT Bias final w copyright.pdf\n",
      "Cleaned PDF saved to: data/raw/Chat GPT Bias final w copyright_cleaned.pdf\n",
      "Cleaning annotations for: Genetic_Algorithms_for_Prompt_Optimization.pdf\n",
      "Cleaned PDF saved to: data/raw/Genetic_Algorithms_for_Prompt_Optimization_cleaned.pdf\n",
      "Cleaning annotations for: DIVERSE_LLM_Dataset___IEEE_Big_Data.pdf\n",
      "Cleaned PDF saved to: data/raw/DIVERSE_LLM_Dataset___IEEE_Big_Data_cleaned.pdf\n",
      "Cleaning annotations for: Hashtag_Revival.pdf\n",
      "Cleaned PDF saved to: data/raw/Hashtag_Revival_cleaned.pdf\n",
      "Cleaning annotations for: FBI_Recruit_Hire_Final.pdf\n",
      "Cleaned PDF saved to: data/raw/FBI_Recruit_Hire_Final_cleaned.pdf\n",
      "Cleaning annotations for: Benson_MA491_NLP.pdf\n",
      "Cleaned PDF saved to: data/raw/Benson_MA491_NLP_cleaned.pdf\n",
      "Cleaning annotations for: Extreme Cohesion Darknet 20190815.pdf\n",
      "Cleaned PDF saved to: data/raw/Extreme Cohesion Darknet 20190815_cleaned.pdf\n",
      "Cleaning annotations for: Encyclopedia of SNA - R Packages.pdf\n",
      "Cleaned PDF saved to: data/raw/Encyclopedia of SNA - R Packages_cleaned.pdf\n",
      "Cleaning annotations for: RES2D.pdf\n",
      "Cleaned PDF saved to: data/raw/RES2D_cleaned.pdf\n",
      "Cleaning annotations for: ClassifiersCrowdSource.pdf\n",
      "Cleaned PDF saved to: data/raw/ClassifiersCrowdSource_cleaned.pdf\n",
      "Cleaning annotations for: FSS-19_paper_137.pdf\n",
      "Cleaned PDF saved to: data/raw/FSS-19_paper_137_cleaned.pdf\n",
      "Cleaning annotations for: Kidney_Behavioral.pdf\n",
      "Cleaned PDF saved to: data/raw/Kidney_Behavioral_cleaned.pdf\n",
      "Cleaning annotations for: Sim of Decon.pdf\n",
      "Cleaned PDF saved to: data/raw/Sim of Decon_cleaned.pdf\n",
      "Cleaning annotations for: Cross_Platform_Information_Spread_During_the_January_6th_Capitol_Riots.pdf\n",
      "Cleaned PDF saved to: data/raw/Cross_Platform_Information_Spread_During_the_January_6th_Capitol_Riots_cleaned.pdf\n",
      "Cleaning annotations for: Political_Networks_Conference.pdf\n",
      "Cleaned PDF saved to: data/raw/Political_Networks_Conference_cleaned.pdf\n",
      "Cleaning annotations for: On the Science of Networks.pdf\n",
      "Cleaned PDF saved to: data/raw/On the Science of Networks_cleaned.pdf\n",
      "Cleaning annotations for: Simmelian-Gamma-LDA.pdf\n",
      "Cleaned PDF saved to: data/raw/Simmelian-Gamma-LDA_cleaned.pdf\n",
      "Cleaning annotations for: BotBuster___AAAI.pdf\n",
      "Cleaned PDF saved to: data/raw/BotBuster___AAAI_cleaned.pdf\n",
      "Cleaning annotations for: Planning for AI Sustainment A Methodology for Maintenance and Cost Management_V5.pdf\n",
      "Cleaned PDF saved to: data/raw/Planning for AI Sustainment A Methodology for Maintenance and Cost Management_V5_cleaned.pdf\n",
      "Cleaning annotations for: Symbolic Generative AI 20231012.pdf\n",
      "Cleaned PDF saved to: data/raw/Symbolic Generative AI 20231012_cleaned.pdf\n",
      "Cleaning annotations for: 23-US-DHS-001.pdf\n",
      "Cleaned PDF saved to: data/raw/23-US-DHS-001_cleaned.pdf\n",
      "Cleaning annotations for: 2024_ICWSM_Data_Challenge__Post_API_Data_Collection.pdf\n",
      "Cleaned PDF saved to: data/raw/2024_ICWSM_Data_Challenge__Post_API_Data_Collection_cleaned.pdf\n",
      "Cleaning annotations for: Misinformation_Simulation.pdf\n",
      "Cleaned PDF saved to: data/raw/Misinformation_Simulation_cleaned.pdf\n",
      "Cleaning annotations for: Supply Chain Excellence.pdf\n",
      "Cleaned PDF saved to: data/raw/Supply Chain Excellence_cleaned.pdf\n",
      "Cleaning annotations for: 2021_EPJ_MVMCInfoOps.pdf\n",
      "Cleaned PDF saved to: data/raw/2021_EPJ_MVMCInfoOps_cleaned.pdf\n",
      "Cleaning annotations for: Network Simulation Models.pdf\n",
      "Cleaned PDF saved to: data/raw/Network Simulation Models_cleaned.pdf\n",
      "Cleaning annotations for: ALL18.pdf\n",
      "Cleaned PDF saved to: data/raw/ALL18_cleaned.pdf\n",
      "Cleaning annotations for: ONA-in-R.pdf\n",
      "Cleaned PDF saved to: data/raw/ONA-in-R_cleaned.pdf\n",
      "Cleaning annotations for: Helene_and_Milton_ACM.pdf\n",
      "Cleaned PDF saved to: data/raw/Helene_and_Milton_ACM_cleaned.pdf\n",
      "Cleaning annotations for: Unobtrusive Email.pdf\n",
      "Cleaned PDF saved to: data/raw/Unobtrusive Email_cleaned.pdf\n",
      "Cleaning annotations for: docnet.pdf\n",
      "Cleaned PDF saved to: data/raw/docnet_cleaned.pdf\n",
      "Cleaning annotations for: Utility Seeking in Complex Social Systems.pdf\n",
      "Cleaned PDF saved to: data/raw/Utility Seeking in Complex Social Systems_cleaned.pdf\n",
      "Cleaning annotations for: Lessons from Advising in Afghanistan.pdf\n",
      "Cleaned PDF saved to: data/raw/Lessons from Advising in Afghanistan_cleaned.pdf\n",
      "Cleaning annotations for: MOOC 20190828.pdf\n",
      "Cleaned PDF saved to: data/raw/MOOC 20190828_cleaned.pdf\n",
      "Cleaning annotations for: WEIRD.pdf\n",
      "Cleaned PDF saved to: data/raw/WEIRD_cleaned.pdf\n",
      "Cleaning annotations for: IkeNet.pdf\n",
      "Cleaned PDF saved to: data/raw/IkeNet_cleaned.pdf\n",
      "Cleaning annotations for: EmergencyResponseAI.pdf\n",
      "Cleaned PDF saved to: data/raw/EmergencyResponseAI_cleaned.pdf\n",
      "Cleaning annotations for: Quantifying_Information_Advantage.pdf\n",
      "Cleaned PDF saved to: data/raw/Quantifying_Information_Advantage_cleaned.pdf\n",
      "Cleaning annotations for: Confidence_Chaining.pdf\n",
      "Cleaned PDF saved to: data/raw/Confidence_Chaining_cleaned.pdf\n",
      "Cleaning annotations for: RatingsVRankings.pdf\n",
      "Cleaned PDF saved to: data/raw/RatingsVRankings_cleaned.pdf\n",
      "Cleaning annotations for: Analysis_of_Malware_Communities_Using_Multi_Modal_Features.pdf\n",
      "Cleaned PDF saved to: data/raw/Analysis_of_Malware_Communities_Using_Multi_Modal_Features_cleaned.pdf\n",
      "Cleaning annotations for: jfq-110_46-53_Cruickshank.pdf\n",
      "Cleaned PDF saved to: data/raw/jfq-110_46-53_Cruickshank_cleaned.pdf\n",
      "Cleaning annotations for: MLTEing_Models_for_NIER_at_ICSE_2023.pdf\n",
      "Cleaned PDF saved to: data/raw/MLTEing_Models_for_NIER_at_ICSE_2023_cleaned.pdf\n",
      "Cleaning annotations for: SocNetAlQaeda.pdf\n",
      "Cleaned PDF saved to: data/raw/SocNetAlQaeda_cleaned.pdf\n",
      "Cleaning annotations for: Leadership of Data Annotation 20180304v2.pdf\n",
      "Cleaned PDF saved to: data/raw/Leadership of Data Annotation 20180304v2_cleaned.pdf\n",
      "Cleaning annotations for: Parler_Disinformation_Challenge___CMOT_Extended.pdf\n",
      "Cleaned PDF saved to: data/raw/Parler_Disinformation_Challenge___CMOT_Extended_cleaned.pdf\n",
      "Cleaning annotations for: ICWSM_2025_Political_Bias.pdf\n",
      "Cleaned PDF saved to: data/raw/ICWSM_2025_Political_Bias_cleaned.pdf\n",
      "Cleaning annotations for: HIV.pdf\n",
      "Cleaned PDF saved to: data/raw/HIV_cleaned.pdf\n",
      "Cleaning annotations for: Limit Velocity.pdf\n",
      "Cleaned PDF saved to: data/raw/Limit Velocity_cleaned.pdf\n",
      "Cleaning annotations for: Spectral Analysis SNA.pdf\n",
      "Cleaned PDF saved to: data/raw/Spectral Analysis SNA_cleaned.pdf\n",
      "Cleaning annotations for: LLM_Confidence_Metrics.pdf\n",
      "Cleaned PDF saved to: data/raw/LLM_Confidence_Metrics_cleaned.pdf\n",
      "Cleaning annotations for: Food Addiction 20231222 v3.pdf\n",
      "Cleaned PDF saved to: data/raw/Food Addiction 20231222 v3_cleaned.pdf\n",
      "Cleaning annotations for: NBA Performance.pdf\n",
      "Cleaned PDF saved to: data/raw/NBA Performance_cleaned.pdf\n",
      "Cleaning annotations for: MIPB-CDA.pdf\n",
      "Cleaned PDF saved to: data/raw/MIPB-CDA_cleaned.pdf\n",
      "Cleaning annotations for: Vol33Iss1_INSNApdf.pdf\n",
      "Cleaned PDF saved to: data/raw/Vol33Iss1_INSNApdf_cleaned.pdf\n",
      "Cleaning annotations for: NeuroCogInfluence.pdf\n",
      "Cleaned PDF saved to: data/raw/NeuroCogInfluence_cleaned.pdf\n",
      "Cleaning annotations for: Frontiers COVID.pdf\n",
      "Cleaned PDF saved to: data/raw/Frontiers COVID_cleaned.pdf\n",
      "Cleaning annotations for: Evolution_of_Terrorism_PNAS.pdf\n",
      "Cleaned PDF saved to: data/raw/Evolution_of_Terrorism_PNAS_cleaned.pdf\n",
      "Cleaning annotations for: Text Analysis Using Automated Language Translators.pdf\n",
      "Cleaned PDF saved to: data/raw/Text Analysis Using Automated Language Translators_cleaned.pdf\n",
      "Cleaning annotations for: Vulnerable_Code_Detection.pdf\n",
      "Cleaned PDF saved to: data/raw/Vulnerable_Code_Detection_cleaned.pdf\n",
      "Cleaning annotations for: Dormant Bots 20190814.pdf\n",
      "Cleaned PDF saved to: data/raw/Dormant Bots 20190814_cleaned.pdf\n",
      "Cleaning annotations for: NAP Behavioral Sci Intel.pdf\n",
      "Cleaned PDF saved to: data/raw/NAP Behavioral Sci Intel_cleaned.pdf\n",
      "Cleaning annotations for: YouTube-COVID.pdf\n",
      "Cleaned PDF saved to: data/raw/YouTube-COVID_cleaned.pdf\n",
      "Cleaning annotations for: Organizational risk using network analysis.pdf\n",
      "Cleaned PDF saved to: data/raw/Organizational risk using network analysis_cleaned.pdf\n",
      "Cleaning annotations for: k-truss.pdf\n",
      "Cleaned PDF saved to: data/raw/k-truss_cleaned.pdf\n",
      "Cleaning annotations for: Political Party Cohesion.pdf\n",
      "Cleaned PDF saved to: data/raw/Political Party Cohesion_cleaned.pdf\n",
      "Cleaning annotations for: Sailer McCulloh Soc Net and Spatial Config.pdf\n",
      "Cleaned PDF saved to: data/raw/Sailer McCulloh Soc Net and Spatial Config_cleaned.pdf\n",
      "Cleaning annotations for: CUSUM Parameterization.pdf\n",
      "Cleaned PDF saved to: data/raw/CUSUM Parameterization_cleaned.pdf\n",
      "Cleaning annotations for: Multi_view_Clustering_for_Social_Based_Data.pdf\n",
      "Cleaned PDF saved to: data/raw/Multi_view_Clustering_for_Social_Based_Data_cleaned.pdf\n",
      "Cleaning annotations for: ONA-using-igraph.pdf\n",
      "Cleaned PDF saved to: data/raw/ONA-using-igraph_cleaned.pdf\n",
      "Cleaning annotations for: improving-decision-support-for-organ-transplant.pdf\n",
      "Cleaned PDF saved to: data/raw/improving-decision-support-for-organ-transplant_cleaned.pdf\n",
      "Cleaning annotations for: Tweets-to-touchdowns.pdf\n",
      "Cleaned PDF saved to: data/raw/Tweets-to-touchdowns_cleaned.pdf\n",
      "Cleaning annotations for: Social_Det_COVID_Mortality.pdf\n",
      "Cleaned PDF saved to: data/raw/Social_Det_COVID_Mortality_cleaned.pdf\n",
      "Cleaning annotations for: White Paper Brain Gaze.pdf\n",
      "Cleaned PDF saved to: data/raw/White Paper Brain Gaze_cleaned.pdf\n",
      "Cleaning annotations for: McCullohCarleyJOSS.pdf\n",
      "Cleaned PDF saved to: data/raw/McCullohCarleyJOSS_cleaned.pdf\n",
      "Cleaning annotations for: Course Info Security.pdf\n",
      "Cleaned PDF saved to: data/raw/Course Info Security_cleaned.pdf\n",
      "Cleaning annotations for: A_Complex_Network_Approach_to_Find_Latent_Terorrist_Communities.pdf\n",
      "Cleaned PDF saved to: data/raw/A_Complex_Network_Approach_to_Find_Latent_Terorrist_Communities_cleaned.pdf\n",
      "Cleaning annotations for: Knowing the Terrain.pdf\n",
      "Cleaned PDF saved to: data/raw/Knowing the Terrain_cleaned.pdf\n",
      "Cleaning annotations for: Social_Network_Probability_Mechanics.pdf\n",
      "Cleaned PDF saved to: data/raw/Social_Network_Probability_Mechanics_cleaned.pdf\n",
      "Cleaning annotations for: The ABCs of AI-Enabled Intelligence Analysis - War on the Rocks.pdf\n",
      "Cleaned PDF saved to: data/raw/The ABCs of AI-Enabled Intelligence Analysis - War on the Rocks_cleaned.pdf\n",
      "Cleaning annotations for: SocNetChgDet.pdf\n",
      "Cleaned PDF saved to: data/raw/SocNetChgDet_cleaned.pdf\n",
      "Cleaning annotations for: Take_boards.pdf\n",
      "Cleaned PDF saved to: data/raw/Take_boards_cleaned.pdf\n",
      "Cleaning annotations for: Arrow White Paper DExTra.pdf\n",
      "Cleaned PDF saved to: data/raw/Arrow White Paper DExTra_cleaned.pdf\n",
      "Cleaning annotations for: TrainingSetSize.pdf\n",
      "Cleaned PDF saved to: data/raw/TrainingSetSize_cleaned.pdf\n",
      "Cleaning annotations for: CausalOrgInorgContent.pdf\n",
      "Cleaned PDF saved to: data/raw/CausalOrgInorgContent_cleaned.pdf\n",
      "Cleaning annotations for: Overcoming_Social_Media_API_Restrictions__Building_an_Effective_Web_Scraper.pdf\n",
      "Cleaned PDF saved to: data/raw/Overcoming_Social_Media_API_Restrictions__Building_an_Effective_Web_Scraper_cleaned.pdf\n",
      "Cleaning annotations for: LLM_UQ.pdf\n",
      "Cleaned PDF saved to: data/raw/LLM_UQ_cleaned.pdf\n",
      "Cleaning annotations for: Kent2022_Chapter_MicroscopicMarkovChainApproach.pdf\n",
      "Cleaned PDF saved to: data/raw/Kent2022_Chapter_MicroscopicMarkovChainApproach_cleaned.pdf\n",
      "Cleaning annotations for: Acquiring Maintainable AI_Enable Systems_Final.pdf\n",
      "Cleaned PDF saved to: data/raw/Acquiring Maintainable AI_Enable Systems_Final_cleaned.pdf\n",
      "Cleaning annotations for: Characterizing_Communities_of_Hashtag_Usage_on_Twitter_During_the_2020_COVID_19_Pandemic.pdf\n",
      "Cleaned PDF saved to: data/raw/Characterizing_Communities_of_Hashtag_Usage_on_Twitter_During_the_2020_COVID_19_Pandemic_cleaned.pdf\n",
      "Cleaning annotations for: SecurityPrivAIML.pdf\n",
      "Cleaned PDF saved to: data/raw/SecurityPrivAIML_cleaned.pdf\n",
      "Cleaning annotations for: AAAI IAA CV.pdf\n",
      "Cleaned PDF saved to: data/raw/AAAI IAA CV_cleaned.pdf\n",
      "Cleaning annotations for: LSA email.pdf\n",
      "Cleaned PDF saved to: data/raw/LSA email_cleaned.pdf\n",
      "Cleaning annotations for: Clustering_Analysis_of_Website_Usage_on_Twitter_during_the_COVID_19_Pandemic.pdf\n",
      "Cleaned PDF saved to: data/raw/Clustering_Analysis_of_Website_Usage_on_Twitter_during_the_COVID_19_Pandemic_cleaned.pdf\n",
      "Cleaning annotations for: Designed Networks.pdf\n",
      "Cleaned PDF saved to: data/raw/Designed Networks_cleaned.pdf\n",
      "Cleaning annotations for: COVID Bayesian Data Aug.pdf\n",
      "Cleaned PDF saved to: data/raw/COVID Bayesian Data Aug_cleaned.pdf\n",
      "Cleaning annotations for: Cohort_Optimization_Methods_SNAMS_2021_working_draft (4).pdf\n",
      "Cleaned PDF saved to: data/raw/Cohort_Optimization_Methods_SNAMS_2021_working_draft (4)_cleaned.pdf\n",
      "Cleaning annotations for: SM Customer Feedback_FAB_2019_rev3.pdf\n",
      "Cleaned PDF saved to: data/raw/SM Customer Feedback_FAB_2019_rev3_cleaned.pdf\n",
      "Cleaning annotations for: IkekNet1.pdf\n",
      "Cleaned PDF saved to: data/raw/IkekNet1_cleaned.pdf\n",
      "Cleaning annotations for: Social Media Mental Health Final.pdf\n",
      "Cleaned PDF saved to: data/raw/Social Media Mental Health Final_cleaned.pdf\n",
      "Cleaning annotations for: Reforming Sectarian Beliefs.pdf\n",
      "Cleaned PDF saved to: data/raw/Reforming Sectarian Beliefs_cleaned.pdf\n",
      "Cleaning annotations for: Savas.pdf\n",
      "Cleaned PDF saved to: data/raw/Savas_cleaned.pdf\n",
      "Cleaning annotations for: Data_Education__Emerging_Challenges_and_Opportunities.pdf\n",
      "Cleaned PDF saved to: data/raw/Data_Education__Emerging_Challenges_and_Opportunities_cleaned.pdf\n",
      "Cleaning annotations for: Dissertation.pdf\n",
      "Cleaned PDF saved to: data/raw/Dissertation_cleaned.pdf\n",
      "Cleaning annotations for: Leveraging_AI_to_Improve_Viral_Information_Detection_in_Online_Discourse.pdf\n",
      "Cleaned PDF saved to: data/raw/Leveraging_AI_to_Improve_Viral_Information_Detection_in_Online_Discourse_cleaned.pdf\n",
      "Cleaning annotations for: ICWSM___Use_of_Large_Language_Models_for_Stance_Classification.pdf\n",
      "Cleaned PDF saved to: data/raw/ICWSM___Use_of_Large_Language_Models_for_Stance_Classification_cleaned.pdf\n",
      "Cleaning annotations for: NeuroSynchrony.pdf\n",
      "Cleaned PDF saved to: data/raw/NeuroSynchrony_cleaned.pdf\n",
      "Cleaning annotations for: Multi_Agent_Systems_for_Frame_Detection.pdf\n",
      "Cleaned PDF saved to: data/raw/Multi_Agent_Systems_for_Frame_Detection_cleaned.pdf\n",
      "Cleaning annotations for: Review of R Packages_20161026.pdf\n",
      "Cleaned PDF saved to: data/raw/Review of R Packages_20161026_cleaned.pdf\n",
      "Cleaning annotations for: Lead-Azide.pdf\n",
      "Cleaned PDF saved to: data/raw/Lead-Azide_cleaned.pdf\n",
      "Cleaning annotations for: LongNetViewerORA.pdf\n",
      "Cleaned PDF saved to: data/raw/LongNetViewerORA_cleaned.pdf\n",
      "Annotation removal log written to: annotation_log.json\n",
      "All research PDFs cleaned and saved in data/raw/\n"
     ]
    }
   ],
   "source": [
    "# PDF Cleaning Step: Remove non-visual annotations (comments, links, form fields)\n",
    "# Keeps images, diagrams, and visible callouts intact\n",
    "\n",
    "# Import required libraries for core functionality\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import json  # <-- NEW: Required to write annotation logs\n",
    "\n",
    "# Create a global dictionary to store removed annotations\n",
    "annotation_log = {}  # <-- NEW: Accumulates logs of all removed annotations\n",
    "\n",
    "# Initial standardization step to remove annotations for parsing\n",
    "def clean_pdf_annotations(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Strips non-visual annotations (comments, form fields, links) from a PDF\n",
    "    while preserving visible images and diagrams.\n",
    "    Also logs removed annotations to a global dictionary.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(input_path)\n",
    "    removed_annots = []  # <-- NEW: Stores removed annotations for this PDF\n",
    "\n",
    "    for page in doc:\n",
    "        # Iterate over all annotations (not images)\n",
    "        annot = page.first_annot\n",
    "        while annot:\n",
    "            next_annot = annot.next  # store reference to next annotation\n",
    "            \n",
    "            # Try to extract meaningful annotation content\n",
    "            try:\n",
    "                annot_info = annot.info  # Dictionary of annotation metadata\n",
    "                content = annot_info.get(\"content\", \"\").strip()\n",
    "                subtype = annot_info.get(\"subtype\", \"\").strip()\n",
    "                if content:\n",
    "                    removed_annots.append(f\"{subtype}: {content}\")\n",
    "                else:\n",
    "                    removed_annots.append(f\"{subtype}: [no content]\")\n",
    "            except Exception as e:\n",
    "                # Fallback if annotation metadata is inaccessible\n",
    "                removed_annots.append(\"Unknown annotation (could not extract content)\")\n",
    "\n",
    "            # Remove annotation object (highlights, comments, links)\n",
    "            page.delete_annot(annot)\n",
    "            annot = next_annot\n",
    "\n",
    "    # Save cleaned PDF\n",
    "    doc.save(output_path, garbage=4, deflate=True)\n",
    "    doc.close()\n",
    "\n",
    "    # Add entry to annotation log using the input filename as key\n",
    "    annotation_log[os.path.basename(input_path)] = removed_annots  # <-- NEW: Log entries keyed by file\n",
    "\n",
    "# Clean NOFO file\n",
    "input_pdf = \"../data/NOFO.pdf\"\n",
    "cleaned_pdf = \"../data/NOFO_cleaned.pdf\"\n",
    "clean_pdf_annotations(input_pdf, cleaned_pdf)\n",
    "print(f\"Cleaned PDF saved to: {cleaned_pdf}\")\n",
    "\n",
    "# Get de-annotated NOFO doc content using PyPDFLoader for evaluation step\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "pdf_file = \"../data/NOFO_cleaned.pdf\"\n",
    "pdf_loader = PyPDFLoader(pdf_file)\n",
    "NOFO_pdf = pdf_loader.load()\n",
    "\n",
    "# Prepare output folder for de-annotated research papers\n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "\n",
    "# Set variables for de-annotating the research paper PDF collection \n",
    "source_dir = \"../content\"\n",
    "output_dir = \"data/raw\"\n",
    "\n",
    "# Loop through content folder, de-annotate each PDF, and save to a 'clean' output directory\n",
    "for file_name in os.listdir(source_dir):\n",
    "    if file_name.lower().endswith(\".pdf\"):\n",
    "        input_pdf = os.path.join(source_dir, file_name)\n",
    "        cleaned_pdf = os.path.join(output_dir, file_name.replace(\".pdf\", \"_cleaned.pdf\"))\n",
    "        print(f\"Cleaning annotations for: {file_name}\")\n",
    "        clean_pdf_annotations(input_pdf, cleaned_pdf)\n",
    "        print(f\"Cleaned PDF saved to: {cleaned_pdf}\")\n",
    "\n",
    "# Write annotation log to disk after all PDFs are processed\n",
    "log_path = \"annotation_log.json\"  # <-- NEW: File to store the annotation log\n",
    "with open(log_path, \"w\", encoding=\"utf-8\") as log_file:\n",
    "    json.dump(annotation_log, log_file, indent=2, ensure_ascii=False)  # <-- NEW: Write log to file\n",
    "print(f\"Annotation removal log written to: {log_path}\")  # <-- NEW: Confirm log creation\n",
    "\n",
    "print(\"All research PDFs cleaned and saved in data/raw/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and Chunk Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Function to split cleaned text into 3000-token chunks with overlap for RAG\n",
    "# ---------------------------------------------------------------\n",
    "# This function breaks long text into overlapping token-based chunks for use in\n",
    "# Retrieval-Augmented Generation (RAG) pipelines. Overlapping chunks help\n",
    "# preserve context continuity across boundaries, improving answer quality.\n",
    "\n",
    "import tiktoken  # OpenAI tokenizer library for counting and managing tokens\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Load tokenizer for the target model\n",
    "# ---------------------------------------------------------------\n",
    "# `tiktoken` provides tokenization rules tailored to specific OpenAI models.\n",
    "# Here we select the encoding used by gpt-4o-mini to ensure our token counting\n",
    "# aligns with how the model actually interprets input.\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Define chunking function\n",
    "# ---------------------------------------------------------------\n",
    "# Inputs:\n",
    "# - text: full string to be split into chunks\n",
    "# - chunk_size: max number of tokens per chunk (default 3000)\n",
    "# - overlap: number of tokens to repeat from the previous chunk (default 200)\n",
    "# This overlap preserves some context from earlier chunks in each new chunk.\n",
    "\n",
    "def chunk_text(text, chunk_size=3000, overlap=200):\n",
    "    # Convert text into a list of token IDs using the tokenizer\n",
    "    tokens = encoding.encode(text)\n",
    "\n",
    "    # Initialize an empty list to store the final chunks\n",
    "    chunks = []\n",
    "\n",
    "    # Step through the token list in increments of (chunk_size - overlap)\n",
    "    # This ensures that each new chunk shares `overlap` tokens with the previous one\n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        # Slice the token list to get a window of `chunk_size` tokens\n",
    "        chunk_tokens = tokens[i:i+chunk_size]\n",
    "\n",
    "        # Decode the token slice back into text and add it to the list of chunks\n",
    "        chunks.append(encoding.decode(chunk_tokens))\n",
    "\n",
    "    # Return the full list of overlapping text chunks\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Function to clean extracted text by:\n",
    "# - removing headers/footers\n",
    "# - removing noise\n",
    "# - fixing multi-column layout issues\n",
    "# ---------------------------------------------------------------\n",
    "# This function is useful for preprocessing text extracted from PDFs\n",
    "# (e.g., via OCR or PDF parsers), which often contain artifacts such as\n",
    "# page numbers, repeating headers/footers, hyphenated line breaks,\n",
    "# and broken column layouts.\n",
    "\n",
    "import re  # Regular expressions for pattern matching and substitution\n",
    "\n",
    "def clean_extracted_text(text):\n",
    "    \"\"\"Remove noise (page numbers, headers, footers), merge hyphenated words,\n",
    "    and flatten potential two-column layouts.\"\"\"\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # 1. Remove page numbering and common artifacts\n",
    "    # ---------------------------------------------------------------\n",
    "    # These patterns often appear in academic papers, reports, and government documents.\n",
    "    # Removing them improves the quality of downstream embedding and summarization.\n",
    "\n",
    "    text = re.sub(r'\\bPage \\d+\\b', '', text, flags=re.IGNORECASE)  # Remove 'Page X'\n",
    "    text = re.sub(r'\\d+ of \\d+', '', text, flags=re.IGNORECASE)    # Remove 'X of Y' style page counts\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # 2. Identify and remove repeating headers/footers\n",
    "    # ---------------------------------------------------------------\n",
    "    # Strategy: count how many times each line occurs.\n",
    "    # Merge two-column text by pairing lines\n",
    "    merged_lines = []\n",
    "    lines = text.split('\\n')\n",
    "    for i in range(0, len(lines), 2):\n",
    "        if i+1 < len(lines):\n",
    "            merged_lines.append(lines[i] + \" \" + lines[i+1])\n",
    "        else:\n",
    "            merged_lines.append(lines[i])\n",
    "    return \"\\n\".join(merged_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Function to extract, clean, and chunk research paper PDFs\n",
    "# ---------------------------------------------------------------\n",
    "# This function performs a full preprocessing pipeline for PDF documents,\n",
    "# including text extraction (via PyPDF), cleaning (removing headers/footers, noise),\n",
    "# and token-based chunking for use in downstream RAG pipelines.\n",
    "\n",
    "from pypdf import PdfReader  # PyPDF is used for reading PDF documents and extracting text\n",
    "\n",
    "# Additional imports for patching\n",
    "from pdf2image import convert_from_path  # Convert PDF pages to images\n",
    "import pytesseract  # OCR engine to extract text from images\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def process_pdf_multistage(file_path):\n",
    "    # Initialize an empty string to collect the full text from the PDF\n",
    "    content = \"\"\n",
    "\n",
    "    # Extract filename for metadata\n",
    "    filename = os.path.basename(file_path)\n",
    "    author = None\n",
    "    creation_date = None\n",
    "    num_pages = None\n",
    "\n",
    "    try:\n",
    "        # ---------------------------------------------------------------\n",
    "        # 1. Attempt to load and parse the PDF\n",
    "        # ---------------------------------------------------------------\n",
    "        reader = PdfReader(file_path)  # Create a PdfReader object from the file path\n",
    "\n",
    "        # Get basic document metadata (if available)\n",
    "        meta = reader.metadata or {}\n",
    "        author = meta.get('/Author', None)\n",
    "\n",
    "        # Iterate through each page in the PDF\n",
    "        for page in reader.pages:\n",
    "            # Extract text from the page; if extraction fails or returns None, use an empty string\n",
    "            page_text = page.extract_text() or \"\"\n",
    "\n",
    "            # Append the page's text to the full document content\n",
    "            content += page_text\n",
    "\n",
    "    except Exception as e:\n",
    "        # ---------------------------------------------------------------\n",
    "        # 2. Handle extraction failures gracefully\n",
    "        # ---------------------------------------------------------------\n",
    "        # If any exception is raised during PDF reading or parsing,\n",
    "        # log the error and allow the function to continue (returning empty chunks).\n",
    "        print(f\"PyPDF extraction failed: {e}\")\n",
    "        print(\"Falling back to OCR...\")\n",
    "\n",
    "        try:\n",
    "            # Convert PDF pages to images using pdf2image\n",
    "            images = convert_from_path(file_path)\n",
    "            ocr_text_list = []\n",
    "\n",
    "            for i, img in enumerate(images):\n",
    "                # Run OCR on each image page using pytesseract\n",
    "                page_text = pytesseract.image_to_string(img)\n",
    "                ocr_text_list.append(page_text)\n",
    "\n",
    "            # Combine all OCR'd page text into one document\n",
    "            content = \"\\n\".join(ocr_text_list)\n",
    "\n",
    "        except Exception as ocr_error:\n",
    "            print(f\"OCR fallback also failed: {ocr_error}\")\n",
    "            return []\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # 3. Clean the raw extracted text\n",
    "    # ---------------------------------------------------------------\n",
    "    # Use a dedicated cleaning function to:\n",
    "    # - Remove headers, footers, and page numbers\n",
    "    # - Merge hyphenated line breaks\n",
    "    # - Flatten multi-column layouts\n",
    "    # This improves the quality of embeddings and downstream retrieval.\n",
    "    cleaned_text = clean_extracted_text(content)\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # 4. Chunk the cleaned text into token-bounded segments\n",
    "    # ---------------------------------------------------------------\n",
    "    # Break the cleaned document into overlapping token chunks (e.g., 3000 tokens with 200-token overlap),\n",
    "    # ensuring context continuity across chunks. This is critical for performance in RAG.\n",
    "    chunks = chunk_text(cleaned_text)\n",
    "\n",
    "    def safe_str(obj):\n",
    "        \"\"\"\n",
    "        Convert a potentially non-serializable object (e.g., PyPDF's IndirectObject)\n",
    "        into a JSON-compatible Python string or None.\n",
    "\n",
    "        This is especially useful when working with metadata fields extracted from PDFs,\n",
    "        where objects may be wrapped in non-primitive types (like PyPDF2.generic.IndirectObject),\n",
    "        which the `json` module cannot serialize directly.\n",
    "\n",
    "        Returns:\n",
    "            - `str(obj)` if the object can be stringified without error\n",
    "            - `None` if string conversion fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Attempt to cast the object to a string (e.g., IndirectObject → str)\n",
    "            # This is usually sufficient for basic metadata like author, title, date, etc.\n",
    "            return str(obj)\n",
    "        \n",
    "        except Exception:\n",
    "            # If casting to string fails (e.g., object is not readable or triggers an exception),\n",
    "            # return None instead, making the output JSON-safe.\n",
    "            return None\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # 4.1 Attach file-level metadata to each chunk\n",
    "    # ---------------------------------------------------------------\n",
    "    # This metadata can help with filtering, attribution, and retrieval analysis.\n",
    "    chunks_with_metadata = [\n",
    "        {\n",
    "            \"text\": chunk,\n",
    "            \"metadata\": {\n",
    "                \"source_file\": safe_str(filename),\n",
    "                \"author\": safe_str(author),\n",
    "                \"creation_date\": safe_str(creation_date),\n",
    "                \"num_pages\": safe_str(num_pages),\n",
    "                \"chunk_index\": i\n",
    "            }\n",
    "        }\n",
    "        for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # 5. Return the processed chunks\n",
    "    # ---------------------------------------------------------------\n",
    "    # The final output is a list of text chunks, ready for embedding, storage, or retrieval.\n",
    "    return chunks_with_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: data/raw/AAAI IAA CV_cleaned.pdf\n",
      "Error processing data/raw/AAAI IAA CV_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Sim of Decon_cleaned.pdf\n",
      "Error processing data/raw/Sim of Decon_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/BotBuster___AAAI_cleaned.pdf\n",
      "Error processing data/raw/BotBuster___AAAI_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Political_Networks_Conference_cleaned.pdf\n",
      "Error processing data/raw/Political_Networks_Conference_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/EmergencyResponseAI_cleaned.pdf\n",
      "Error processing data/raw/EmergencyResponseAI_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/FSS-19_paper_137_cleaned.pdf\n",
      "Error processing data/raw/FSS-19_paper_137_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/DIVERSE_LLM_Dataset___IEEE_Big_Data_cleaned.pdf\n",
      "Error processing data/raw/DIVERSE_LLM_Dataset___IEEE_Big_Data_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Clustering_Analysis_of_Website_Usage_on_Twitter_during_the_COVID_19_Pandemic_cleaned.pdf\n",
      "Error processing data/raw/Clustering_Analysis_of_Website_Usage_on_Twitter_during_the_COVID_19_Pandemic_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Cohort_Optimization_Methods_SNAMS_2021_working_draft (4)_cleaned.pdf\n",
      "Error processing data/raw/Cohort_Optimization_Methods_SNAMS_2021_working_draft (4)_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Lead-Azide_cleaned.pdf\n",
      "Error processing data/raw/Lead-Azide_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Knowing the Terrain_cleaned.pdf\n",
      "Error processing data/raw/Knowing the Terrain_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Leadership of Data Annotation 20180304v2_cleaned.pdf\n",
      "Error processing data/raw/Leadership of Data Annotation 20180304v2_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/A_Complex_Network_Approach_to_Find_Latent_Terorrist_Communities_cleaned.pdf\n",
      "Error processing data/raw/A_Complex_Network_Approach_to_Find_Latent_Terorrist_Communities_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Designed Networks_cleaned.pdf\n",
      "Error processing data/raw/Designed Networks_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Organizational risk using network analysis_cleaned.pdf\n",
      "Error processing data/raw/Organizational risk using network analysis_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/LongNetViewerORA_cleaned.pdf\n",
      "Error processing data/raw/LongNetViewerORA_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Unobtrusive Email_cleaned.pdf\n",
      "Error processing data/raw/Unobtrusive Email_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/SM Customer Feedback_FAB_2019_rev3_cleaned.pdf\n",
      "Error processing data/raw/SM Customer Feedback_FAB_2019_rev3_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Benson_MA491_NLP_cleaned.pdf\n",
      "Error processing data/raw/Benson_MA491_NLP_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/SecurityPrivAIML_cleaned.pdf\n",
      "Error processing data/raw/SecurityPrivAIML_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/SocNetAlQaeda_cleaned.pdf\n",
      "Error processing data/raw/SocNetAlQaeda_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/COVID Bayesian Data Aug_cleaned.pdf\n",
      "Error processing data/raw/COVID Bayesian Data Aug_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Savas_cleaned.pdf\n",
      "Error processing data/raw/Savas_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Lessons from Advising in Afghanistan_cleaned.pdf\n",
      "Error processing data/raw/Lessons from Advising in Afghanistan_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Confidence_Chaining_cleaned.pdf\n",
      "Error processing data/raw/Confidence_Chaining_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Limit Velocity_cleaned.pdf\n",
      "Error processing data/raw/Limit Velocity_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Text Analysis Using Automated Language Translators_cleaned.pdf\n",
      "Error processing data/raw/Text Analysis Using Automated Language Translators_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Multi_Agent_Systems_for_Frame_Detection_cleaned.pdf\n",
      "Error processing data/raw/Multi_Agent_Systems_for_Frame_Detection_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/IkekNet1_cleaned.pdf\n",
      "Error processing data/raw/IkekNet1_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Dissertation_cleaned.pdf\n",
      "Error processing data/raw/Dissertation_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/LLM_UQ_cleaned.pdf\n",
      "Error processing data/raw/LLM_UQ_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Parler_Disinformation_Challenge___CMOT_Extended_cleaned.pdf\n",
      "Error processing data/raw/Parler_Disinformation_Challenge___CMOT_Extended_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/LSA email_cleaned.pdf\n",
      "Error processing data/raw/LSA email_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Political Party Cohesion_cleaned.pdf\n",
      "Error processing data/raw/Political Party Cohesion_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Spectral Analysis SNA_cleaned.pdf\n",
      "Error processing data/raw/Spectral Analysis SNA_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/NeuroCogInfluence_cleaned.pdf\n",
      "Error processing data/raw/NeuroCogInfluence_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Simmelian-Gamma-LDA_cleaned.pdf\n",
      "Error processing data/raw/Simmelian-Gamma-LDA_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/MIPB-CDA_cleaned.pdf\n",
      "Error processing data/raw/MIPB-CDA_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/CausalOrgInorgContent_cleaned.pdf\n",
      "Error processing data/raw/CausalOrgInorgContent_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Genetic_Algorithms_for_Prompt_Optimization_cleaned.pdf\n",
      "Error processing data/raw/Genetic_Algorithms_for_Prompt_Optimization_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/ALL18_cleaned.pdf\n",
      "Error processing data/raw/ALL18_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/LLM_Confidence_Metrics_cleaned.pdf\n",
      "Error processing data/raw/LLM_Confidence_Metrics_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/FBI_Recruit_Hire_Final_cleaned.pdf\n",
      "Error processing data/raw/FBI_Recruit_Hire_Final_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/ICWSM_2025_Political_Bias_cleaned.pdf\n",
      "Error processing data/raw/ICWSM_2025_Political_Bias_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Review of R Packages_20161026_cleaned.pdf\n",
      "Error processing data/raw/Review of R Packages_20161026_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Evolution_of_Terrorism_PNAS_cleaned.pdf\n",
      "Error processing data/raw/Evolution_of_Terrorism_PNAS_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Planning for AI Sustainment A Methodology for Maintenance and Cost Management_V5_cleaned.pdf\n",
      "Error processing data/raw/Planning for AI Sustainment A Methodology for Maintenance and Cost Management_V5_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Social Media Mental Health Final_cleaned.pdf\n",
      "Error processing data/raw/Social Media Mental Health Final_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Cross_Platform_Information_Spread_During_the_January_6th_Capitol_Riots_cleaned.pdf\n",
      "Error processing data/raw/Cross_Platform_Information_Spread_During_the_January_6th_Capitol_Riots_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Multi_view_Clustering_for_Social_Based_Data_cleaned.pdf\n",
      "Error processing data/raw/Multi_view_Clustering_for_Social_Based_Data_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Extreme Cohesion Darknet 20190815_cleaned.pdf\n",
      "Error processing data/raw/Extreme Cohesion Darknet 20190815_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Social_Det_COVID_Mortality_cleaned.pdf\n",
      "Error processing data/raw/Social_Det_COVID_Mortality_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/White Paper Brain Gaze_cleaned.pdf\n",
      "Error processing data/raw/White Paper Brain Gaze_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Encyclopedia of SNA - R Packages_cleaned.pdf\n",
      "Error processing data/raw/Encyclopedia of SNA - R Packages_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/CUSUM Parameterization_cleaned.pdf\n",
      "Error processing data/raw/CUSUM Parameterization_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Tweets-to-touchdowns_cleaned.pdf\n",
      "Error processing data/raw/Tweets-to-touchdowns_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/SocNetChgDet_cleaned.pdf\n",
      "Error processing data/raw/SocNetChgDet_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/23-US-DHS-001_cleaned.pdf\n",
      "Error processing data/raw/23-US-DHS-001_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Analysis_of_Malware_Communities_Using_Multi_Modal_Features_cleaned.pdf\n",
      "Error processing data/raw/Analysis_of_Malware_Communities_Using_Multi_Modal_Features_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Acquiring Maintainable AI_Enable Systems_Final_cleaned.pdf\n",
      "Error processing data/raw/Acquiring Maintainable AI_Enable Systems_Final_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Food Addiction 20231222 v3_cleaned.pdf\n",
      "Error processing data/raw/Food Addiction 20231222 v3_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/HIV_cleaned.pdf\n",
      "Error processing data/raw/HIV_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/TrainingSetSize_cleaned.pdf\n",
      "Error processing data/raw/TrainingSetSize_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/WEIRD_cleaned.pdf\n",
      "Error processing data/raw/WEIRD_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Dormant Bots 20190814_cleaned.pdf\n",
      "Error processing data/raw/Dormant Bots 20190814_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/The ABCs of AI-Enabled Intelligence Analysis - War on the Rocks_cleaned.pdf\n",
      "Error processing data/raw/The ABCs of AI-Enabled Intelligence Analysis - War on the Rocks_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/improving-decision-support-for-organ-transplant_cleaned.pdf\n",
      "Error processing data/raw/improving-decision-support-for-organ-transplant_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Leveraging_AI_to_Improve_Viral_Information_Detection_in_Online_Discourse_cleaned.pdf\n",
      "Error processing data/raw/Leveraging_AI_to_Improve_Viral_Information_Detection_in_Online_Discourse_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Vulnerable_Code_Detection_cleaned.pdf\n",
      "Error processing data/raw/Vulnerable_Code_Detection_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Overcoming_Social_Media_API_Restrictions__Building_an_Effective_Web_Scraper_cleaned.pdf\n",
      "Error processing data/raw/Overcoming_Social_Media_API_Restrictions__Building_an_Effective_Web_Scraper_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Supply Chain Excellence_cleaned.pdf\n",
      "Error processing data/raw/Supply Chain Excellence_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Reforming Sectarian Beliefs_cleaned.pdf\n",
      "Error processing data/raw/Reforming Sectarian Beliefs_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/ICWSM___Use_of_Large_Language_Models_for_Stance_Classification_cleaned.pdf\n",
      "Error processing data/raw/ICWSM___Use_of_Large_Language_Models_for_Stance_Classification_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Course Info Security_cleaned.pdf\n",
      "Error processing data/raw/Course Info Security_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Take_boards_cleaned.pdf\n",
      "Error processing data/raw/Take_boards_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Hashtag_Revival_cleaned.pdf\n",
      "Error processing data/raw/Hashtag_Revival_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Misinformation_Simulation_cleaned.pdf\n",
      "Error processing data/raw/Misinformation_Simulation_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Network Simulation Models_cleaned.pdf\n",
      "Error processing data/raw/Network Simulation Models_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/cycon-final-draft_cleaned.pdf\n",
      "Error processing data/raw/cycon-final-draft_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/NAP Behavioral Sci Intel_cleaned.pdf\n",
      "Error processing data/raw/NAP Behavioral Sci Intel_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Utility Seeking in Complex Social Systems_cleaned.pdf\n",
      "Error processing data/raw/Utility Seeking in Complex Social Systems_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/k-truss_cleaned.pdf\n",
      "Error processing data/raw/k-truss_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Arrow White Paper DExTra_cleaned.pdf\n",
      "Error processing data/raw/Arrow White Paper DExTra_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Helene_and_Milton_ACM_cleaned.pdf\n",
      "Error processing data/raw/Helene_and_Milton_ACM_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Data_Education__Emerging_Challenges_and_Opportunities_cleaned.pdf\n",
      "Error processing data/raw/Data_Education__Emerging_Challenges_and_Opportunities_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/RatingsVRankings_cleaned.pdf\n",
      "Error processing data/raw/RatingsVRankings_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/RES2D_cleaned.pdf\n",
      "Error processing data/raw/RES2D_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/McCullohCarleyJOSS_cleaned.pdf\n",
      "Error processing data/raw/McCullohCarleyJOSS_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/ONA-using-igraph_cleaned.pdf\n",
      "Error processing data/raw/ONA-using-igraph_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Vol33Iss1_INSNApdf_cleaned.pdf\n",
      "Error processing data/raw/Vol33Iss1_INSNApdf_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Kidney_Behavioral_cleaned.pdf\n",
      "Error processing data/raw/Kidney_Behavioral_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/ClassifiersCrowdSource_cleaned.pdf\n",
      "Error processing data/raw/ClassifiersCrowdSource_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/MLTEing_Models_for_NIER_at_ICSE_2023_cleaned.pdf\n",
      "Error processing data/raw/MLTEing_Models_for_NIER_at_ICSE_2023_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Quantifying_Information_Advantage_cleaned.pdf\n",
      "Error processing data/raw/Quantifying_Information_Advantage_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/NeuroSynchrony_cleaned.pdf\n",
      "Error processing data/raw/NeuroSynchrony_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Frontiers COVID_cleaned.pdf\n",
      "Error processing data/raw/Frontiers COVID_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Kent2022_Chapter_MicroscopicMarkovChainApproach_cleaned.pdf\n",
      "Error processing data/raw/Kent2022_Chapter_MicroscopicMarkovChainApproach_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/jfq-110_46-53_Cruickshank_cleaned.pdf\n",
      "Error processing data/raw/jfq-110_46-53_Cruickshank_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Sailer McCulloh Soc Net and Spatial Config_cleaned.pdf\n",
      "Error processing data/raw/Sailer McCulloh Soc Net and Spatial Config_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/MOOC 20190828_cleaned.pdf\n",
      "Error processing data/raw/MOOC 20190828_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/2021_EPJ_MVMCInfoOps_cleaned.pdf\n",
      "Error processing data/raw/2021_EPJ_MVMCInfoOps_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/ONA-in-R_cleaned.pdf\n",
      "Error processing data/raw/ONA-in-R_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/docnet_cleaned.pdf\n",
      "Error processing data/raw/docnet_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Characterizing_Communities_of_Hashtag_Usage_on_Twitter_During_the_2020_COVID_19_Pandemic_cleaned.pdf\n",
      "Error processing data/raw/Characterizing_Communities_of_Hashtag_Usage_on_Twitter_During_the_2020_COVID_19_Pandemic_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Symbolic Generative AI 20231012_cleaned.pdf\n",
      "Error processing data/raw/Symbolic Generative AI 20231012_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/NBA Performance_cleaned.pdf\n",
      "Error processing data/raw/NBA Performance_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Chat GPT Bias final w copyright_cleaned.pdf\n",
      "Error processing data/raw/Chat GPT Bias final w copyright_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/IkeNet_cleaned.pdf\n",
      "Error processing data/raw/IkeNet_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/On the Science of Networks_cleaned.pdf\n",
      "Error processing data/raw/On the Science of Networks_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/Social_Network_Probability_Mechanics_cleaned.pdf\n",
      "Error processing data/raw/Social_Network_Probability_Mechanics_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/2024_ICWSM_Data_Challenge__Post_API_Data_Collection_cleaned.pdf\n",
      "Error processing data/raw/2024_ICWSM_Data_Challenge__Post_API_Data_Collection_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Processing: data/raw/YouTube-COVID_cleaned.pdf\n",
      "Error processing data/raw/YouTube-COVID_cleaned.pdf: name 'process_pdf_multistage' is not defined\n",
      "Saved cleaned + chunked text for 0 PDFs to data/cleaned_chunked_papers.json\n"
     ]
    }
   ],
   "source": [
    "# Extract, clean, chunk, and store raw chunks for all research paper PDFs\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1. Import necessary libraries\n",
    "# ---------------------------------------------------------------\n",
    "import os              # Used for file path manipulation and directory handling\n",
    "import json            # Used to save the final result as a JSON file\n",
    "from glob import glob  # Used to match all PDF files in a directory\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2. Set input/output paths\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# Folder containing raw research paper PDFs (to be processed)\n",
    "pdf_folder = \"data/raw\"\n",
    "\n",
    "# Output file to save cleaned + chunked results\n",
    "output_json_path = \"data/cleaned_chunked_papers.json\"\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3. Initialize storage for processed results\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# This list will store the result for each paper.\n",
    "# Each element is a dictionary with:\n",
    "#   - 'id': PDF filename\n",
    "#   - 'chunks': list of cleaned and tokenized text chunks from that PDF\n",
    "all_chunks = []\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4. Loop through all PDF files in the target folder\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# `glob` finds all .pdf files in the specified folder\n",
    "for pdf_path in glob(os.path.join(pdf_folder, \"*.pdf\")):\n",
    "    doc_name = os.path.basename(pdf_path)  # Extract just the filename (used as a unique ID)\n",
    "    print(f\"Processing: {pdf_path}\")       # Log the file being processed\n",
    "    \n",
    "    try:\n",
    "        # ---------------------------------------------------------------\n",
    "        # Attempt to extract, clean, and chunk the PDF content\n",
    "        # ---------------------------------------------------------------\n",
    "        # `process_pdf_multistage()` is your custom pipeline that:\n",
    "        #   1. Extracts text using PyPDF (and optionally OCR if needed)\n",
    "        #   2. Cleans the text (removes noise, merges hyphenated lines, etc.)\n",
    "        #   3. Chunks the cleaned text into token-bounded segments\n",
    "        chunks = process_pdf_multistage(pdf_path)\n",
    "\n",
    "        # ---------------------------------------------------------------\n",
    "        # Append the processed result to the `all_chunks` list\n",
    "        # ---------------------------------------------------------------\n",
    "        # Each record contains the filename (as ID) and a list of chunks\n",
    "        all_chunks.append({\n",
    "            \"id\": doc_name,\n",
    "            \"chunks\": chunks\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        # ---------------------------------------------------------------\n",
    "        # If anything goes wrong during processing, catch the error\n",
    "        # ---------------------------------------------------------------\n",
    "        print(f\"Error processing {pdf_path}: {e}\")  # Log the error for debugging\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 5. Save all processed results to a JSON file\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# Write the list of all processed documents to a single JSON file\n",
    "# - `indent=2` for human-readable formatting\n",
    "# - `ensure_ascii=False` allows Unicode characters (like symbols or accents)\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Final confirmation message\n",
    "print(f\"Saved cleaned + chunked text for {len(all_chunks)} PDFs to {output_json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-compute and Store Embeddings for RAG-enabled Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/genai_capstone/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects' from 'transformers.utils' (/workspaces/genai_capstone/.venv/lib/python3.10/site-packages/transformers/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOptionalDependencyNotAvailable\u001b[0m            Traceback (most recent call last)",
      "File \u001b[0;32m/workspaces/genai_capstone/.venv/lib/python3.10/site-packages/transformers/__init__.py:4444\u001b[0m\n\u001b[1;32m   4437\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m   4438\u001b[0m         is_librosa_available()\n\u001b[1;32m   4439\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m is_essentia_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4442\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m is_pretty_midi_available()\n\u001b[1;32m   4443\u001b[0m     ):\n\u001b[0;32m-> 4444\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m OptionalDependencyNotAvailable()\n\u001b[1;32m   4445\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OptionalDependencyNotAvailable:\n",
      "\u001b[0;31mOptionalDependencyNotAvailable\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check torch version\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch version:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch path:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m)\n",
      "File \u001b[0;32m/workspaces/genai_capstone/.venv/lib/python3.10/site-packages/transformers/__init__.py:4446\u001b[0m\n\u001b[1;32m   4444\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m OptionalDependencyNotAvailable()\n\u001b[1;32m   4445\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OptionalDependencyNotAvailable:\n\u001b[0;32m-> 4446\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m   4447\u001b[0m         dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects,\n\u001b[1;32m   4448\u001b[0m     )\n\u001b[1;32m   4450\u001b[0m     _import_structure[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutils.dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   4451\u001b[0m         name\n\u001b[1;32m   4452\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects)\n\u001b[1;32m   4453\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4454\u001b[0m     ]\n\u001b[1;32m   4455\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects' from 'transformers.utils' (/workspaces/genai_capstone/.venv/lib/python3.10/site-packages/transformers/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Check torch version\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Torch path:\", torch.__file__)\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"Transformers path:\", transformers.__file__)\n",
    "print(\"uint64 exists:\", hasattr(torch, \"uint64\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nmodule 'torch' has no attribute 'uint64'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/workspaces/genai_capstone/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1364\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/workspaces/genai_capstone/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:42\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     BaseModelOutputWithPastAndCrossAttentions,\n\u001b[1;32m     33\u001b[0m     BaseModelOutputWithPoolingAndCrossAttentions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     TokenClassifierOutput,\n\u001b[1;32m     41\u001b[0m )\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n",
      "File \u001b[0;32m/workspaces/genai_capstone/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:46\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     Conv1D,\n\u001b[1;32m     48\u001b[0m     apply_chunking_to_forward,\n\u001b[1;32m     49\u001b[0m     find_pruneable_heads_and_indices,\n\u001b[1;32m     50\u001b[0m     id_tensor_storage,\n\u001b[1;32m     51\u001b[0m     is_torch_greater_or_equal_than_1_13,\n\u001b[1;32m     52\u001b[0m     prune_conv1d_layer,\n\u001b[1;32m     53\u001b[0m     prune_layer,\n\u001b[1;32m     54\u001b[0m     prune_linear_layer,\n\u001b[1;32m     55\u001b[0m )\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msafetensors_conversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m auto_conversion\n",
      "File \u001b[0;32m/workspaces/genai_capstone/.venv/lib/python3.10/site-packages/transformers/pytorch_utils.py:19\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msafetensors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m storage_ptr, storage_size\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n",
      "File \u001b[0;32m/workspaces/genai_capstone/.venv/lib/python3.10/site-packages/safetensors/torch.py:439\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Version(torch\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m>\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    437\u001b[0m     _SIZE\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    438\u001b[0m         {\n\u001b[0;32m--> 439\u001b[0m             \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint64\u001b[49m: \u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m    440\u001b[0m             torch\u001b[38;5;241m.\u001b[39muint32: \u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m    441\u001b[0m             torch\u001b[38;5;241m.\u001b[39muint16: \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    442\u001b[0m         }\n\u001b[1;32m    443\u001b[0m     )\n\u001b[1;32m    445\u001b[0m _TYPES \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF64\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfloat64,\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF32\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF8_E5M2\u001b[39m\u001b[38;5;124m\"\u001b[39m: _float8_e5m2,\n\u001b[1;32m    458\u001b[0m }\n",
      "File \u001b[0;32m/workspaces/genai_capstone/.venv/lib/python3.10/site-packages/torch/__init__.py:1833\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   1831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m-> 1833\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'uint64'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 1. Initialize embedding model\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# This wraps a HuggingFace model (MiniLM) so it can be used with LangChain.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# MiniLM is a lightweight transformer model that produces sentence embeddings.\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mall-MiniLM-L6-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 2. Set the persistent storage directory for Chroma\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# This is where Chroma will store the vector index on disk.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# The directory is placed *outside the repo* to avoid accidentally committing large files to Git.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m persist_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workspaces/chroma_storage/chroma_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/workspaces/genai_capstone/.venv/lib/python3.10/site-packages/langchain_huggingface/embeddings/huggingface.py:59\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import sentence_transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m \u001b[43msentence_transformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/genai_capstone/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:191\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, cache_folder, trust_remote_code, revision, token, use_auth_token)\u001b[0m\n\u001b[1;32m    188\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(model_name_or_path, token, cache_folder\u001b[38;5;241m=\u001b[39mcache_folder, revision\u001b[38;5;241m=\u001b[39mrevision):\n\u001b[0;32m--> 191\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[1;32m    200\u001b[0m         model_name_or_path,\n\u001b[1;32m    201\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m    205\u001b[0m     )\n",
      "File \u001b[0;32m/workspaces/genai_capstone/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1233\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code)\u001b[0m\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1232\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_args\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m hub_kwargs\n\u001b[0;32m-> 1233\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1235\u001b[0m     \u001b[38;5;66;03m# Normalize does not require any files to be loaded\u001b[39;00m\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_class \u001b[38;5;241m==\u001b[39m Normalize:\n",
      "File \u001b[0;32m/workspaces/genai_capstone/.venv/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:36\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_lower_case \u001b[38;5;241m=\u001b[39m do_lower_case\n\u001b[1;32m     35\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     39\u001b[0m     tokenizer_name_or_path \u001b[38;5;28;01mif\u001b[39;00m tokenizer_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model_name_or_path,\n\u001b[1;32m     40\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_args,\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# No max_seq_length set. Try to infer from model\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/genai_capstone/.venv/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:65\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[0;34m(self, model_name_or_path, config, cache_dir, **model_args)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_mt5_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/genai_capstone/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:565\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    562\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m \u001b[43m_get_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    567\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    568\u001b[0m     )\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m/workspaces/genai_capstone/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:387\u001b[0m, in \u001b[0;36m_get_model_class\u001b[0;34m(config, model_mapping)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_model_class\u001b[39m(config, model_mapping):\n\u001b[0;32m--> 387\u001b[0m     supported_models \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(supported_models, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m supported_models\n",
      "File \u001b[0;32m/workspaces/genai_capstone/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:740\u001b[0m, in \u001b[0;36m_LazyAutoMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping:\n\u001b[1;32m    739\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping[model_type]\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;66;03m# Maybe there was several model types associated with this config.\u001b[39;00m\n\u001b[1;32m    743\u001b[0m model_types \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;241m==\u001b[39m key\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m]\n",
      "File \u001b[0;32m/workspaces/genai_capstone/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:754\u001b[0m, in \u001b[0;36m_LazyAutoMapping._load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[module_name] \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/genai_capstone/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:698\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attr, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(getattribute_from_module(module, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attr)\n\u001b[0;32m--> 698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr)\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;66;03m# object at the top level.\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/genai_capstone/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1354\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1352\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1354\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1355\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/workspaces/genai_capstone/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1366\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1367\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1368\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1369\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nmodule 'torch' has no attribute 'uint64'"
     ]
    }
   ],
   "source": [
    "# Import Chroma vectorstore and HuggingFace embedding wrapper from LangChain\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1. Initialize embedding model\n",
    "# ---------------------------------------------------------------\n",
    "# This wraps a HuggingFace model (MiniLM) so it can be used with LangChain.\n",
    "# MiniLM is a lightweight transformer model that produces sentence embeddings.\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2. Set the persistent storage directory for Chroma\n",
    "# ---------------------------------------------------------------\n",
    "# This is where Chroma will store the vector index on disk.\n",
    "# The directory is placed *outside the repo* to avoid accidentally committing large files to Git.\n",
    "persist_dir = \"/workspaces/chroma_storage/chroma_embeddings\"\n",
    "\n",
    "# Create the directory if it doesn’t exist (idempotent)\n",
    "os.makedirs(persist_dir, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3. Prepare LangChain Document objects\n",
    "# ---------------------------------------------------------------\n",
    "# LangChain expects documents in a specific format: each one must be a Document object\n",
    "# containing `page_content` (the raw text) and optional `metadata`.\n",
    "# Here, we pair each text chunk with its corresponding paper ID.\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "docs = [\n",
    "    Document(page_content=chunk, metadata={\"paper_id\": paper_id})\n",
    "    for chunk, paper_id in zip(all_chunks, chunk_to_paper)\n",
    "]\n",
    "# - `all_chunks`: list of text snippets extracted from research papers\n",
    "# - `chunk_to_paper`: list of paper IDs, one for each chunk, to track provenance\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4. Create and persist Chroma vectorstore\n",
    "# ---------------------------------------------------------------\n",
    "# This creates a Chroma vector index and embeds all documents using the MiniLM model.\n",
    "# The index will be saved to `persist_dir`, allowing it to be reloaded later without recomputation.\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,                     # List of Document objects with chunk text + metadata\n",
    "    embedding=embedding_model,          # Embedding function used to vectorize each document\n",
    "    collection_name=\"research_chunks\",  # Name of the collection (can be queried later)\n",
    "    persist_directory=persist_dir,      # Filesystem path where vectors and metadata are stored\n",
    "    client_settings={\"is_persistent\": True}  # activates server/client mode\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 5. Confirm that embeddings were stored\n",
    "# ---------------------------------------------------------------\n",
    "# Show the files written by Chroma in the storage directory.\n",
    "# This confirms that the persistent index exists and can be reloaded later.\n",
    "import os\n",
    "print(os.listdir(\"./chroma_embeddings\"))\n",
    "\n",
    "# Final confirmation message\n",
    "print(\"Embeddings precomputed and stored in ChromaDB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Generic query to test retrieval from ChromaDB\n",
    "# ---------------------------------------------------------------\n",
    "# This block is used to perform a semantic search query over previously embedded and stored text chunks.\n",
    "# It demonstrates how to reload a persisted Chroma vectorstore and retrieve the top-k most relevant documents\n",
    "# based on a natural language query.\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Reload vectorstore (no need to re-embed)\n",
    "# ---------------------------------------------------------------\n",
    "# We reload the Chroma vectorstore from disk using the same collection name and persistence directory\n",
    "# that were used during initial indexing. This allows the session to access precomputed embeddings and metadata\n",
    "# without recomputing or reprocessing the original documents.\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"research_chunks\",     # Must match the name used in .from_documents()\n",
    "    embedding_function=embedding_model,    # Same embedding function as before (MiniLM)\n",
    "    persist_directory=persist_dir          # Path where vector index and metadata were stored\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Set up the query topic (can be static or dynamic)\n",
    "# ---------------------------------------------------------------\n",
    "# This is the search string used to query the vectorstore. The embedding model will convert this query\n",
    "# into a vector, and Chroma will use vector similarity to retrieve the most relevant chunks.\n",
    "# In this example, the priority topic is a simple string, but in production it could be generated\n",
    "# dynamically from a NOFO or user input.\n",
    "\n",
    "priority_topic = \"mental health\"  # Example query topic, e.g., extracted from a NOFO\n",
    "\n",
    "query = priority_topic  # Alias for clarity — supports swapping in a different query object\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Run similarity search\n",
    "# ---------------------------------------------------------------\n",
    "# Chroma performs a nearest-neighbor search in vector space using cosine similarity.\n",
    "# The result is a list of the top-k most similar documents (k=5 in this case).\n",
    "# Each result is a Document object containing both content and metadata.\n",
    "\n",
    "results = vectorstore.similarity_search(query, k=5)  # Return top 5 most relevant chunks\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Display results\n",
    "# ---------------------------------------------------------------\n",
    "# For each result, we print:\n",
    "# - The paper ID from metadata (to identify the source)\n",
    "# - A 250-character snippet of the matched chunk (for preview)\n",
    "# This is useful for debugging or validating whether the retrieval matches user expectations.\n",
    "\n",
    "for r in results:\n",
    "    print(f\"{r.metadata['paper_id']}:\\n{r.page_content[:250]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZG1Ah1eDi7aK"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Step 1: Topic Extraction - [3 Marks]**\n",
    "\n",
    "> **Read the NOFO doc and identify the topic for which the funding is to be given.**\n",
    "---\n",
    "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt and 1 mark for the successful completion of this section, including debugging or modifying the code if necessary.*\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkW_lO_CHSpc"
   },
   "source": [
    "**TASK:** Write an LLM prompt to extract the Topic for what the funding is been provided, from the NOFO document, Ask the LLM to respond back with the topic name only and nothing else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DcEpaUYNM_W"
   },
   "outputs": [],
   "source": [
    "# Topic extraction prompt\n",
    "topic_extraction_prompt = f\"\"\"\n",
    "You are a research grant specialist with expertise in analyzing NIH funding announcements and extracting key research priorities.\n",
    "\n",
    "Your task: Analyze this NOFO document from the National Institute of Mental Health (NIMH) to identify the PRIMARY funding topic.\n",
    "\n",
    "The document may describe multiple research areas, objectives, and priorities. Extract the single overarching topic that encompasses the main focus of this funding opportunity.\n",
    "\n",
    "Return ONLY the primary topic in 3-8 words. No explanations, descriptions, or additional text.\n",
    "\n",
    "Document:\n",
    "{NOFO_pdf[0].page_content}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "WtmCEbaKN9aW",
    "outputId": "7f9d4f38-5e38-4c9c-f70a-3976dfb8c881"
   },
   "outputs": [],
   "source": [
    "# Finding the topic for which the Funding is been given\n",
    "topic_extraction = llm.invoke(topic_extraction_prompt)\n",
    "topic = topic_extraction.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot Prompt Setup for Assessing Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"prompt_evaluation_log_cleaned.json\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# FEW-SHOT FALLBACK EXAMPLES\n",
    "# ------------------------------------------------------------\n",
    "FALLBACK_EXAMPLES = [\n",
    "    (\n",
    "        \"Digital CBT for Adolescents\",\n",
    "        \"\"\"{\n",
    "  \"criteria_results\": {\n",
    "    \"domain_relevance\": \"YES - focuses on mental health digital interventions\",\n",
    "    \"methodological_alignment\": \"YES - randomized controlled trial design\",\n",
    "    \"theoretical_connection\": \"NO - lacks explicit framework reference\",\n",
    "    \"practical_application\": \"YES - informs deployment in youth settings\"\n",
    "  },\n",
    "  \"decision\": \"RELEVANT\",\n",
    "  \"confidence\": \"85\",\n",
    "  \"summary\": \"This study evaluates a mobile CBT app for adolescents, showing significant reduction in anxiety and depression symptoms compared to control. It highlights engagement strategies relevant to NOFO objectives.\"\n",
    "}\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"Oncology Drug Delivery Review\",\n",
    "        \"\"\"{\n",
    "  \"criteria_results\": {\n",
    "    \"domain_relevance\": \"NO - focuses on oncology drug mechanisms\",\n",
    "    \"methodological_alignment\": \"NO\",\n",
    "    \"theoretical_connection\": \"NO\",\n",
    "    \"practical_application\": \"NO\"\n",
    "  },\n",
    "  \"decision\": \"PAPER NOT RELATED TO TOPIC\",\n",
    "  \"confidence\": \"0\",\n",
    "  \"summary\": null\n",
    "}\"\"\"\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# FEW-SHOT RETRIEVAL + PROMPT BUILDER\n",
    "# ------------------------------------------------------------\n",
    "def get_few_shot_examples(\n",
    "    json_path,\n",
    "# Define configuration for few-shot example retrieval (number of examples)\n",
    "    max_examples=FEW_SHOT_MAX_EXAMPLES,\n",
    "# Minimum confidence threshold for including examples in few-shot prompting\n",
    "    min_confidence=MIN_CONFIDENCE_FOR_FEWSHOT\n",
    "):\n",
    "    \"\"\"\n",
    "    Pulls balanced high-confidence examples from log or uses fallback if none found.\n",
    "    \"\"\"\n",
    "# Import required libraries for core functionality\n",
    "    import os, json, random\n",
    "\n",
    "    examples = []\n",
    "\n",
    "    # Attempt to pull from log\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                data = []\n",
    "\n",
    "        relevant_examples, irrelevant_examples = [], []\n",
    "\n",
    "        for iteration in data:\n",
    "            for doc in iteration.get(\"relevant_documents\", []):\n",
    "                hybrid_conf = max(doc.get(\"model_confidence\", 0), doc.get(\"rule_confidence\", 0))\n",
    "                if hybrid_conf >= min_confidence:\n",
    "                    relevant_examples.append((doc[\"title\"], doc[\"reasoning\"]))\n",
    "\n",
    "            for doc in iteration.get(\"irrelevant_documents\", []):\n",
    "                irrelevant_examples.append((doc, \"PAPER NOT RELATED TO TOPIC\"))\n",
    "\n",
    "        # Balance relevant and irrelevant (half and half)\n",
    "        half = max_examples // 2\n",
    "        random.shuffle(relevant_examples)\n",
    "        random.shuffle(irrelevant_examples)\n",
    "        selected_relevant = relevant_examples[:half]\n",
    "        selected_irrelevant = irrelevant_examples[:half]\n",
    "        examples = selected_relevant + selected_irrelevant\n",
    "\n",
    "    # Fallback if no examples found\n",
    "    if not examples:\n",
    "        print(\"No high-confidence examples found. Using fallback seed examples.\")\n",
    "        examples = FALLBACK_EXAMPLES[:max_examples]\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def build_prompt_with_examples(topic, base_prompt, examples):\n",
    "    \"\"\"\n",
    "    Prepend few-shot examples (from log or fallback) to the base prompt.\n",
    "    \"\"\"\n",
    "    if not examples:\n",
    "        return base_prompt\n",
    "\n",
    "    examples_str = \"\\n\\n\".join(\n",
    "        [f\"Example ({title}):\\n{reasoning}\" for title, reasoning in examples]\n",
    "    )\n",
    "\n",
    "    return f\"\"\"\n",
    "You are a research grant specialist evaluating research papers for relevance to NIH NOFO objectives: {topic}.\n",
    "\n",
    "Below are examples of prior evaluations for context:\n",
    "{examples_str}\n",
    "\n",
    "Now evaluate the following paper using the same structure and logic:\n",
    "\n",
    "{base_prompt}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXHRa9IlMycZ"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Step 2: Research Paper Relevance Assessment - [3 Marks]**\n",
    "> **Analyze all the Research Papers and filter out the research papers based on the topic of NOFO**\n",
    "---\n",
    "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt and 1 mark for the successful completion of this section, including debugging or modifying the code if necessary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kWc0LaCGPo3"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "**TASK:** Write an Prompt which can be used to analyze the relevance of the provided research paper in relation to the topic outlined in the NOFO (Notice of Funding Opportunity) document. Determine whether the research aligns with the goals, objectives, and funding criteria specified in the NOFO. Additionally, assess whether the research paper can be used to support or develop a viable project idea that fits within the scope of the funding opportunity.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Note:** If the paper does **not** significantly relate to the topic—by domain, method, theory, or application ask the LLM to return: **\"PAPER NOT RELATED TO TOPIC\"**\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Ask the LLM to respond in the below specified structure:\n",
    "\n",
    "```\n",
    "### Output Format:\n",
    "\"summary\": \"<summary of the paper under 300 words, or return: PAPER NOT RELATED TO TOPIC>\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3LPwoNeyXC7"
   },
   "outputs": [],
   "source": [
    "relevance_prompt_a = f\"\"\"\n",
    "You are a research grant specialist evaluating research papers for relevance to NIH NOFO objectives: {topic}.\n",
    "\n",
    "Evaluate the paper step-by-step against these criteria:\n",
    "1. Domain relevance (mental health, digital health, intervention effectiveness)\n",
    "2. Methodological alignment (clinical trials, user engagement studies, technology development)\n",
    "3. Theoretical connection (frameworks, evidence, insights for intervention design/implementation)\n",
    "4. Practical application (supports development or testing of digital mental health interventions)\n",
    "\n",
    "Instructions:\n",
    "- For EACH criterion, respond YES or NO and justify briefly.\n",
    "- A paper is RELEVANT if at least ONE criterion is YES.\n",
    "- Assign a confidence score (0–100%) to the RELEVANT decision, based on how strongly the paper meets the criteria (higher = more confident relevance).\n",
    "- If RELEVANT: provide a <300-word summary focused on digital mental health intervention insights.\n",
    "- If NOT RELEVANT: return exactly \"PAPER NOT RELATED TO TOPIC\".\n",
    "\n",
    "Output format (JSON):\n",
    "{{\n",
    "  \"criteria_results\": {{\n",
    "    \"domain_relevance\": \"YES/NO - justification\",\n",
    "    \"methodological_alignment\": \"YES/NO - justification\",\n",
    "    \"theoretical_connection\": \"YES/NO - justification\",\n",
    "    \"practical_application\": \"YES/NO - justification\"\n",
    "  }},\n",
    "  \"decision\": \"RELEVANT\" or \"PAPER NOT RELATED TO TOPIC\",\n",
    "  \"confidence\": \"<integer between 0 and 100>\",\n",
    "  \"summary\": \"<summary text or null>\"\n",
    "}}\n",
    "\n",
    "### Paper content:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# FEW-SHOT RETRIEVAL FUNCTION (needed for few_shot_examples)\n",
    "# ------------------------------------------------------------\n",
    "def get_few_shot_examples(\n",
    "    json_path,\n",
    "    max_examples=4,                 # total examples to include\n",
    "    min_confidence=70               # minimum confidence threshold\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieve few-shot examples for prompt building:\n",
    "    - Pulls from prior log entries if available\n",
    "    - Falls back to hardcoded seed examples if log is empty\n",
    "\n",
    "    Why this matters:\n",
    "    - Few-shot examples improve LLM reasoning by showing \"good answers\"\n",
    "    - Ensures consistency in relevance classification over multiple runs\n",
    "    \"\"\"\n",
    "\n",
    "    import os, json, random\n",
    "\n",
    "    examples = []\n",
    "\n",
    "    # -------------------------------\n",
    "    # 1. Attempt to load from log\n",
    "    # -------------------------------\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                data = []\n",
    "\n",
    "        relevant_examples, irrelevant_examples = [], []\n",
    "\n",
    "        # Loop through prior iterations\n",
    "        for iteration in data:\n",
    "            # Pull relevant docs with sufficient confidence\n",
    "            for doc in iteration.get(\"relevant_documents\", []):\n",
    "                hybrid_conf = max(doc.get(\"model_confidence\", 0), doc.get(\"rule_confidence\", 0))\n",
    "                if hybrid_conf >= min_confidence:\n",
    "                    relevant_examples.append((doc[\"title\"], doc[\"reasoning\"]))\n",
    "\n",
    "            # Irrelevant docs (fallback reasoning text)\n",
    "            for doc in iteration.get(\"irrelevant_documents\", []):\n",
    "                irrelevant_examples.append((doc, \"PAPER NOT RELATED TO TOPIC\"))\n",
    "\n",
    "        # Randomly select balanced examples\n",
    "        half = max_examples // 2\n",
    "        random.shuffle(relevant_examples)\n",
    "        random.shuffle(irrelevant_examples)\n",
    "        selected_relevant = relevant_examples[:half]\n",
    "        selected_irrelevant = irrelevant_examples[:half]\n",
    "        examples = selected_relevant + selected_irrelevant\n",
    "\n",
    "    # -------------------------------\n",
    "    # 2. Fallback if log is empty\n",
    "    # -------------------------------\n",
    "    if not examples:\n",
    "        print(\"No high-confidence examples found. Using fallback seed examples.\")\n",
    "        examples = [\n",
    "            (\n",
    "                \"Digital CBT for Adolescents\",\n",
    "                \"\"\"{\n",
    "  \"criteria_results\": {\n",
    "    \"domain_relevance\": \"YES - focuses on mental health digital interventions\",\n",
    "    \"methodological_alignment\": \"YES - randomized controlled trial design\",\n",
    "    \"theoretical_connection\": \"NO - lacks explicit framework reference\",\n",
    "    \"practical_application\": \"YES - informs deployment in youth settings\"\n",
    "  },\n",
    "  \"decision\": \"RELEVANT\",\n",
    "  \"confidence\": \"85\",\n",
    "  \"summary\": \"This study evaluates a mobile CBT app for adolescents, showing significant reduction in anxiety and depression symptoms compared to control.\"\n",
    "}\"\"\"\n",
    "            ),\n",
    "            (\n",
    "                \"Oncology Drug Delivery Review\",\n",
    "                \"\"\"{\n",
    "  \"criteria_results\": {\n",
    "    \"domain_relevance\": \"NO - focuses on oncology drug mechanisms\",\n",
    "    \"methodological_alignment\": \"NO\",\n",
    "    \"theoretical_connection\": \"NO\",\n",
    "    \"practical_application\": \"NO\"\n",
    "  },\n",
    "  \"decision\": \"PAPER NOT RELATED TO TOPIC\",\n",
    "  \"confidence\": \"0\",\n",
    "  \"summary\": null\n",
    "}\"\"\"\n",
    "            )\n",
    "        ][:max_examples]\n",
    "\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# FUNCTION: build_prompt_with_examples\n",
    "# ------------------------------------------------------------\n",
    "def build_prompt_with_examples(topic, base_prompt, examples):\n",
    "    \"\"\"\n",
    "    Build a few-shot prompt for relevance classification.\n",
    "\n",
    "    How it works:\n",
    "    - Prepends example evaluations (few-shot) before the actual task prompt\n",
    "    - Provides LLM with context: \"Here is how similar papers were judged\"\n",
    "    - Ensures consistent reasoning across multiple runs\n",
    "\n",
    "    Args:\n",
    "        topic (str): The NOFO topic (priority research area)\n",
    "        base_prompt (str): The evaluation instructions prompt\n",
    "        examples (list of tuples): Few-shot examples [(title, reasoning), ...]\n",
    "\n",
    "    Returns:\n",
    "        str: Full prompt with examples + evaluation instructions\n",
    "    \"\"\"\n",
    "\n",
    "    # Format few-shot examples (each example = title + reasoning text)\n",
    "    examples_str = \"\\n\\n\".join(\n",
    "        [f\"Example ({title}):\\n{reasoning}\" for title, reasoning in examples]\n",
    "    )\n",
    "\n",
    "    # Assemble final prompt\n",
    "    prompt = f\"\"\"\n",
    "You are a research grant specialist evaluating research papers for relevance to NIH NOFO objectives: {topic}.\n",
    "\n",
    "Below are examples of prior evaluations for context:\n",
    "{examples_str}\n",
    "\n",
    "Now evaluate the following paper using the same structure and logic:\n",
    "\n",
    "{base_prompt}\n",
    "\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Few-shot setup ---\n",
    "# Retrieve few-shot examples (previous evaluations) and build prompt prefix\n",
    "few_shot_examples = get_few_shot_examples(LOG_PATH)\n",
    "prompt_with_examples = build_prompt_with_examples(topic, relevance_prompt_a, few_shot_examples)\n",
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import tiktoken\n",
    "from datetime import datetime\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import time  # NEW: for batch delay control\n",
    "import numpy as np  # NEW: for percentile calculation\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CONFIGURATION SECTION\n",
    "# ------------------------------------------------------------\n",
    "TEST_MODE = True                  # If True, process only subset of data (not used in Chroma flow)\n",
    "DISCREPANCY_THRESHOLD = 20        # Threshold for flagging model vs rule confidence mismatch\n",
    "\n",
    "# Toggle modes\n",
    "FAST_MODE = True                  # True = embedding-prioritized retrieval (top-K); False = evaluate all papers\n",
    "TOP_K_PAPERS = 50                 # Target number of papers to evaluate after pre-filtering\n",
    "\n",
    "# NEW CONFIG: Overfetch factor for Phase 1\n",
    "# ------------------------------------------------------------\n",
    "# Retrieve MORE chunks than TOP_K_PAPERS to ensure enough unique papers after aggregation.\n",
    "# Example: 50 papers * factor 3 = 150 chunks\n",
    "CHUNK_OVERFETCH_FACTOR = 3\n",
    "\n",
    "# NEW CONFIG: Chunk cap and long-paper handling\n",
    "CHUNK_CAP_PER_PAPER = 10          # Cap top-N chunks per paper (by similarity) to reduce bias & token load\n",
    "LONG_PAPER_THRESHOLD = 30         # If a paper has >30 chunks, summarize chunks before aggregation\n",
    "TOKEN_LIMIT_BEFORE_SUMMARY = 100000  # If estimated tokens exceed this, auto-summarize\n",
    "\n",
    "# Batch processing settings\n",
    "BATCH_SIZE = 10                   # Process papers in groups of 10\n",
    "BATCH_DELAY = 3                   # Delay between batches (seconds) to avoid rate-limit errors\n",
    "\n",
    "# Token cost estimation (OpenAI GPT-4o-mini pricing as of Aug 2025)\n",
    "INPUT_COST_PER_1K = 0.00015       # $ per 1K input tokens\n",
    "OUTPUT_COST_PER_1K = 0.0006       # $ per 1K output tokens\n",
    "\n",
    "# Initialize cost tracking variables\n",
    "total_input_tokens = 0            # Cumulative input tokens sent to API\n",
    "total_output_tokens = 0           # Approximate output tokens (assume 10% of input)\n",
    "total_cost_usd = 0.0              # Running cost estimate\n",
    "\n",
    "prior_classification = {\n",
    "    \"relevant\": [],\n",
    "    \"irrelevant\": [],\n",
    "    \"unknown\": []\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# LOGGING FUNCTION (unchanged)\n",
    "# ------------------------------------------------------------\n",
    "def log_prompt_iteration(\n",
    "    json_path,\n",
    "    prompt,\n",
    "    relevant_docs_with_reasoning,\n",
    "    irrelevant_docs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Appends classification results to master JSON log for auditability and trend tracking.\n",
    "    \"\"\"\n",
    "    iteration_id = len(json.load(open(json_path))) + 1 if os.path.exists(json_path) else 1\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    entry = {\n",
    "        \"iteration_id\": iteration_id,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"prompt\": prompt,\n",
    "        \"relevant_documents\": relevant_docs_with_reasoning,\n",
    "        \"irrelevant_documents\": irrelevant_docs\n",
    "    }\n",
    "\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                data = []\n",
    "    else:\n",
    "        data = []\n",
    "\n",
    "    data.append(entry)\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Logged iteration {iteration_id} to {json_path}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SELF-CHECK FUNCTION (unchanged)\n",
    "# ------------------------------------------------------------\n",
    "def verify_decision(llm, reasoning_output):\n",
    "    \"\"\"\n",
    "    Performs secondary pass: verifies relevance decision from reasoning text.\n",
    "    Only returns YES/NO (binary) to catch contradictory outputs.\n",
    "    \"\"\"\n",
    "    verification_prompt = f\"\"\"\n",
    "You are verifying the relevance decision based on the following evaluation:\n",
    "\n",
    "{reasoning_output}\n",
    "\n",
    "Only answer with 'YES' if the decision should be considered relevant, or 'NO' if not relevant.\n",
    "    \"\"\"\n",
    "    verification_response = llm.invoke(verification_prompt)\n",
    "    return \"YES\" in verification_response.content.upper()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# RULE-DERIVED CONFIDENCE FUNCTION (unchanged)\n",
    "# ------------------------------------------------------------\n",
    "def calculate_rule_confidence(criteria_results):\n",
    "    \"\"\"\n",
    "    Calculates deterministic confidence (0-95%) based on count of YES answers\n",
    "    in criteria evaluations (domain, methods, theory, application).\n",
    "    \"\"\"\n",
    "    yes_count = sum(1 for v in criteria_results.values() if v.upper().startswith(\"YES\"))\n",
    "    if yes_count == 0:\n",
    "        return 0\n",
    "    elif yes_count == 1:\n",
    "        return 50\n",
    "    elif yes_count == 2:\n",
    "        return 70\n",
    "    elif yes_count == 3:\n",
    "        return 85\n",
    "    else:\n",
    "        return 95\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CHROMADB INTEGRATION\n",
    "# ------------------------------------------------------------\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Load same embedding model used for precomputing\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "persist_dir = \"./chroma_embeddings\"\n",
    "\n",
    "# Connect to persisted Chroma vectorstore (contains chunks + metadata)\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"research_chunks\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=persist_dir\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 1: PAPER-LEVEL PRE-FILTER (OVERFETCH + RANKING)\n",
    "# ============================================================\n",
    "\n",
    "print(f\"[PHASE 1] Overfetching chunks for paper-level scoring (factor={CHUNK_OVERFETCH_FACTOR})\")\n",
    "\n",
    "# Step 1: Retrieve top-N chunks by similarity (overfetch)\n",
    "overfetch_k = TOP_K_PAPERS * CHUNK_OVERFETCH_FACTOR\n",
    "retrieved_chunks = vectorstore.similarity_search_with_score(topic, k=overfetch_k)  # returns (doc, score)\n",
    "\n",
    "# Step 2: Aggregate scores per paper_id\n",
    "paper_scores = {}         # {paper_id: [scores]}\n",
    "for doc, score in retrieved_chunks:\n",
    "    pid = doc.metadata.get(\"paper_id\", \"Unknown_Paper\")\n",
    "    if pid not in paper_scores:\n",
    "        paper_scores[pid] = []\n",
    "    paper_scores[pid].append(score)\n",
    "\n",
    "# Step 3: Rank papers by **max score** (primary) and print **average** for debugging\n",
    "ranked_papers = sorted(\n",
    "    paper_scores.items(),\n",
    "    key=lambda x: max(x[1]),  # still using max for ranking\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# Step 4: Select top-K unique papers\n",
    "top_paper_ids = [pid for pid, _ in ranked_papers[:TOP_K_PAPERS]]\n",
    "\n",
    "# Debug: Print ranking summary\n",
    "print(f\"Selected top {len(top_paper_ids)} papers for Phase 2 evaluation:\")\n",
    "for pid in top_paper_ids:\n",
    "    print(f\" - {pid}: max={max(paper_scores[pid]):.2f}, avg={sum(paper_scores[pid])/len(paper_scores[pid]):.2f}, chunks={len(paper_scores[pid])}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# NEW: Percentile Statistics Printout for Debugging\n",
    "# ------------------------------------------------------------\n",
    "scores_max = [max(scores) for scores in paper_scores.values()]\n",
    "if scores_max:\n",
    "    percentiles = [25, 50, 75, 90]\n",
    "    print(\"\\n[PHASE 1] Score Percentile Summary (max chunk scores per paper):\")\n",
    "    for p in percentiles:\n",
    "        val = np.percentile(scores_max, p)\n",
    "        print(f\"  {p}th percentile: {val:.2f}\")\n",
    "    print(f\"  Min score: {min(scores_max):.2f}\")\n",
    "    print(f\"  Max score: {max(scores_max):.2f}\")\n",
    "else:\n",
    "    print(\"[PHASE 1] No scores available to compute percentiles.\")\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 2: FULL-CHUNK RETRIEVAL WITH CAP + EARLY SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print(\"[PHASE 2] Retrieving all chunks (bulk) and filtering for top papers...\")\n",
    "\n",
    "# Retrieve all chunks at once (faster than per-paper search)\n",
    "all_chunks = vectorstore.similarity_search_with_score(topic, k=9999)\n",
    "\n",
    "# Organize chunks by paper_id and sort by score (so we can cap top-N)\n",
    "paper_chunk_data = {}  # {paper_id: [(score, text), ...]}\n",
    "\n",
    "for doc, score in all_chunks:\n",
    "    pid = doc.metadata.get(\"paper_id\", \"Unknown_Paper\")\n",
    "    if pid in top_paper_ids:\n",
    "        paper_chunk_data.setdefault(pid, []).append((score, doc.page_content))\n",
    "\n",
    "# Build aggregated papers with **chunk cap** and **early summary for long papers**\n",
    "paper_chunks = {}\n",
    "paper_chunk_counts = {}\n",
    "\n",
    "for pid, chunks in paper_chunk_data.items():\n",
    "    # Sort chunks by descending similarity score (keep top most relevant)\n",
    "    sorted_chunks = sorted(chunks, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Cap to top-N chunks (prevents extremely long papers from dominating)\n",
    "    capped_chunks = sorted_chunks[:CHUNK_CAP_PER_PAPER]\n",
    "\n",
    "    # Count total chunks for debug\n",
    "    paper_chunk_counts[pid] = len(sorted_chunks)\n",
    "\n",
    "    # If paper has > LONG_PAPER_THRESHOLD chunks → summarize early\n",
    "    if len(sorted_chunks) > LONG_PAPER_THRESHOLD:\n",
    "        print(f\"[EARLY SUMMARY] Paper '{pid}' exceeds {LONG_PAPER_THRESHOLD} chunks → summarizing chunks before aggregation.\")\n",
    "        # Summarize each chunk first, then combine summaries\n",
    "        chunk_summaries = []\n",
    "        for _, chunk_text in capped_chunks:\n",
    "            summary = summarize_text(chunk_text)  # reuse summarization helper\n",
    "            chunk_summaries.append(summary)\n",
    "        combined_summary = \"\\n\".join(chunk_summaries)\n",
    "        paper_chunks[pid] = combined_summary\n",
    "    else:\n",
    "        # Normal case: concatenate capped chunks directly\n",
    "        combined_text = \"\\n\".join([text for _, text in capped_chunks])\n",
    "        paper_chunks[pid] = combined_text\n",
    "\n",
    "# Final aggregated papers for downstream evaluation\n",
    "aggregated_papers = list(paper_chunks.items())\n",
    "print(f\"[PHASE 2] Aggregated {len(aggregated_papers)} papers for full relevance evaluation.\")\n",
    "\n",
    "# Debug print: show capped chunk count (original vs capped)\n",
    "print(\"[PHASE 2] Chunk count per selected paper (post-aggregation with cap):\")\n",
    "for pid, total_chunks in paper_chunk_counts.items():\n",
    "    print(f\" - {pid}: {min(total_chunks, CHUNK_CAP_PER_PAPER)} chunks used (original {total_chunks})\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# NEW: Automatic Warning for Overrepresented Papers\n",
    "# ------------------------------------------------------------\n",
    "# This block calculates the percentage of total chunks contributed by each paper\n",
    "# (AFTER capping) and issues a warning if any paper contributes more than X% of total chunks.\n",
    "# This helps detect potential bias where one very long paper dominates context.\n",
    "\n",
    "WARNING_THRESHOLD_PERCENT = 25  # e.g., warn if >25% of chunks come from a single paper\n",
    "\n",
    "# Total chunks after capping (sum of min(total, cap))\n",
    "total_chunks_used = sum(min(count, CHUNK_CAP_PER_PAPER) for count in paper_chunk_counts.values())\n",
    "\n",
    "for pid, count in paper_chunk_counts.items():\n",
    "    used_chunks = min(count, CHUNK_CAP_PER_PAPER)\n",
    "    percent = (used_chunks / total_chunks_used) * 100 if total_chunks_used > 0 else 0\n",
    "    if percent > WARNING_THRESHOLD_PERCENT:\n",
    "        print(f\"*** WARNING: Paper '{pid}' contributes {percent:.1f}% of total chunks \"\n",
    "              f\"({used_chunks}/{total_chunks_used}) → may indicate overrepresentation. ***\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TOKENIZER SETUP\n",
    "# ------------------------------------------------------------\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "MAX_TOKENS = 300000  # Safety ceiling for prompt+context\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SUMMARIZATION HELPER\n",
    "# ------------------------------------------------------------\n",
    "def summarize_text(paper_text):\n",
    "    \"\"\"\n",
    "    Summarizes full paper to ~300 words focusing on digital mental health interventions.\n",
    "    Reduces token load while preserving conceptual content for classification.\n",
    "    \"\"\"\n",
    "    summary_prompt = f\"\"\"\n",
    "    Summarize the following research paper into ~300 words, focusing on\n",
    "    digital mental health interventions, methods, and outcomes:\n",
    "\n",
    "    {paper_text}\n",
    "    \"\"\"\n",
    "    summary_response = llm.invoke(summary_prompt)\n",
    "    return summary_response.content.strip()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# MAIN LOOP: BATCH PROCESSING + COST TRACKING\n",
    "# ------------------------------------------------------------\n",
    "documents = []\n",
    "irrelevant_docs_list = []\n",
    "progress_cnt = 1\n",
    "relevant_papers_count = 0\n",
    "irrelevant_papers_count = 0\n",
    "total_files = len(aggregated_papers)\n",
    "\n",
    "for batch_start in range(0, total_files, BATCH_SIZE):\n",
    "    batch = aggregated_papers[batch_start: batch_start + BATCH_SIZE]\n",
    "    print(f\"\\nProcessing batch {batch_start//BATCH_SIZE + 1} \"\n",
    "          f\"({len(batch)} papers) out of {total_files} total papers...\")\n",
    "\n",
    "    for paper_id, paper_text in batch:\n",
    "        try:\n",
    "            # --- Dynamic Token Budget Check ---\n",
    "            # Estimate token count BEFORE building full prompt\n",
    "            token_estimate = len(encoding.encode(paper_text))\n",
    "            if token_estimate > TOKEN_LIMIT_BEFORE_SUMMARY:\n",
    "                print(f\"[TOKEN GUARD] Paper '{paper_id}' estimated {token_estimate} tokens → auto-summarizing.\")\n",
    "                paper_text = summarize_text(paper_text)  # Summarize entire paper\n",
    "\n",
    "            # --- Summarize paper (fallback for smaller papers) ---\n",
    "            summarized_text = summarize_text(paper_text)\n",
    "\n",
    "            # --- Build relevance prompt ---\n",
    "            available_tokens = MAX_TOKENS - len(encoding.encode(prompt_with_examples))\n",
    "            truncated_text = encoding.decode(encoding.encode(summarized_text)[:available_tokens])\n",
    "            full_prompt = prompt_with_examples + truncated_text\n",
    "\n",
    "            # --- Token count + cost estimation ---\n",
    "            token_count = len(encoding.encode(full_prompt))\n",
    "            total_input_tokens += token_count\n",
    "            total_output_tokens += int(token_count * 0.1)  # estimate 10% output\n",
    "            total_cost_usd = (\n",
    "                (total_input_tokens / 1000) * INPUT_COST_PER_1K +\n",
    "                (total_output_tokens / 1000) * OUTPUT_COST_PER_1K\n",
    "            )\n",
    "            print(f\"[Token Count] {paper_id}: {token_count} tokens \"\n",
    "                  f\"(Estimated running cost: ${total_cost_usd:.4f})\")\n",
    "\n",
    "            # --- LLM relevance classification ---\n",
    "            response = llm.invoke(full_prompt)\n",
    "            print(f\"Successfully processed paper {progress_cnt}/{total_files} ({paper_id})\")\n",
    "            progress_cnt += 1\n",
    "\n",
    "            # --- Self-check verification ---\n",
    "            is_relevant = verify_decision(llm, response.content)\n",
    "\n",
    "            if not is_relevant or \"PAPER NOT RELATED TO TOPIC\" in response.content:\n",
    "                irrelevant_papers_count += 1\n",
    "                irrelevant_docs_list.append(paper_id)\n",
    "                continue\n",
    "\n",
    "            # --- Parse LLM JSON output ---\n",
    "            try:\n",
    "                parsed_json = json.loads(response.content)\n",
    "            except json.JSONDecodeError:\n",
    "                json_match = re.search(r\"\\{.*\\}\", response.content, re.DOTALL)\n",
    "                parsed_json = json.loads(json_match.group(0)) if json_match else {}\n",
    "\n",
    "            # --- Confidence scoring (model + rule) ---\n",
    "            model_confidence = int(parsed_json.get(\"confidence\", 0)) if parsed_json else None\n",
    "            rule_confidence = calculate_rule_confidence(parsed_json[\"criteria_results\"]) \\\n",
    "                if \"criteria_results\" in parsed_json else 0\n",
    "            discrepancy = abs(model_confidence - rule_confidence) if model_confidence else None\n",
    "            flagged = discrepancy > DISCREPANCY_THRESHOLD if discrepancy is not None else False\n",
    "\n",
    "            # --- Store result ---\n",
    "            documents.append({\n",
    "                'title': paper_id,\n",
    "                'file_path': \"(from ChromaDB)\",\n",
    "                'llm_reasoning': response.content,\n",
    "                'model_confidence': model_confidence,\n",
    "                'rule_confidence': rule_confidence,\n",
    "                'confidence_discrepancy': discrepancy,\n",
    "                'flagged_for_review': flagged\n",
    "            })\n",
    "            relevant_papers_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!!! Error processing {paper_id}: {str(e)}\")\n",
    "\n",
    "    # --- Delay between batches ---\n",
    "    print(f\"Batch {batch_start//BATCH_SIZE + 1} complete. Sleeping {BATCH_DELAY} seconds...\")\n",
    "    time.sleep(BATCH_DELAY)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SUMMARY OUTPUT\n",
    "# ------------------------------------------------------------\n",
    "print(\"=\" * 50)\n",
    "print(f\"Relevant Papers: {relevant_papers_count}/{total_files}\")\n",
    "print(f\"Irrelevant Papers: {irrelevant_papers_count}/{total_files}\")\n",
    "print(f\"Estimated Total Input Tokens: {total_input_tokens}\")\n",
    "print(f\"Estimated Total Output Tokens: {total_output_tokens}\")\n",
    "print(f\"Estimated Total Cost: ${total_cost_usd:.4f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nList of relevant papers:\")\n",
    "for doc in documents:\n",
    "    print(f\"\\nTitle: {doc['title']}\")\n",
    "    print(f\"Model Confidence: {doc['model_confidence']}\")\n",
    "    print(f\"Rule Confidence: {doc['rule_confidence']}\")\n",
    "    print(f\"Discrepancy: {doc['confidence_discrepancy']} (Flagged: {doc['flagged_for_review']})\")\n",
    "    print(f\"Reasoning (truncated): {doc['llm_reasoning'][:500]}...\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# LOGGING\n",
    "# ------------------------------------------------------------\n",
    "relevant_docs_with_reasoning = [\n",
    "    {\n",
    "        \"title\": doc['title'],\n",
    "        \"reasoning\": doc['llm_reasoning'],\n",
    "        \"model_confidence\": doc['model_confidence'],\n",
    "        \"rule_confidence\": doc['rule_confidence'],\n",
    "        \"confidence_discrepancy\": doc['confidence_discrepancy'],\n",
    "        \"flagged_for_review\": doc['flagged_for_review']\n",
    "    }\n",
    "    for doc in documents\n",
    "]\n",
    "\n",
    "log_prompt_iteration(\n",
    "    json_path=\"prompt_evaluation_log_cleaned.json\",\n",
    "    prompt=prompt_with_examples,\n",
    "    relevant_docs_with_reasoning=relevant_docs_with_reasoning,\n",
    "    irrelevant_docs=irrelevant_docs_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNdBg6Iei7VJ"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Step 3: Proposal Ideation Based on Filtered Research - [4 marks]**\n",
    "> **Use the filtered papers, to generate ideas for the Reseach Proposal.**\n",
    "---\n",
    "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt, 1 mark for the Generating Idea and 1 mark for fetching file path of chosen idea along with successful completion of this section, including debugging or modifying the code if necessary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN5c3WhIEpzL"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "**TASK:** Write an Prompt which can be used to generate 5 ideas for the Research Proposal, each idea should consist:\n",
    "\n",
    "1. **Idea X:** [Concise Title of the Project Idea]  \\n\n",
    "2. **Description:** [Brief and targeted description summarizing the objectives, innovative elements, scientific rationale, and anticipated impact.]  \\n\n",
    "3. **Citation:** [Author(s), Year or Paper Title]  \\n\n",
    "4. **NOFO Alignment:** [List two or more specific NOFO requirements that this idea directly addresses]  \\n\n",
    "5. **File Path of the Research Paper:** [Exact file path, ending in .pdf]\n",
    "\n",
    "- Use the Delimiter `---` for defining the structure of the sample outputs in the prompt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLgOVonjveNM"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "#### Generating 5 Ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NoHQ6HH1kiD4"
   },
   "outputs": [],
   "source": [
    "# Note to self: Be sure to add additional details from page linked in the NOFO pdf\n",
    "# Also need to include constraints, e.g., \"Digital health test beds that leverage well-established \n",
    "# digital health platforms to optimize evidence-based digital mental health interventions\"\n",
    "\n",
    "gen_idea_prompt = f\"\"\"\n",
    "\n",
    "\n",
    "<WRITE YOUR PROMPT HERE>\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-oCJeXkcBKd"
   },
   "outputs": [],
   "source": [
    "ideas = llm.invoke(gen_idea_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952
    },
    "id": "TQpW_7cKco8Q",
    "outputId": "6b59af58-d64c-4804-f5a6-bfe89bb023dd"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(ideas.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For consideration if extracted text is not clean enough\n",
    "# Add post-extraction GPT-enabled noise removal step\n",
    "# to remove additional noise from chunks\n",
    "\n",
    "# Too resource intensive for full data set. Add later if needed.\n",
    "\n",
    "# import json\n",
    "# from openai import OpenAI\n",
    "\n",
    "# # Initialize OpenAI client\n",
    "# client = OpenAI()\n",
    "\n",
    "# def semantic_clean_text(raw_text):\n",
    "#     prompt = f\"\"\"\n",
    "# You are a document cleaner. Extract ONLY the main body text from the following academic or technical document:\n",
    "# - Remove page numbers, headers/footers\n",
    "# - Remove title page, author affiliations, figure/table captions\n",
    "# - Remove references/bibliography sections\n",
    "# - Keep abstracts, introductions, main sections, and conclusions\n",
    "\n",
    "# Document:\n",
    "# \\\"\\\"\\\"{raw_text}\\\"\\\"\\\"\n",
    "\n",
    "# Return only the cleaned text.\n",
    "# \"\"\"\n",
    "#     response = client.responses.create(\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         input=prompt,\n",
    "#         max_output_tokens=4000\n",
    "#     )\n",
    "#     return response.output_text\n",
    "\n",
    "# # --- Ingest cleaned + chunked data and post-process with GPT ---\n",
    "# input_path = \"data/cleaned_chunked_papers.json\"\n",
    "# output_path = \"data/cleaned_gpt.json\"\n",
    "\n",
    "# # Load chunked data\n",
    "# with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     chunked_data = json.load(f)\n",
    "\n",
    "# # Prepare list for GPT-processed results\n",
    "# gpt_cleaned_data = []\n",
    "\n",
    "# # Loop through each document\n",
    "# for record in chunked_data:\n",
    "#     doc_id = record[\"id\"]\n",
    "#     gpt_chunks = []\n",
    "\n",
    "#     print(f\"Post-processing (GPT cleanup): {doc_id}\")\n",
    "\n",
    "#     # Apply GPT cleaning to each chunk\n",
    "#     for chunk in record[\"chunks\"]:\n",
    "#         cleaned_chunk = semantic_clean_text(chunk)\n",
    "#         gpt_chunks.append(cleaned_chunk)\n",
    "\n",
    "#     # Store result\n",
    "#     gpt_cleaned_data.append({\n",
    "#         \"id\": doc_id,\n",
    "#         \"chunks\": gpt_chunks\n",
    "#     })\n",
    "\n",
    "# # Save GPT-cleaned data\n",
    "# with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(gpt_cleaned_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# print(f\"Saved GPT post-processed chunks to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLD4_7trvhKL"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "#### Choosing 1 Idea and fetching details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3ulgD6_dkrJ"
   },
   "outputs": [],
   "source": [
    "# Modify the idea_number for choosing the different idea\n",
    "idea_number = 5   # change the number if you wish to choose and generate the research proposal for another idea\n",
    "chosen_idea = ideas.content.split(\"---\")[idea_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DcV8QY1irIyH",
    "outputId": "d71e7380-e5d3-49d8-cd93-3dceedd57cf8"
   },
   "outputs": [],
   "source": [
    "# Import required libraries for core functionality\n",
    "import re\n",
    "\n",
    "# Use a regular expression to find the file path of the research paper\n",
    "\n",
    "pattern = r\"File Path of the Research Paper:\\*\\*\\s*(.+?)\\n\"\n",
    "# If you are unable to extract the file path successfully using this pattern, use the `ChatGPT` or any other LLM to find the pattern that works for you, simply provide the LLM the sample response of your whole ideas and ask the LLM to generate the regex patterm for extracting the \"File Path of the Research Paper\"\n",
    "\n",
    "match = re.search(pattern, chosen_idea)\n",
    "\n",
    "if match:\n",
    "  idea_generated_from_research_paper = match.group(1).strip()\n",
    "  print(\"Filepath : \", idea_generated_from_research_paper)\n",
    "else:\n",
    "  print(\"File Path of the Research Paper not found in the chosen idea.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51quLkMgi7S5"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Step 4: Proposal Blueprint Preparation - [3 Marks]**\n",
    "\n",
    "> **Select appropriate research ideas for the proposal and supply 'Sample Research Proposals' as templates to the LLM to support the generation of the final proposal.**\n",
    "---   \n",
    "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt and 1 mark for the successful completion of this section, including debugging or modifying the code if necessary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJhO1BFHC7cE"
   },
   "source": [
    "**TASK:** Write an Prompt which can be used to generate the Research Proposal.\n",
    "\n",
    "The prompt should be able to craft a research proposal based on the sample research proposal template, using one of the ideas generated above. The proposal should include references to the actual research papers from which the ideas are derived and should align well with the NOFO documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMOe-9_AgKvN"
   },
   "outputs": [],
   "source": [
    "# Here we need to add the full papers instead of the summary\n",
    "# Load PDF files and extract content using PyPDFLoader\n",
    "chosen_idea_rp = PyPDFLoader(idea_generated_from_research_paper, mode=\"single\").load()\n",
    "\n",
    "# Loading the sample research proposal template\n",
    "# Load PDF files and extract content using PyPDFLoader\n",
    "research_proposal_template = PyPDFLoader(\" <Path of Research Proposal Template> \", mode=\"single\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e6c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pypdf import PdfReader\n",
    "import camelot\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import tiktoken\n",
    "\n",
    "# --- Tokenization setup ---\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "MAX_TOKENS = 127500          # total model context window\n",
    "EXTRACTION_BUDGET = 100000   # reserve ~20% for prompts/response\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Count tokens using tiktoken encoding.\"\"\"\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# --- Load matching papers from JSON log ---\n",
    "def load_matched_papers(json_path, pdf_folder=\"content\"):\n",
    "    \"\"\"\n",
    "    Extract list of relevant document file paths from the latest JSON iteration.\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Take the last iteration's relevant_documents\n",
    "    last_iteration = data[-1]\n",
    "    relevant_docs = last_iteration.get(\"relevant_documents\", [])\n",
    "    \n",
    "    # Build file paths for each relevant doc (assumes they exist in pdf_folder)\n",
    "    file_paths = []\n",
    "    for doc in relevant_docs:\n",
    "        title = doc[\"title\"]\n",
    "        pdf_path = os.path.join(pdf_folder, title)\n",
    "        if os.path.exists(pdf_path):\n",
    "            file_paths.append(pdf_path)\n",
    "        else:\n",
    "            print(f\"Warning: {pdf_path} not found. Skipping.\")\n",
    "    return file_paths\n",
    "\n",
    "# --- Stage 1 & 2: Text + Table extraction ---\n",
    "def extract_text_and_tables(file_path, token_budget):\n",
    "    \"\"\"Extract text and tables within token budget.\"\"\"\n",
    "    content = \"\"\n",
    "    token_count = 0\n",
    "\n",
    "    # Stage 1: PyPDF text extraction\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text() or \"\"\n",
    "            token_count += count_tokens(page_text)\n",
    "            if token_count > token_budget:\n",
    "                print(f\"Token budget reached during text extraction: {file_path}\")\n",
    "                break\n",
    "            content += page_text\n",
    "    except Exception as e:\n",
    "        print(f\"PyPDF extraction failed: {e}\")\n",
    "\n",
    "    # Stage 2: Table extraction (Camelot)\n",
    "    # try:\n",
    "    #     tables = camelot.read_pdf(file_path, pages='all')\n",
    "    #     for table in tables:\n",
    "    #         table_text = \"\\n[Table Extracted]\\n\" + table.df.to_string()\n",
    "    #         token_count += count_tokens(table_text)\n",
    "    #         if token_count > token_budget:\n",
    "    #             print(f\"Token budget reached during table extraction: {file_path}\")\n",
    "    #             break\n",
    "    #         content += table_text\n",
    "    # except Exception:\n",
    "    #     pass\n",
    "\n",
    "    return content, token_count\n",
    "\n",
    "# --- Stage 3: OCR extraction ---\n",
    "# def extract_ocr(file_path, token_budget, current_tokens=0):\n",
    "#     \"\"\"Extract OCR text (figures/scanned pages) within remaining token budget.\"\"\"\n",
    "#     content = \"\"\n",
    "#     token_count = current_tokens\n",
    "\n",
    "#     try:\n",
    "#         images = convert_from_path(file_path)\n",
    "#         for image in images:\n",
    "#             ocr_text = pytesseract.image_to_string(image)\n",
    "#             token_count += count_tokens(ocr_text)\n",
    "#             if token_count > token_budget:\n",
    "#                 print(f\"Token budget reached during OCR extraction: {file_path}\")\n",
    "#                 break\n",
    "#             content += \"\\n[OCR Extracted]\\n\" + ocr_text\n",
    "#     except Exception:\n",
    "#         pass\n",
    "\n",
    "    return content\n",
    "\n",
    "# --- Process all matched papers ---\n",
    "def process_matched_papers(json_path, pdf_folder=\"content\"):\n",
    "    \"\"\"\n",
    "    Load matched papers from JSON and process them using multi-stage extraction:\n",
    "    Pass 1: Text + Tables\n",
    "    Pass 2: OCR (Figures)\n",
    "    Returns dict mapping filename -> combined extracted content.\n",
    "    \"\"\"\n",
    "    matched_files = load_matched_papers(json_path, pdf_folder)\n",
    "    text_table_data = {}\n",
    "    token_usage = {}\n",
    "\n",
    "    for file_path in matched_files:\n",
    "        print(f\"Extracting text/tables: {os.path.basename(file_path)}\")\n",
    "        content, tokens_used = extract_text_and_tables(file_path, EXTRACTION_BUDGET)\n",
    "        text_table_data[os.path.basename(file_path)] = content\n",
    "        token_usage[os.path.basename(file_path)] = tokens_used\n",
    "\n",
    "    # Return text_table_data directly\n",
    "    return text_table_data\n",
    "\n",
    "    # Pass 2: Extract OCR for all files (if budget allows)\n",
    "    # for file_path in matched_files:\n",
    "    #     filename = os.path.basename(file_path)\n",
    "    #     remaining_budget = EXTRACTION_BUDGET - token_usage.get(filename, 0)\n",
    "    #     if remaining_budget > 0:\n",
    "    #         print(f\"Extracting OCR: {filename} (remaining budget: {remaining_budget})\")\n",
    "    #         ocr_content = extract_ocr(file_path, EXTRACTION_BUDGET, token_usage[filename])\n",
    "    #         results[filename] = text_table_data[filename] + ocr_content\n",
    "    #     else:\n",
    "    #         print(f\"Skipping OCR for {filename} (no remaining token budget)\")\n",
    "    #         results[filename] = text_table_data[filename]\n",
    "\n",
    "# Example usage:\n",
    "# matched_content = process_matched_papers(\"/mnt/data/prompt_evaluation_log_cleaned.json\", pdf_folder=\"../content\")\n",
    "# print(matched_content.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_content = process_matched_papers(\"prompt_evaluation_log_cleaned.json\", pdf_folder=\"data/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matched_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OGcodZLo7VE"
   },
   "outputs": [],
   "source": [
    "research_proposal_template_prompt = f\"\"\"\n",
    "\n",
    "\n",
    "<WRITE YOUR PROMPT HERE>\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gXIZE0jkieg"
   },
   "outputs": [],
   "source": [
    "research_plan = llm.invoke(research_proposal_template_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1UFS-Vxlkib0",
    "outputId": "2761571a-d2a0-421c-b429-252cc78ddd41"
   },
   "outputs": [],
   "source": [
    "display(Markdown(research_plan.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkuiPncysdCa"
   },
   "outputs": [],
   "source": [
    "# @title **Optional Part - Creating a PDF of the Research Proposal**\n",
    "# The code in this cell block is used for printing out the output in the PDF format\n",
    "from markdown_pdf import MarkdownPdf, Section\n",
    "\n",
    "pdf = MarkdownPdf()\n",
    "pdf.add_section(Section(research_plan.content))\n",
    "pdf.save(\"Reseach Proposal First Draft.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77t_iYgni7QV"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Step 5: Proposal Evaluation Against NOFO Criteria - [3 Marks]**\n",
    "> **Use the LLM to evaluate the generated proposal (LLM-as-Judge) and assess its alignment with the NOFO criteria.**\n",
    "   \n",
    "\n",
    "---\n",
    "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt and 1 mark for the successful completion of this section, including debugging or modifying the code if necessary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXWK_mZewlim"
   },
   "source": [
    "**TASK:** Write an Prompt which can be used to evaluate the Research Proposal based on:\n",
    "1. **Innovation**\n",
    "2. **Significance**\n",
    "3. **Approach**\n",
    "4. **Investigator Expertise**\n",
    "\n",
    "- Ask the LLM to rate on each of the criteria from **1 (Poor)** to **5 (Excellent)**\n",
    "- Ask the LLM to provide the resonse in the json format\n",
    "```JSON\n",
    "name: Innovation\n",
    "    justification: \"<Justification>\"\n",
    "    score: <1-5>\n",
    "    strengths: \"<Strength 1>\"\n",
    "    weaknesses: \"<Weakness 1>\"\n",
    "    recommendations: \"<Recommendation 1>\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ax5H703ZhZ7y"
   },
   "outputs": [],
   "source": [
    "evaluation_prompt = f'''\n",
    "\n",
    "\n",
    "<WRITE YOUR PROMPT HERE>\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aZox8iNhZ5g"
   },
   "outputs": [],
   "source": [
    "# Call the LLM with the prepared prompt and truncated paper content\n",
    "eval_response = llm.invoke(evaluation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCx7_am-hZ3H"
   },
   "outputs": [],
   "source": [
    "# Import required libraries for core functionality\n",
    "import json\n",
    "json_resp = json.loads(eval_response.content[7:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vwAFtxolhZpD",
    "outputId": "a5d2b346-065a-428a-8c5d-a38cd57a90ae"
   },
   "outputs": [],
   "source": [
    "for key, value in json_resp.items():\n",
    "  print(f\"---\\n{key}:\")\n",
    "  if isinstance(value, list):\n",
    "    for item in value:\n",
    "      for k, v in item.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "      print(\"=\"*50)\n",
    "  elif isinstance(value, dict):\n",
    "    for k, v in value.items():\n",
    "      print(f\"  {k}: {v}\")\n",
    "  else:\n",
    "    print(f\"  {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fomQFyZAi7N4"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Step 6: Human Review and Refinement of Proposal**\n",
    "> **Perform Human Evaluation of the generated Proposal. Edit or Modify the proposal as necessary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HWwfHOXHmMOu",
    "outputId": "87320d16-f244-429e-e5cb-215e5eae01e7"
   },
   "outputs": [],
   "source": [
    "display(Markdown(research_plan.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRbWRHAJ_KXz"
   },
   "source": [
    "# **Step 7: Summary and Recommendation - [2 Marks]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jC6F4JezA840"
   },
   "source": [
    "Based on the projects, learners are expected to share their observations, key learnings, and insights related to this business use case, including the challenges they encountered.\n",
    "\n",
    "Additionally, they should recommend or explain any changes that could improve the project, along with suggesting additional steps that could be taken for further enhancement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8_WFIYIB12b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c868c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Enhanced PDF Processing (Commenting original PyPDF-only approach) ---\n",
    "# Original starter code (commented for traceability):\n",
    "# Load PDF files and extract content using PyPDFLoader\n",
    "# docs = PyPDFLoader(file_path, mode=\"single\").load()\n",
    "\n",
    "# New Implementation: Multi-stage parsing (PyPDF → Camelot/Tabula → OCR fallback)\n",
    "# Purpose: Capture text, tables, and figures from diverse PDF formats (Mermaid C node, Rubric Step 2).\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "# Import required libraries for core functionality\n",
    "import camelot\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def process_pdf_multistage(file_path):\n",
    "    \"\"\"\n",
    "    Multi-stage pipeline for extracting text, tables, and figures from PDFs.\n",
    "    Stages:\n",
    "    1. PyPDF (text)\n",
    "    2. Camelot/Tabula (tables)\n",
    "    3. OCR (scanned pages/figures)\n",
    "    \"\"\"\n",
    "    content = \"\"\n",
    "\n",
    "    # Stage 1: PyPDF text extraction\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        for page in reader.pages:\n",
    "            content += page.extract_text() or \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"PyPDF extraction failed: {e}\")\n",
    "\n",
    "    # Stage 2: Table extraction (Camelot)\n",
    "    try:\n",
    "        tables = camelot.read_pdf(file_path, pages='all')\n",
    "        for table in tables:\n",
    "            content += \"\\n[Table Extracted]\\n\" + table.df.to_string()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Stage 3: OCR fallback for scanned pages or figures\n",
    "    try:\n",
    "        images = convert_from_path(file_path)\n",
    "        for image in images:\n",
    "            text = pytesseract.image_to_string(image)\n",
    "            content += \"\\n[OCR Extracted]\\n\" + text\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6527505",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Hybrid Retrieval (BM25 + Embeddings) ---\n",
    "# Original code used either BM25 OR embeddings; this combines both (Mermaid D node, Rubric Step 2).\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def hybrid_retrieval_setup(docs_text):\n",
    "    \"\"\"\n",
    "    Creates BM25 and embedding indexes for hybrid search.\n",
    "    \"\"\"\n",
    "    # BM25 Index\n",
    "    tokenized_corpus = [doc.split(\" \") for doc in docs_text]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    # Embedding Index\n",
    "    embed_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectorstore = Chroma.from_texts(docs_text, embed_model)\n",
    "\n",
    "    return bm25, vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aabb353",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Agentic Components (Research Analyst, Proposal Writer, Compliance Checker) ---\n",
    "# Implements multi-agent workflow (Mermaid E subgraph, Rubric Step 3-4).\n",
    "\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "def analyze_papers(query):\n",
    "    return \"Synthesis of relevant papers\"\n",
    "\n",
    "def check_compliance(proposal):\n",
    "    return \"Compliance report\"\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"Research Analyst\", func=analyze_papers, description=\"Synthesizes relevant papers.\"),\n",
    "    Tool(name=\"Compliance Checker\", func=check_compliance, description=\"Ensures NOFO alignment.\")\n",
    "]\n",
    "\n",
    "# Initialize agent with zero-shot reasoning and tools\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a917953",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Agentic Components (Research Analyst, Proposal Writer, Compliance Checker) ---\n",
    "# Implements multi-agent workflow (Mermaid E subgraph, Rubric Step 3-4).\n",
    "\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "def analyze_papers(query):\n",
    "    return \"Synthesis of relevant papers\"\n",
    "\n",
    "def check_compliance(proposal):\n",
    "    return \"Compliance report\"\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"Research Analyst\", func=analyze_papers, description=\"Synthesizes relevant papers.\"),\n",
    "    Tool(name=\"Compliance Checker\", func=check_compliance, description=\"Ensures NOFO alignment.\")\n",
    "]\n",
    "\n",
    "# Initialize agent with zero-shot reasoning and tools\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5141a498",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Multi-Criteria Evaluation with Guardrails ---\n",
    "# Original evaluation only scored NIH criteria; now adds guardrail flags (Mermaid G node, Rubric Step 5).\n",
    "\n",
    "evaluation_prompt = f\"\"\"\n",
    "Evaluate the proposal on:\n",
    "1. Innovation\n",
    "2. Significance\n",
    "3. Approach\n",
    "4. Investigator Expertise\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"criteria\": [\n",
    "    {{\n",
    "      \"name\": \"Innovation\",\n",
    "      \"score\": 1-5,\n",
    "      \"strengths\": \"...\",\n",
    "      \"weaknesses\": \"...\",\n",
    "      \"recommendations\": \"...\"\n",
    "    }},\n",
    "    ...\n",
    "  ],\n",
    "  \"overall_score\": 1-5,\n",
    "  \"guardrail_flags\": [\"hallucination risk\", \"compliance gap\"]\n",
    "}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b907ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Caching Intermediate Steps ---\n",
    "# Saves embeddings, filtered papers, and draft proposals for reuse (Mermaid J node, Rubric Step 7).\n",
    "\n",
    "# Import required libraries for core functionality\n",
    "import pickle\n",
    "\n",
    "def save_checkpoint(data, name):\n",
    "    with open(f\"checkpoint_{name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_checkpoint(name):\n",
    "    try:\n",
    "        with open(f\"checkpoint_{name}.pkl\", \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7667a6",
   "metadata": {},
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "\n",
    "# Quick Reference: Few-Shot + Agentic Enhancements\n",
    "\n",
    "This section provides details about the few-shot pool, semantic versioning, and agentic conflict resolver integrated into this workflow.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "**Semantic Versioning**\n",
    "- Automatically increments version numbers (`v2-fewshot`, `v3-agentic`) based on features used.\n",
    "- Few-shot only → `-fewshot`\n",
    "- Few-shot + agentic resolver → `-agentic`\n",
    "\n",
    "**Few-Shot Pool**\n",
    "- Derived from cleaned log (`prompt_evaluation_log_cleaned.json`).\n",
    "- Filters examples with ≥80% hybrid confidence.\n",
    "- Balances relevant/irrelevant examples 50/50 and ensures diversity.\n",
    "\n",
    "**Agentic Conflict Resolver**\n",
    "- Activates when model vs. rule confidence differs by >20%.\n",
    "- Produces reconciled decision and rationale logged under `agentic_resolution`.\n",
    "\n",
    "**Enhanced Logging Fields**\n",
    "- `decision_source`: hybrid (model + rule)\n",
    "- `hybrid_confidence`: average of model and rule confidence\n",
    "- `agentic_resolution`: reconciliation result (if applicable)\n",
    "- `prompt_version`: auto-generated semantic version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da55b749",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# VERSION TRACKING + FEW-SHOT REBUILDER + AGENTIC RESOLVER\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Function: Determine the next semantic version string for the prompt\n",
    "def get_next_prompt_version(log_path, agentic_enabled=False):\n",
    "    \"\"\"\n",
    "    Determine next semantic version based on last logged version.\n",
    "    Increments number, adds suffix based on features used.\n",
    "    \"\"\"\n",
    "# Import required libraries for core functionality\n",
    "    import os, json, re\n",
    "    version_num = 1\n",
    "    if os.path.exists(log_path):\n",
    "        with open(log_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                data = []\n",
    "        # Extract last version number\n",
    "        for entry in reversed(data):\n",
    "            if \"prompt_version\" in entry:\n",
    "                match = re.match(r\"v(\\d+)\", entry[\"prompt_version\"])\n",
    "                if match:\n",
    "                    version_num = int(match.group(1)) + 1\n",
    "                break\n",
    "\n",
    "    suffix = \"-agentic\" if agentic_enabled else \"-fewshot\"\n",
    "    return f\"v{version_num}{suffix}\"\n",
    "\n",
    "\n",
    "# Function: Build balanced high-confidence few-shot example pool from the log\n",
    "def rebuild_few_shot_pool(cleaned_log_path, min_conf=80, max_examples=4):\n",
    "    \"\"\"\n",
    "    Build balanced high-confidence few-shot pool from cleaned log.\n",
    "    Balances relevant and irrelevant, ensures diversity.\n",
    "    \"\"\"\n",
    "# Import required libraries for core functionality\n",
    "    import json, random\n",
    "    with open(cleaned_log_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    relevant, irrelevant = [], []\n",
    "    for iteration in data:\n",
    "        for doc in iteration.get(\"relevant_documents\", []):\n",
    "            hybrid_conf = max(doc.get(\"model_confidence\", 0), doc.get(\"rule_confidence\", 0))\n",
    "            if hybrid_conf >= min_conf:\n",
    "                relevant.append((doc[\"title\"], doc[\"reasoning\"]))\n",
    "        for doc in iteration.get(\"irrelevant_documents\", []):\n",
    "            irrelevant.append((doc, \"PAPER NOT RELATED TO TOPIC\"))\n",
    "\n",
    "    # Shuffle and balance\n",
    "    half = max_examples // 2\n",
    "    random.shuffle(relevant)\n",
    "    random.shuffle(irrelevant)\n",
    "    return relevant[:half] + irrelevant[:half]\n",
    "\n",
    "\n",
    "# Function: Resolve discrepancies between model and rule confidences using agentic logic\n",
    "def agentic_conflict_resolver(doc_title, reasoning_json, model_conf, rule_conf):\n",
    "    \"\"\"\n",
    "    Agentic layer to reconcile conflicts:\n",
    "    - Triggered when discrepancy exceeds threshold\n",
    "    - Returns reconciled decision and rationale\n",
    "    \"\"\"\n",
    "    rationale = []\n",
    "    if abs(model_conf - rule_conf) > 20:\n",
    "        if rule_conf > model_conf:\n",
    "            final_decision = \"RELEVANT\" if rule_conf >= 50 else \"PAPER NOT RELATED TO TOPIC\"\n",
    "            rationale.append(\"Rule confidence higher; prioritizing deterministic criteria.\")\n",
    "        else:\n",
    "            final_decision = \"RELEVANT\" if model_conf >= 50 else \"PAPER NOT RELATED TO TOPIC\"\n",
    "            rationale.append(\"Model confidence higher; prioritizing LLM interpretation.\")\n",
    "    else:\n",
    "        final_decision = \"RELEVANT\" if (model_conf + rule_conf) / 2 >= 50 else \"PAPER NOT RELATED TO TOPIC\"\n",
    "        rationale.append(\"Confidences close; hybrid average used for decision.\")\n",
    "\n",
    "    return {\n",
    "        \"final_decision\": final_decision,\n",
    "        \"rationale\": \" \".join(rationale)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16955fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# ENHANCED LOGGING WITH SEMANTIC VERSIONING AND AGENTIC RESOLUTION\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Ensure this cell is run AFTER document processing and building relevant_docs_with_reasoning\n",
    "\n",
    "# Define constants for few-shot\n",
    "# Define configuration for few-shot example retrieval (number of examples)\n",
    "FEW_SHOT_MAX_EXAMPLES = 4\n",
    "# Minimum confidence threshold for including examples in few-shot prompting\n",
    "MIN_CONFIDENCE_FOR_FEWSHOT = 70\n",
    "# Path to the cleaned JSON log file where prompt evaluation iterations are stored\n",
    "LOG_PATH = \"prompt_evaluation_log_cleaned.json\"\n",
    "\n",
    "# Determine prompt version\n",
    "# Path to the cleaned JSON log file where prompt evaluation iterations are stored\n",
    "current_version = get_next_prompt_version(LOG_PATH, agentic_enabled=any(doc.get('flagged_for_review', False) for doc in relevant_docs_with_reasoning))\n",
    "\n",
    "# Add decision source and hybrid confidence\n",
    "for doc in relevant_docs_with_reasoning:\n",
    "    doc[\"decision_source\"] = \"hybrid\"\n",
    "    doc[\"hybrid_confidence\"] = (doc[\"model_confidence\"] + doc[\"rule_confidence\"]) / 2\n",
    "\n",
    "# Add agentic resolution for flagged docs\n",
    "for doc in relevant_docs_with_reasoning:\n",
    "    if doc.get(\"flagged_for_review\"):\n",
    "        resolution = agentic_conflict_resolver(\n",
    "            doc_title=doc[\"title\"],\n",
    "            reasoning_json=doc[\"reasoning\"],\n",
    "            model_conf=doc[\"model_confidence\"],\n",
    "            rule_conf=doc[\"rule_confidence\"]\n",
    "        )\n",
    "        doc[\"agentic_resolution\"] = resolution\n",
    "\n",
    "# Append prompt_version to log\n",
    "# Path to the cleaned JSON log file where prompt evaluation iterations are stored\n",
    "with open(LOG_PATH, \"r+\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    if data:\n",
    "        data[-1][\"prompt_version\"] = current_version\n",
    "    f.seek(0)\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    f.truncate()\n",
    "\n",
    "print(f\"Logged with prompt version: {current_version}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional enhancements proposed by Claude\n",
    "\n",
    "Your flowchart shows a well-structured approach to the RFP response generation system. Here are several improvements I'd recommend to enhance the robustness and effectiveness of your solution:\n",
    "\n",
    "1. Enhanced RFP Requirements Extraction\n",
    "After step B, add a sub-process for:\n",
    "\n",
    "Requirement Categorization: Classify requirements into mandatory vs. optional, technical vs. administrative\n",
    "Scoring Rubric Extraction: Specifically parse how proposals will be evaluated\n",
    "Budget Constraints Analysis: Extract funding limits and cost-effectiveness criteria\n",
    "Timeline Extraction: Identify key dates and milestone requirements\n",
    "\n",
    "2. Improved Paper Processing Pipeline\n",
    "Between steps C and D, consider adding:\n",
    "\n",
    "Citation Network Analysis: Map relationships between papers to identify influential work\n",
    "Method/Innovation Extraction: Specifically extract methodologies and novel approaches\n",
    "Results/Outcomes Extraction: Capture quantitative results and impact metrics\n",
    "Quality Assessment: Add a paper quality scoring mechanism (impact factor, recency, relevance)\n",
    "\n",
    "3. Enhanced Retrieval and Ranking\n",
    "Expand step D with:\n",
    "\n",
    "Multi-Query Generation: Generate multiple search queries from different RFP aspects\n",
    "Cross-Reference Validation: Verify that selected papers actually support proposed innovations\n",
    "Diversity Scoring: Ensure selected papers cover different aspects of the RFP\n",
    "Gap Analysis: Identify what the RFP asks for that isn't well-covered in existing research\n",
    "\n",
    "4. Strengthened Agentic Architecture\n",
    "Add these specialized agents to your existing three:\n",
    "\n",
    "Innovation Synthesizer Agent: Combines findings from multiple papers into novel approaches\n",
    "Budget Estimator Agent: Ensures proposals are financially realistic\n",
    "Risk Assessment Agent: Identifies potential implementation challenges\n",
    "Competitive Analysis Agent: Positions your proposal against likely competitors\n",
    "\n",
    "5. Improved Evaluation and Refinement\n",
    "Enhance the evaluation loop (G-I) with:\n",
    "\n",
    "Specific Weakness Detection: Not just overall score, but identify specific weak sections\n",
    "Competitive Benchmarking: Compare against successful past proposals if available\n",
    "Consistency Checking: Ensure all sections align and support each other\n",
    "Technical Feasibility Validation: Verify proposed solutions are implementable\n",
    "\n",
    "6. Additional Process Improvements\n",
    "Consider these architectural enhancements:\n",
    "flowchart LR\n",
    "    subgraph \"Knowledge Management\"\n",
    "        KB1[Domain Ontology]\n",
    "        KB2[Success Patterns DB]\n",
    "        KB3[Common Pitfalls DB]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Feedback Loops\"\n",
    "        FL1[Real-time Agent Collaboration]\n",
    "        FL2[Iterative Improvement Tracking]\n",
    "        FL3[Version Control System]\n",
    "    end\n",
    "7. Quality Assurance Additions\n",
    "\n",
    "Plagiarism Detection: Ensure generated content is original\n",
    "Fact Verification: Cross-check claims against source papers\n",
    "Readability Analysis: Ensure proposal meets target audience expectations\n",
    "Compliance Validation: Automated checks against all RFP requirements\n",
    "\n",
    "8. Output Enhancement\n",
    "For the final deliverables (step N), consider generating:\n",
    "\n",
    "Executive Summary: One-page overview for quick review\n",
    "Technical Appendix: Detailed methodology descriptions\n",
    "Budget Justification: Line-by-line cost explanations\n",
    "Risk Mitigation Plan: Addressing identified challenges\n",
    "Evaluation Metrics: How success will be measured\n",
    "\n",
    "9. Monitoring and Logging\n",
    "Add throughout the pipeline:\n",
    "\n",
    "Decision Logging: Track why papers were selected/rejected\n",
    "Agent Reasoning Traces: Understand how proposals were generated\n",
    "Performance Metrics: Time taken, resources used, quality scores\n",
    "Error Handling: Graceful degradation if components fail\n",
    "\n",
    "10. Advanced Features\n",
    "Consider these stretch goals:\n",
    "\n",
    "Multi-RFP Learning: Learn from multiple RFPs to improve over time\n",
    "Collaborative Filtering: If multiple users, learn from collective behavior\n",
    "Adaptive Prompting: Adjust prompts based on intermediate results\n",
    "Uncertainty Quantification: Flag areas where the system is less confident\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3cxtywSY4tq0",
    "ZG1Ah1eDi7aK",
    "cXHRa9IlMycZ"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
