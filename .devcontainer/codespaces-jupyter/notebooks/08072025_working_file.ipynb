{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1942eb2c",
   "metadata": {},
   "source": [
    "\n",
    "# Notebook Map: Relevance Evaluation Pipeline with Few-Shot + Agentic Enhancements\n",
    "\n",
    "This table of contents provides a structured overview of the notebook, describing each section's purpose and how it fits into the workflow.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Quick Reference\n",
    "- Overview of semantic versioning, few-shot prompting, and agentic conflict resolver.\n",
    "\n",
    "## 2. Imports and Configuration\n",
    "- Load required libraries and define configuration constants (e.g., few-shot parameters, log paths).\n",
    "\n",
    "## 3. Core Utility Functions\n",
    "- `verify_decision`: Ensures model decisions are consistent.\n",
    "- `calculate_rule_confidence`: Computes rule-based confidence from criteria.\n",
    "- `get_next_prompt_version`: Auto-increments semantic prompt version.\n",
    "- `rebuild_few_shot_pool`: Builds balanced few-shot example set from log.\n",
    "- `agentic_conflict_resolver`: Resolves discrepancies between model and rule evaluations.\n",
    "\n",
    "## 4. Data Preparation\n",
    "- Load PDF research papers from `data/raw`.\n",
    "- Truncate text to fit LLM context window.\n",
    "\n",
    "## 5. Few-Shot Prompt Building\n",
    "- Retrieve high-confidence examples from log.\n",
    "- Prepend examples to base relevance prompt.\n",
    "\n",
    "## 6. Main Evaluation Loop\n",
    "- Iterate through PDFs.\n",
    "- Evaluate relevance using LLM.\n",
    "- Apply rule-based scoring and hybrid confidence calculation.\n",
    "- Flag documents for review when model vs. rule confidence diverges.\n",
    "\n",
    "## 7. Logging and Versioning\n",
    "- Append results to `prompt_evaluation_log.json`.\n",
    "- Add `prompt_version`, `decision_source`, and `agentic_resolution` where applicable.\n",
    "\n",
    "## 8. Visualization\n",
    "- Display confidence distribution, relevance drift, and flagged discrepancy trends.\n",
    "\n",
    "## 9. Enhancements (Appended)\n",
    "- Additional functions and logging improvements appended at the end for optional use.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRJ1_wsbi7cZ"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "<font size=10>**End-Term / Final Project**</font>\n",
    "\n",
    "<font size=6>**AI for Research Proposal Automation**</font>\n",
    "\n",
    "### **Business Problem - Create an AI system which will help you writing the research proposal aligning with the NOFO Document**\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsZ53h43336R"
   },
   "source": [
    "Meet Dr. Ian McCulloh, a seasoned research advisor and a leading voice in interdisciplinary science. Over the years, his lab has explored everything from AI for counterterrorism to social network analysis in neuroscience. His publication portfolio is vast, rich, and... chaotic.\n",
    "\n",
    "When the National Institute of Mental Health released a new NOFO (Notice of Funding Opportunity) seeking innovative digital health solutions for mental health equity, Dr. Ian saw an opportunity. But there was a problem: despite his extensive work, none of his existing research was directly aligned with digital mental health interventions. And with NIH deadlines looming, manually identifying relevant angles and generating a competitive proposal would be a massive lift.\n",
    "\n",
    "Dr. Ian wished for a smart assistant—one that could digest his past work, interpret the NOFO’s intent, spark new research directions, and even help draft proposal sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATYzMPf1333q"
   },
   "source": [
    "**The Challenge:**\n",
    "\n",
    "Organizations and researchers often maintain large archives of publications and prior work. When responding to competitive grants—especially highly specific ones like NIH NOFOs—it becomes extremely difficult and time-consuming to:\n",
    "\n",
    "1. Align past work with a new funding call.\n",
    "2. Extract relevant expertise from unrelated projects.\n",
    "3. Ideate novel, fundable research proposals tailored to complex criteria.\n",
    "4. Generate high-quality text for grant submission that satisfies technical and scientific review criteria.\n",
    "\n",
    "The manual effort to sift through dense research documents, match them to nuanced funding criteria, and write compelling, compliant proposals is labor-intensive, inconsistent, and prone to missed opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsDECO7z5eMJ"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "### **The Case Study Approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syX7mfDi5eb8"
   },
   "source": [
    "**Objective**\n",
    "1. Develop a generative AI-powered system using LLMs to automate and optimize the creation of NIH research proposals.\n",
    "2. The tool will identify relevant prior research, generate aligned project ideas, and draft high-quality proposal content tailored to specific NOFO requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given workflow:**\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Read NOFO Document] --> B[Analyze Research Papers]\n",
    "    B --> C[Filter Papers by Topic]\n",
    "    C --> D[Generate Research Ideas]\n",
    "    D --> E[Upload ideas to LLM]\n",
    "    E --> F[Generate Proposal]\n",
    "    F --> G[LLM Evaluation]\n",
    "    G --> H{Meets criteria?}\n",
    "    H -- NO --> F\n",
    "    H -- YES --> I[Human Review]\n",
    "    I --> J{Approved?}\n",
    "    J -- NO --> F\n",
    "    J -- YES --> K[Final Proposal]\n",
    "```\n",
    "\n",
    "**Enhanced workflow based on conversations with ChatGPT and Claude:**\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Read NOFO Document] --> B[Extract Key Requirements & Evaluation Criteria]\n",
    "    B --> C[Multi-Stage Paper Processing<br>(PyPDF → OCR)]\n",
    "    C --> C1[Table Extraction]\n",
    "    C --> C2[Figure Extraction (OCR + Captioning)]\n",
    "    C1 --> D\n",
    "    C2 --> D\n",
    "    D[Hybrid Indexing & Filtering<br>(BM25 + Embeddings + Metadata)]\n",
    "    D --> E[Agentic Research Synthesis<br>(Research Analyst + Proposal Writer + Compliance Checker)]\n",
    "    E --> F[Generate Proposal Blueprint + Draft]\n",
    "    F --> G[Multi-Criteria Evaluation<br/>(RAG + LLM-as-Judge + Guardrails)]\n",
    "    G --> H{Score ≥ Threshold?}\n",
    "    H -- NO --> I[Targeted Refinement Loop<br/>(Weakness-Specific Prompts)]\n",
    "    I --> F\n",
    "    H -- YES --> J[Caching + Checkpointing of Results]\n",
    "    J --> K[Human Review Interface]\n",
    "    K --> L{Approved?}\n",
    "    L -- NO --> M[Capture Feedback & Return to Refinement]\n",
    "    M --> F\n",
    "    L -- YES --> N[Final Proposal + Deliverables]\n",
    "    \n",
    "    subgraph \"Agentic Components\"\n",
    "        E1[Research Analyst Agent]\n",
    "        E2[Proposal Writer Agent]\n",
    "        E3[Compliance Checker Agent]\n",
    "        E1 <--> E2\n",
    "        E2 <--> E3\n",
    "        E3 <--> E1\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cxtywSY4tq0"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Setup - [2 Marks]**\n",
    "---\n",
    "<font color=Red>**Note:**</font> *1 marks is awarded for the Embedding Model configuration and 1 mark for the LLM Configuration.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "b52EI78ZiY1X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "\n",
    "!pip install -q langchain==0.3.21 \\\n",
    "                huggingface_hub==0.29.3 \\\n",
    "                openai==1.68.2 \\\n",
    "                chromadb==0.6.3 \\\n",
    "                langchain-community==0.3.20 \\\n",
    "                langchain_openai==0.3.10 \\\n",
    "                lark==1.2.2 \\\n",
    "                rank_bm25==0.2.2 \\\n",
    "                numpy==2.2.4 \\\n",
    "                scipy==1.15.2 \\\n",
    "                scikit-learn==1.6.1 \\\n",
    "                transformers==4.50.0 \\\n",
    "                pypdf==5.4.0 \\\n",
    "                markdown-pdf==1.7 \\\n",
    "                tiktoken==0.9.0 \\\n",
    "                sentence_transformers==4.0.0 \\\n",
    "                torch==2.6.0 \\\n",
    "                pypdf \\\n",
    "                camelot-py \\\n",
    "                opencv-python \\\n",
    "                pytesseract \\\n",
    "                pdf2image \\\n",
    "                chromadb \\\n",
    "                langchain-chroma \\\n",
    "                langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r ../requirements.txt\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for core functionality\n",
    "import os\n",
    "import warnings\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "base_url = os.getenv(\"OPENAI_BASE_URL\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o-mini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopenai_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOPENAI_API_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOPENAI_BASE_URL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# optional; only if using non-default\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_core/load/serializable.py:130\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:616\u001b[0m, in \u001b[0;36mBaseChatOpenAI.validate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mClient(\n\u001b[1;32m    613\u001b[0m             proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy, verify\u001b[38;5;241m=\u001b[39mglobal_ssl_context\n\u001b[1;32m    614\u001b[0m         )\n\u001b[1;32m    615\u001b[0m     sync_specific \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client}\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msync_specific\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_client.py:114\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    112\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "# Define the LLM Model - Use `gpt-4o-mini` Model\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\")  # optional; only if using non-default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# FEW-SHOT AND LOGGING CONFIG\n",
    "# ------------------------------------------------------------\n",
    "# These constants control how many examples are retrieved and the minimum confidence threshold.\n",
    "# Modify here if you want more or fewer few-shot examples or to change the confidence cutoff.\n",
    "FEW_SHOT_MAX_EXAMPLES = 4         # Total examples (balanced between relevant/irrelevant if possible)\n",
    "# Minimum confidence threshold for including examples in few-shot prompting\n",
    "MIN_CONFIDENCE_FOR_FEWSHOT = 70   # Minimum hybrid confidence (%) to consider for few-shot retrieval\n",
    "\n",
    "# JSON log path\n",
    "# Path to the cleaned JSON log file where prompt evaluation iterations are stored\n",
    "LOG_PATH = \"prompt_evaluation_log_cleaned.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned PDF saved to: ../data/NOFO_cleaned.pdf\n",
      "Cleaning annotations for: cycon-final-draft.pdf\n",
      "Cleaned PDF saved to: data/raw/cycon-final-draft_cleaned.pdf\n",
      "Cleaning annotations for: Chat GPT Bias final w copyright.pdf\n",
      "Cleaned PDF saved to: data/raw/Chat GPT Bias final w copyright_cleaned.pdf\n",
      "Cleaning annotations for: Genetic_Algorithms_for_Prompt_Optimization.pdf\n",
      "Cleaned PDF saved to: data/raw/Genetic_Algorithms_for_Prompt_Optimization_cleaned.pdf\n",
      "Cleaning annotations for: DIVERSE_LLM_Dataset___IEEE_Big_Data.pdf\n",
      "Cleaned PDF saved to: data/raw/DIVERSE_LLM_Dataset___IEEE_Big_Data_cleaned.pdf\n",
      "Cleaning annotations for: Hashtag_Revival.pdf\n",
      "Cleaned PDF saved to: data/raw/Hashtag_Revival_cleaned.pdf\n",
      "Cleaning annotations for: FBI_Recruit_Hire_Final.pdf\n",
      "Cleaned PDF saved to: data/raw/FBI_Recruit_Hire_Final_cleaned.pdf\n",
      "Cleaning annotations for: Benson_MA491_NLP.pdf\n",
      "Cleaned PDF saved to: data/raw/Benson_MA491_NLP_cleaned.pdf\n",
      "Cleaning annotations for: Extreme Cohesion Darknet 20190815.pdf\n",
      "Cleaned PDF saved to: data/raw/Extreme Cohesion Darknet 20190815_cleaned.pdf\n",
      "Cleaning annotations for: Encyclopedia of SNA - R Packages.pdf\n",
      "Cleaned PDF saved to: data/raw/Encyclopedia of SNA - R Packages_cleaned.pdf\n",
      "Cleaning annotations for: RES2D.pdf\n",
      "Cleaned PDF saved to: data/raw/RES2D_cleaned.pdf\n",
      "Cleaning annotations for: ClassifiersCrowdSource.pdf\n",
      "Cleaned PDF saved to: data/raw/ClassifiersCrowdSource_cleaned.pdf\n",
      "Cleaning annotations for: FSS-19_paper_137.pdf\n",
      "Cleaned PDF saved to: data/raw/FSS-19_paper_137_cleaned.pdf\n",
      "Cleaning annotations for: Kidney_Behavioral.pdf\n",
      "Cleaned PDF saved to: data/raw/Kidney_Behavioral_cleaned.pdf\n",
      "Cleaning annotations for: Sim of Decon.pdf\n",
      "Cleaned PDF saved to: data/raw/Sim of Decon_cleaned.pdf\n",
      "Cleaning annotations for: Cross_Platform_Information_Spread_During_the_January_6th_Capitol_Riots.pdf\n",
      "Cleaned PDF saved to: data/raw/Cross_Platform_Information_Spread_During_the_January_6th_Capitol_Riots_cleaned.pdf\n",
      "Cleaning annotations for: Political_Networks_Conference.pdf\n",
      "Cleaned PDF saved to: data/raw/Political_Networks_Conference_cleaned.pdf\n",
      "Cleaning annotations for: On the Science of Networks.pdf\n",
      "Cleaned PDF saved to: data/raw/On the Science of Networks_cleaned.pdf\n",
      "Cleaning annotations for: Simmelian-Gamma-LDA.pdf\n",
      "Cleaned PDF saved to: data/raw/Simmelian-Gamma-LDA_cleaned.pdf\n",
      "Cleaning annotations for: BotBuster___AAAI.pdf\n",
      "Cleaned PDF saved to: data/raw/BotBuster___AAAI_cleaned.pdf\n",
      "Cleaning annotations for: Planning for AI Sustainment A Methodology for Maintenance and Cost Management_V5.pdf\n",
      "Cleaned PDF saved to: data/raw/Planning for AI Sustainment A Methodology for Maintenance and Cost Management_V5_cleaned.pdf\n",
      "Cleaning annotations for: Symbolic Generative AI 20231012.pdf\n",
      "Cleaned PDF saved to: data/raw/Symbolic Generative AI 20231012_cleaned.pdf\n",
      "Cleaning annotations for: 23-US-DHS-001.pdf\n",
      "Cleaned PDF saved to: data/raw/23-US-DHS-001_cleaned.pdf\n",
      "Cleaning annotations for: 2024_ICWSM_Data_Challenge__Post_API_Data_Collection.pdf\n",
      "Cleaned PDF saved to: data/raw/2024_ICWSM_Data_Challenge__Post_API_Data_Collection_cleaned.pdf\n",
      "Cleaning annotations for: Misinformation_Simulation.pdf\n",
      "Cleaned PDF saved to: data/raw/Misinformation_Simulation_cleaned.pdf\n",
      "Cleaning annotations for: Supply Chain Excellence.pdf\n",
      "Cleaned PDF saved to: data/raw/Supply Chain Excellence_cleaned.pdf\n",
      "Cleaning annotations for: 2021_EPJ_MVMCInfoOps.pdf\n",
      "Cleaned PDF saved to: data/raw/2021_EPJ_MVMCInfoOps_cleaned.pdf\n",
      "Cleaning annotations for: Network Simulation Models.pdf\n",
      "Cleaned PDF saved to: data/raw/Network Simulation Models_cleaned.pdf\n",
      "Cleaning annotations for: ALL18.pdf\n",
      "Cleaned PDF saved to: data/raw/ALL18_cleaned.pdf\n",
      "Cleaning annotations for: ONA-in-R.pdf\n",
      "Cleaned PDF saved to: data/raw/ONA-in-R_cleaned.pdf\n",
      "Cleaning annotations for: Helene_and_Milton_ACM.pdf\n",
      "Cleaned PDF saved to: data/raw/Helene_and_Milton_ACM_cleaned.pdf\n",
      "Cleaning annotations for: Unobtrusive Email.pdf\n",
      "Cleaned PDF saved to: data/raw/Unobtrusive Email_cleaned.pdf\n",
      "Cleaning annotations for: docnet.pdf\n",
      "Cleaned PDF saved to: data/raw/docnet_cleaned.pdf\n",
      "Cleaning annotations for: Utility Seeking in Complex Social Systems.pdf\n",
      "Cleaned PDF saved to: data/raw/Utility Seeking in Complex Social Systems_cleaned.pdf\n",
      "Cleaning annotations for: Lessons from Advising in Afghanistan.pdf\n",
      "Cleaned PDF saved to: data/raw/Lessons from Advising in Afghanistan_cleaned.pdf\n",
      "Cleaning annotations for: MOOC 20190828.pdf\n",
      "Cleaned PDF saved to: data/raw/MOOC 20190828_cleaned.pdf\n",
      "Cleaning annotations for: WEIRD.pdf\n",
      "Cleaned PDF saved to: data/raw/WEIRD_cleaned.pdf\n",
      "Cleaning annotations for: IkeNet.pdf\n",
      "Cleaned PDF saved to: data/raw/IkeNet_cleaned.pdf\n",
      "Cleaning annotations for: EmergencyResponseAI.pdf\n",
      "Cleaned PDF saved to: data/raw/EmergencyResponseAI_cleaned.pdf\n",
      "Cleaning annotations for: Quantifying_Information_Advantage.pdf\n",
      "Cleaned PDF saved to: data/raw/Quantifying_Information_Advantage_cleaned.pdf\n",
      "Cleaning annotations for: Confidence_Chaining.pdf\n",
      "Cleaned PDF saved to: data/raw/Confidence_Chaining_cleaned.pdf\n",
      "Cleaning annotations for: RatingsVRankings.pdf\n",
      "Cleaned PDF saved to: data/raw/RatingsVRankings_cleaned.pdf\n",
      "Cleaning annotations for: Analysis_of_Malware_Communities_Using_Multi_Modal_Features.pdf\n",
      "Cleaned PDF saved to: data/raw/Analysis_of_Malware_Communities_Using_Multi_Modal_Features_cleaned.pdf\n",
      "Cleaning annotations for: jfq-110_46-53_Cruickshank.pdf\n",
      "Cleaned PDF saved to: data/raw/jfq-110_46-53_Cruickshank_cleaned.pdf\n",
      "Cleaning annotations for: MLTEing_Models_for_NIER_at_ICSE_2023.pdf\n",
      "Cleaned PDF saved to: data/raw/MLTEing_Models_for_NIER_at_ICSE_2023_cleaned.pdf\n",
      "Cleaning annotations for: SocNetAlQaeda.pdf\n",
      "Cleaned PDF saved to: data/raw/SocNetAlQaeda_cleaned.pdf\n",
      "Cleaning annotations for: Leadership of Data Annotation 20180304v2.pdf\n",
      "Cleaned PDF saved to: data/raw/Leadership of Data Annotation 20180304v2_cleaned.pdf\n",
      "Cleaning annotations for: Parler_Disinformation_Challenge___CMOT_Extended.pdf\n",
      "Cleaned PDF saved to: data/raw/Parler_Disinformation_Challenge___CMOT_Extended_cleaned.pdf\n",
      "Cleaning annotations for: ICWSM_2025_Political_Bias.pdf\n",
      "Cleaned PDF saved to: data/raw/ICWSM_2025_Political_Bias_cleaned.pdf\n",
      "Cleaning annotations for: HIV.pdf\n",
      "Cleaned PDF saved to: data/raw/HIV_cleaned.pdf\n",
      "Cleaning annotations for: Limit Velocity.pdf\n",
      "Cleaned PDF saved to: data/raw/Limit Velocity_cleaned.pdf\n",
      "Cleaning annotations for: Spectral Analysis SNA.pdf\n",
      "Cleaned PDF saved to: data/raw/Spectral Analysis SNA_cleaned.pdf\n",
      "Cleaning annotations for: LLM_Confidence_Metrics.pdf\n",
      "Cleaned PDF saved to: data/raw/LLM_Confidence_Metrics_cleaned.pdf\n",
      "Cleaning annotations for: Food Addiction 20231222 v3.pdf\n",
      "Cleaned PDF saved to: data/raw/Food Addiction 20231222 v3_cleaned.pdf\n",
      "Cleaning annotations for: NBA Performance.pdf\n",
      "Cleaned PDF saved to: data/raw/NBA Performance_cleaned.pdf\n",
      "Cleaning annotations for: MIPB-CDA.pdf\n",
      "Cleaned PDF saved to: data/raw/MIPB-CDA_cleaned.pdf\n",
      "Cleaning annotations for: Vol33Iss1_INSNApdf.pdf\n",
      "Cleaned PDF saved to: data/raw/Vol33Iss1_INSNApdf_cleaned.pdf\n",
      "Cleaning annotations for: NeuroCogInfluence.pdf\n",
      "Cleaned PDF saved to: data/raw/NeuroCogInfluence_cleaned.pdf\n",
      "Cleaning annotations for: Frontiers COVID.pdf\n",
      "Cleaned PDF saved to: data/raw/Frontiers COVID_cleaned.pdf\n",
      "Cleaning annotations for: Evolution_of_Terrorism_PNAS.pdf\n",
      "Cleaned PDF saved to: data/raw/Evolution_of_Terrorism_PNAS_cleaned.pdf\n",
      "Cleaning annotations for: Text Analysis Using Automated Language Translators.pdf\n",
      "Cleaned PDF saved to: data/raw/Text Analysis Using Automated Language Translators_cleaned.pdf\n",
      "Cleaning annotations for: Vulnerable_Code_Detection.pdf\n",
      "Cleaned PDF saved to: data/raw/Vulnerable_Code_Detection_cleaned.pdf\n",
      "Cleaning annotations for: Dormant Bots 20190814.pdf\n",
      "Cleaned PDF saved to: data/raw/Dormant Bots 20190814_cleaned.pdf\n",
      "Cleaning annotations for: NAP Behavioral Sci Intel.pdf\n",
      "Cleaned PDF saved to: data/raw/NAP Behavioral Sci Intel_cleaned.pdf\n",
      "Cleaning annotations for: YouTube-COVID.pdf\n",
      "Cleaned PDF saved to: data/raw/YouTube-COVID_cleaned.pdf\n",
      "Cleaning annotations for: Organizational risk using network analysis.pdf\n",
      "Cleaned PDF saved to: data/raw/Organizational risk using network analysis_cleaned.pdf\n",
      "Cleaning annotations for: k-truss.pdf\n",
      "Cleaned PDF saved to: data/raw/k-truss_cleaned.pdf\n",
      "Cleaning annotations for: Political Party Cohesion.pdf\n",
      "Cleaned PDF saved to: data/raw/Political Party Cohesion_cleaned.pdf\n",
      "Cleaning annotations for: Sailer McCulloh Soc Net and Spatial Config.pdf\n",
      "Cleaned PDF saved to: data/raw/Sailer McCulloh Soc Net and Spatial Config_cleaned.pdf\n",
      "Cleaning annotations for: CUSUM Parameterization.pdf\n",
      "Cleaned PDF saved to: data/raw/CUSUM Parameterization_cleaned.pdf\n",
      "Cleaning annotations for: Multi_view_Clustering_for_Social_Based_Data.pdf\n",
      "Cleaned PDF saved to: data/raw/Multi_view_Clustering_for_Social_Based_Data_cleaned.pdf\n",
      "Cleaning annotations for: ONA-using-igraph.pdf\n",
      "Cleaned PDF saved to: data/raw/ONA-using-igraph_cleaned.pdf\n",
      "Cleaning annotations for: improving-decision-support-for-organ-transplant.pdf\n",
      "Cleaned PDF saved to: data/raw/improving-decision-support-for-organ-transplant_cleaned.pdf\n",
      "Cleaning annotations for: Tweets-to-touchdowns.pdf\n",
      "Cleaned PDF saved to: data/raw/Tweets-to-touchdowns_cleaned.pdf\n",
      "Cleaning annotations for: Social_Det_COVID_Mortality.pdf\n",
      "Cleaned PDF saved to: data/raw/Social_Det_COVID_Mortality_cleaned.pdf\n",
      "Cleaning annotations for: White Paper Brain Gaze.pdf\n",
      "Cleaned PDF saved to: data/raw/White Paper Brain Gaze_cleaned.pdf\n",
      "Cleaning annotations for: McCullohCarleyJOSS.pdf\n",
      "Cleaned PDF saved to: data/raw/McCullohCarleyJOSS_cleaned.pdf\n",
      "Cleaning annotations for: Course Info Security.pdf\n",
      "Cleaned PDF saved to: data/raw/Course Info Security_cleaned.pdf\n",
      "Cleaning annotations for: A_Complex_Network_Approach_to_Find_Latent_Terorrist_Communities.pdf\n",
      "Cleaned PDF saved to: data/raw/A_Complex_Network_Approach_to_Find_Latent_Terorrist_Communities_cleaned.pdf\n",
      "Cleaning annotations for: Knowing the Terrain.pdf\n",
      "Cleaned PDF saved to: data/raw/Knowing the Terrain_cleaned.pdf\n",
      "Cleaning annotations for: Social_Network_Probability_Mechanics.pdf\n",
      "Cleaned PDF saved to: data/raw/Social_Network_Probability_Mechanics_cleaned.pdf\n",
      "Cleaning annotations for: The ABCs of AI-Enabled Intelligence Analysis - War on the Rocks.pdf\n",
      "Cleaned PDF saved to: data/raw/The ABCs of AI-Enabled Intelligence Analysis - War on the Rocks_cleaned.pdf\n",
      "Cleaning annotations for: SocNetChgDet.pdf\n",
      "Cleaned PDF saved to: data/raw/SocNetChgDet_cleaned.pdf\n",
      "Cleaning annotations for: Take_boards.pdf\n",
      "Cleaned PDF saved to: data/raw/Take_boards_cleaned.pdf\n",
      "Cleaning annotations for: Arrow White Paper DExTra.pdf\n",
      "Cleaned PDF saved to: data/raw/Arrow White Paper DExTra_cleaned.pdf\n",
      "Cleaning annotations for: TrainingSetSize.pdf\n",
      "Cleaned PDF saved to: data/raw/TrainingSetSize_cleaned.pdf\n",
      "Cleaning annotations for: CausalOrgInorgContent.pdf\n",
      "Cleaned PDF saved to: data/raw/CausalOrgInorgContent_cleaned.pdf\n",
      "Cleaning annotations for: Overcoming_Social_Media_API_Restrictions__Building_an_Effective_Web_Scraper.pdf\n",
      "Cleaned PDF saved to: data/raw/Overcoming_Social_Media_API_Restrictions__Building_an_Effective_Web_Scraper_cleaned.pdf\n",
      "Cleaning annotations for: LLM_UQ.pdf\n",
      "Cleaned PDF saved to: data/raw/LLM_UQ_cleaned.pdf\n",
      "Cleaning annotations for: Kent2022_Chapter_MicroscopicMarkovChainApproach.pdf\n",
      "Cleaned PDF saved to: data/raw/Kent2022_Chapter_MicroscopicMarkovChainApproach_cleaned.pdf\n",
      "Cleaning annotations for: Acquiring Maintainable AI_Enable Systems_Final.pdf\n",
      "Cleaned PDF saved to: data/raw/Acquiring Maintainable AI_Enable Systems_Final_cleaned.pdf\n",
      "Cleaning annotations for: Characterizing_Communities_of_Hashtag_Usage_on_Twitter_During_the_2020_COVID_19_Pandemic.pdf\n",
      "Cleaned PDF saved to: data/raw/Characterizing_Communities_of_Hashtag_Usage_on_Twitter_During_the_2020_COVID_19_Pandemic_cleaned.pdf\n",
      "Cleaning annotations for: SecurityPrivAIML.pdf\n",
      "Cleaned PDF saved to: data/raw/SecurityPrivAIML_cleaned.pdf\n",
      "Cleaning annotations for: AAAI IAA CV.pdf\n",
      "Cleaned PDF saved to: data/raw/AAAI IAA CV_cleaned.pdf\n",
      "Cleaning annotations for: LSA email.pdf\n",
      "Cleaned PDF saved to: data/raw/LSA email_cleaned.pdf\n",
      "Cleaning annotations for: Clustering_Analysis_of_Website_Usage_on_Twitter_during_the_COVID_19_Pandemic.pdf\n",
      "Cleaned PDF saved to: data/raw/Clustering_Analysis_of_Website_Usage_on_Twitter_during_the_COVID_19_Pandemic_cleaned.pdf\n",
      "Cleaning annotations for: Designed Networks.pdf\n",
      "Cleaned PDF saved to: data/raw/Designed Networks_cleaned.pdf\n",
      "Cleaning annotations for: COVID Bayesian Data Aug.pdf\n",
      "Cleaned PDF saved to: data/raw/COVID Bayesian Data Aug_cleaned.pdf\n",
      "Cleaning annotations for: Cohort_Optimization_Methods_SNAMS_2021_working_draft (4).pdf\n",
      "Cleaned PDF saved to: data/raw/Cohort_Optimization_Methods_SNAMS_2021_working_draft (4)_cleaned.pdf\n",
      "Cleaning annotations for: SM Customer Feedback_FAB_2019_rev3.pdf\n",
      "Cleaned PDF saved to: data/raw/SM Customer Feedback_FAB_2019_rev3_cleaned.pdf\n",
      "Cleaning annotations for: IkekNet1.pdf\n",
      "Cleaned PDF saved to: data/raw/IkekNet1_cleaned.pdf\n",
      "Cleaning annotations for: Social Media Mental Health Final.pdf\n",
      "Cleaned PDF saved to: data/raw/Social Media Mental Health Final_cleaned.pdf\n",
      "Cleaning annotations for: Reforming Sectarian Beliefs.pdf\n",
      "Cleaned PDF saved to: data/raw/Reforming Sectarian Beliefs_cleaned.pdf\n",
      "Cleaning annotations for: Savas.pdf\n",
      "Cleaned PDF saved to: data/raw/Savas_cleaned.pdf\n",
      "Cleaning annotations for: Data_Education__Emerging_Challenges_and_Opportunities.pdf\n",
      "Cleaned PDF saved to: data/raw/Data_Education__Emerging_Challenges_and_Opportunities_cleaned.pdf\n",
      "Cleaning annotations for: Dissertation.pdf\n",
      "Cleaned PDF saved to: data/raw/Dissertation_cleaned.pdf\n",
      "Cleaning annotations for: Leveraging_AI_to_Improve_Viral_Information_Detection_in_Online_Discourse.pdf\n",
      "Cleaned PDF saved to: data/raw/Leveraging_AI_to_Improve_Viral_Information_Detection_in_Online_Discourse_cleaned.pdf\n",
      "Cleaning annotations for: ICWSM___Use_of_Large_Language_Models_for_Stance_Classification.pdf\n",
      "Cleaned PDF saved to: data/raw/ICWSM___Use_of_Large_Language_Models_for_Stance_Classification_cleaned.pdf\n",
      "Cleaning annotations for: NeuroSynchrony.pdf\n",
      "Cleaned PDF saved to: data/raw/NeuroSynchrony_cleaned.pdf\n",
      "Cleaning annotations for: Multi_Agent_Systems_for_Frame_Detection.pdf\n",
      "Cleaned PDF saved to: data/raw/Multi_Agent_Systems_for_Frame_Detection_cleaned.pdf\n",
      "Cleaning annotations for: Review of R Packages_20161026.pdf\n",
      "Cleaned PDF saved to: data/raw/Review of R Packages_20161026_cleaned.pdf\n",
      "Cleaning annotations for: Lead-Azide.pdf\n",
      "Cleaned PDF saved to: data/raw/Lead-Azide_cleaned.pdf\n",
      "Cleaning annotations for: LongNetViewerORA.pdf\n",
      "Cleaned PDF saved to: data/raw/LongNetViewerORA_cleaned.pdf\n",
      "Annotation removal log written to: annotation_log.json\n",
      "All research PDFs cleaned and saved in data/raw/\n"
     ]
    }
   ],
   "source": [
    "# PDF Cleaning Step: Remove non-visual annotations (comments, links, form fields)\n",
    "# Keeps images, diagrams, and visible callouts intact\n",
    "\n",
    "# Import required libraries for core functionality\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import json  # <-- NEW: Required to write annotation logs\n",
    "\n",
    "# Create a global dictionary to store removed annotations\n",
    "annotation_log = {}  # <-- NEW: Accumulates logs of all removed annotations\n",
    "\n",
    "# Initial standardization step to remove annotations for parsing\n",
    "def clean_pdf_annotations(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Strips non-visual annotations (comments, form fields, links) from a PDF\n",
    "    while preserving visible images and diagrams.\n",
    "    Also logs removed annotations to a global dictionary.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(input_path)\n",
    "    removed_annots = []  # <-- NEW: Stores removed annotations for this PDF\n",
    "\n",
    "    for page in doc:\n",
    "        # Iterate over all annotations (not images)\n",
    "        annot = page.first_annot\n",
    "        while annot:\n",
    "            next_annot = annot.next  # store reference to next annotation\n",
    "            \n",
    "            # Try to extract meaningful annotation content\n",
    "            try:\n",
    "                annot_info = annot.info  # Dictionary of annotation metadata\n",
    "                content = annot_info.get(\"content\", \"\").strip()\n",
    "                subtype = annot_info.get(\"subtype\", \"\").strip()\n",
    "                if content:\n",
    "                    removed_annots.append(f\"{subtype}: {content}\")\n",
    "                else:\n",
    "                    removed_annots.append(f\"{subtype}: [no content]\")\n",
    "            except Exception as e:\n",
    "                # Fallback if annotation metadata is inaccessible\n",
    "                removed_annots.append(\"Unknown annotation (could not extract content)\")\n",
    "\n",
    "            # Remove annotation object (highlights, comments, links)\n",
    "            page.delete_annot(annot)\n",
    "            annot = next_annot\n",
    "\n",
    "    # Save cleaned PDF\n",
    "    doc.save(output_path, garbage=4, deflate=True)\n",
    "    doc.close()\n",
    "\n",
    "    # Add entry to annotation log using the input filename as key\n",
    "    annotation_log[os.path.basename(input_path)] = removed_annots  # <-- NEW: Log entries keyed by file\n",
    "\n",
    "# Clean NOFO file\n",
    "input_pdf = \"../data/NOFO.pdf\"\n",
    "cleaned_pdf = \"../data/NOFO_cleaned.pdf\"\n",
    "clean_pdf_annotations(input_pdf, cleaned_pdf)\n",
    "print(f\"Cleaned PDF saved to: {cleaned_pdf}\")\n",
    "\n",
    "# Get de-annotated NOFO doc content using PyPDFLoader for evaluation step\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "pdf_file = \"../data/NOFO_cleaned.pdf\"\n",
    "pdf_loader = PyPDFLoader(pdf_file)\n",
    "NOFO_pdf = pdf_loader.load()\n",
    "\n",
    "# Prepare output folder for de-annotated research papers\n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "\n",
    "# Set variables for de-annotating the research paper PDF collection \n",
    "source_dir = \"../content\"\n",
    "output_dir = \"data/raw\"\n",
    "\n",
    "# Loop through content folder, de-annotate each PDF, and save to a 'clean' output directory\n",
    "for file_name in os.listdir(source_dir):\n",
    "    if file_name.lower().endswith(\".pdf\"):\n",
    "        input_pdf = os.path.join(source_dir, file_name)\n",
    "        cleaned_pdf = os.path.join(output_dir, file_name.replace(\".pdf\", \"_cleaned.pdf\"))\n",
    "        print(f\"Cleaning annotations for: {file_name}\")\n",
    "        clean_pdf_annotations(input_pdf, cleaned_pdf)\n",
    "        print(f\"Cleaned PDF saved to: {cleaned_pdf}\")\n",
    "\n",
    "# Write annotation log to disk after all PDFs are processed\n",
    "log_path = \"annotation_log.json\"  # <-- NEW: File to store the annotation log\n",
    "with open(log_path, \"w\", encoding=\"utf-8\") as log_file:\n",
    "    json.dump(annotation_log, log_file, indent=2, ensure_ascii=False)  # <-- NEW: Write log to file\n",
    "print(f\"Annotation removal log written to: {log_path}\")  # <-- NEW: Confirm log creation\n",
    "\n",
    "print(\"All research PDFs cleaned and saved in data/raw/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and Chunk Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split cleaned text into 3000-token chunks with overlap for RAG\n",
    "\n",
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "def chunk_text(text, chunk_size=3000, overlap=200):\n",
    "    tokens = encoding.encode(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        chunk_tokens = tokens[i:i+chunk_size]\n",
    "        chunks.append(encoding.decode(chunk_tokens))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean extracted text by:\n",
    "# - removing headers/footers\n",
    "# - removing noise\n",
    "# - fixing multi-column layout issues\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_extracted_text(text):\n",
    "    \"\"\"Remove noise (page numbers, headers, footers), merge hyphenated words,\n",
    "    and flatten potential two-column layouts.\"\"\"\n",
    "    # Remove common noise patterns\n",
    "    text = re.sub(r'\\bPage \\d+\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\d+ of \\d+', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove repeated headers/footers (heuristic: lines repeated >3 times)\n",
    "    lines = text.split('\\n')\n",
    "    freq = {}\n",
    "    for line in lines:\n",
    "        freq[line] = freq.get(line, 0) + 1\n",
    "    lines = [line for line in lines if freq[line] <= 3]\n",
    "\n",
    "    # Merge hyphenated words and normalize whitespace\n",
    "    text = re.sub(r'-\\n', '', '\\n'.join(lines))\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "    # Merge two-column text by pairing lines\n",
    "    merged_lines = []\n",
    "    lines = text.split('\\n')\n",
    "    for i in range(0, len(lines), 2):\n",
    "        if i+1 < len(lines):\n",
    "            merged_lines.append(lines[i] + \" \" + lines[i+1])\n",
    "        else:\n",
    "            merged_lines.append(lines[i])\n",
    "    return \"\\n\".join(merged_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract, clean, and chunk research paper pdfs\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def process_pdf_multistage(file_path):\n",
    "    content = \"\"\n",
    "\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text() or \"\"\n",
    "            content += page_text\n",
    "    except Exception as e:\n",
    "        print(f\"PyPDF extraction failed: {e}\")\n",
    "\n",
    "    # Clean text\n",
    "    cleaned_text = clean_extracted_text(content)\n",
    "\n",
    "    # Chunk text\n",
    "    chunks = chunk_text(cleaned_text)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: data/raw/AAAI IAA CV_cleaned.pdf\n",
      "Processing: data/raw/Sim of Decon_cleaned.pdf\n",
      "Processing: data/raw/BotBuster___AAAI_cleaned.pdf\n",
      "Processing: data/raw/Political_Networks_Conference_cleaned.pdf\n",
      "Processing: data/raw/EmergencyResponseAI_cleaned.pdf\n",
      "Processing: data/raw/FSS-19_paper_137_cleaned.pdf\n",
      "Processing: data/raw/DIVERSE_LLM_Dataset___IEEE_Big_Data_cleaned.pdf\n",
      "Processing: data/raw/Clustering_Analysis_of_Website_Usage_on_Twitter_during_the_COVID_19_Pandemic_cleaned.pdf\n",
      "Processing: data/raw/Cohort_Optimization_Methods_SNAMS_2021_working_draft (4)_cleaned.pdf\n",
      "Processing: data/raw/Lead-Azide_cleaned.pdf\n",
      "Processing: data/raw/Knowing the Terrain_cleaned.pdf\n",
      "Processing: data/raw/Leadership of Data Annotation 20180304v2_cleaned.pdf\n",
      "Processing: data/raw/A_Complex_Network_Approach_to_Find_Latent_Terorrist_Communities_cleaned.pdf\n",
      "Processing: data/raw/Designed Networks_cleaned.pdf\n",
      "Processing: data/raw/Organizational risk using network analysis_cleaned.pdf\n",
      "Processing: data/raw/LongNetViewerORA_cleaned.pdf\n",
      "Processing: data/raw/Unobtrusive Email_cleaned.pdf\n",
      "Processing: data/raw/SM Customer Feedback_FAB_2019_rev3_cleaned.pdf\n",
      "Processing: data/raw/Benson_MA491_NLP_cleaned.pdf\n",
      "Processing: data/raw/SecurityPrivAIML_cleaned.pdf\n",
      "Processing: data/raw/SocNetAlQaeda_cleaned.pdf\n",
      "Processing: data/raw/COVID Bayesian Data Aug_cleaned.pdf\n",
      "Processing: data/raw/Savas_cleaned.pdf\n",
      "Processing: data/raw/Lessons from Advising in Afghanistan_cleaned.pdf\n",
      "Processing: data/raw/Confidence_Chaining_cleaned.pdf\n",
      "Processing: data/raw/Limit Velocity_cleaned.pdf\n",
      "Processing: data/raw/Text Analysis Using Automated Language Translators_cleaned.pdf\n",
      "Processing: data/raw/Multi_Agent_Systems_for_Frame_Detection_cleaned.pdf\n",
      "Processing: data/raw/IkekNet1_cleaned.pdf\n",
      "Processing: data/raw/Dissertation_cleaned.pdf\n",
      "Processing: data/raw/LLM_UQ_cleaned.pdf\n",
      "Processing: data/raw/Parler_Disinformation_Challenge___CMOT_Extended_cleaned.pdf\n",
      "Processing: data/raw/LSA email_cleaned.pdf\n",
      "Processing: data/raw/Political Party Cohesion_cleaned.pdf\n",
      "Processing: data/raw/Spectral Analysis SNA_cleaned.pdf\n",
      "Processing: data/raw/NeuroCogInfluence_cleaned.pdf\n",
      "Processing: data/raw/Simmelian-Gamma-LDA_cleaned.pdf\n",
      "Processing: data/raw/MIPB-CDA_cleaned.pdf\n",
      "Processing: data/raw/CausalOrgInorgContent_cleaned.pdf\n",
      "Processing: data/raw/Genetic_Algorithms_for_Prompt_Optimization_cleaned.pdf\n",
      "Processing: data/raw/ALL18_cleaned.pdf\n",
      "Processing: data/raw/LLM_Confidence_Metrics_cleaned.pdf\n",
      "Processing: data/raw/FBI_Recruit_Hire_Final_cleaned.pdf\n",
      "Processing: data/raw/ICWSM_2025_Political_Bias_cleaned.pdf\n",
      "Processing: data/raw/Review of R Packages_20161026_cleaned.pdf\n",
      "Processing: data/raw/Evolution_of_Terrorism_PNAS_cleaned.pdf\n",
      "Processing: data/raw/Planning for AI Sustainment A Methodology for Maintenance and Cost Management_V5_cleaned.pdf\n",
      "Processing: data/raw/Social Media Mental Health Final_cleaned.pdf\n",
      "Processing: data/raw/Cross_Platform_Information_Spread_During_the_January_6th_Capitol_Riots_cleaned.pdf\n",
      "Processing: data/raw/Multi_view_Clustering_for_Social_Based_Data_cleaned.pdf\n",
      "Processing: data/raw/Extreme Cohesion Darknet 20190815_cleaned.pdf\n",
      "Processing: data/raw/Social_Det_COVID_Mortality_cleaned.pdf\n",
      "Processing: data/raw/White Paper Brain Gaze_cleaned.pdf\n",
      "Processing: data/raw/Encyclopedia of SNA - R Packages_cleaned.pdf\n",
      "Processing: data/raw/CUSUM Parameterization_cleaned.pdf\n",
      "Processing: data/raw/Tweets-to-touchdowns_cleaned.pdf\n",
      "Processing: data/raw/SocNetChgDet_cleaned.pdf\n",
      "Processing: data/raw/23-US-DHS-001_cleaned.pdf\n",
      "Processing: data/raw/Analysis_of_Malware_Communities_Using_Multi_Modal_Features_cleaned.pdf\n",
      "Processing: data/raw/Acquiring Maintainable AI_Enable Systems_Final_cleaned.pdf\n",
      "Processing: data/raw/Food Addiction 20231222 v3_cleaned.pdf\n",
      "Processing: data/raw/HIV_cleaned.pdf\n",
      "Processing: data/raw/TrainingSetSize_cleaned.pdf\n",
      "Processing: data/raw/WEIRD_cleaned.pdf\n",
      "Processing: data/raw/Dormant Bots 20190814_cleaned.pdf\n",
      "Processing: data/raw/The ABCs of AI-Enabled Intelligence Analysis - War on the Rocks_cleaned.pdf\n",
      "Processing: data/raw/improving-decision-support-for-organ-transplant_cleaned.pdf\n",
      "Processing: data/raw/Leveraging_AI_to_Improve_Viral_Information_Detection_in_Online_Discourse_cleaned.pdf\n",
      "Processing: data/raw/Vulnerable_Code_Detection_cleaned.pdf\n",
      "Processing: data/raw/Overcoming_Social_Media_API_Restrictions__Building_an_Effective_Web_Scraper_cleaned.pdf\n",
      "Processing: data/raw/Supply Chain Excellence_cleaned.pdf\n",
      "Processing: data/raw/Reforming Sectarian Beliefs_cleaned.pdf\n",
      "Processing: data/raw/ICWSM___Use_of_Large_Language_Models_for_Stance_Classification_cleaned.pdf\n",
      "Processing: data/raw/Course Info Security_cleaned.pdf\n",
      "Processing: data/raw/Take_boards_cleaned.pdf\n",
      "Processing: data/raw/Hashtag_Revival_cleaned.pdf\n",
      "Processing: data/raw/Misinformation_Simulation_cleaned.pdf\n",
      "Processing: data/raw/Network Simulation Models_cleaned.pdf\n",
      "Processing: data/raw/cycon-final-draft_cleaned.pdf\n",
      "Processing: data/raw/NAP Behavioral Sci Intel_cleaned.pdf\n",
      "Processing: data/raw/Utility Seeking in Complex Social Systems_cleaned.pdf\n",
      "Processing: data/raw/k-truss_cleaned.pdf\n",
      "Processing: data/raw/Arrow White Paper DExTra_cleaned.pdf\n",
      "Processing: data/raw/Helene_and_Milton_ACM_cleaned.pdf\n",
      "Processing: data/raw/Data_Education__Emerging_Challenges_and_Opportunities_cleaned.pdf\n",
      "Processing: data/raw/RatingsVRankings_cleaned.pdf\n",
      "Processing: data/raw/RES2D_cleaned.pdf\n",
      "Processing: data/raw/McCullohCarleyJOSS_cleaned.pdf\n",
      "Processing: data/raw/ONA-using-igraph_cleaned.pdf\n",
      "Processing: data/raw/Vol33Iss1_INSNApdf_cleaned.pdf\n",
      "Processing: data/raw/Kidney_Behavioral_cleaned.pdf\n",
      "Processing: data/raw/ClassifiersCrowdSource_cleaned.pdf\n",
      "Processing: data/raw/MLTEing_Models_for_NIER_at_ICSE_2023_cleaned.pdf\n",
      "Processing: data/raw/Quantifying_Information_Advantage_cleaned.pdf\n",
      "Processing: data/raw/NeuroSynchrony_cleaned.pdf\n",
      "Processing: data/raw/Frontiers COVID_cleaned.pdf\n",
      "Processing: data/raw/Kent2022_Chapter_MicroscopicMarkovChainApproach_cleaned.pdf\n",
      "Processing: data/raw/jfq-110_46-53_Cruickshank_cleaned.pdf\n",
      "Processing: data/raw/Sailer McCulloh Soc Net and Spatial Config_cleaned.pdf\n",
      "Processing: data/raw/MOOC 20190828_cleaned.pdf\n",
      "Processing: data/raw/2021_EPJ_MVMCInfoOps_cleaned.pdf\n",
      "Processing: data/raw/ONA-in-R_cleaned.pdf\n",
      "Processing: data/raw/docnet_cleaned.pdf\n",
      "Processing: data/raw/Characterizing_Communities_of_Hashtag_Usage_on_Twitter_During_the_2020_COVID_19_Pandemic_cleaned.pdf\n",
      "Processing: data/raw/Symbolic Generative AI 20231012_cleaned.pdf\n",
      "Processing: data/raw/NBA Performance_cleaned.pdf\n",
      "Processing: data/raw/Chat GPT Bias final w copyright_cleaned.pdf\n",
      "Processing: data/raw/IkeNet_cleaned.pdf\n",
      "Processing: data/raw/On the Science of Networks_cleaned.pdf\n",
      "Processing: data/raw/Social_Network_Probability_Mechanics_cleaned.pdf\n",
      "Processing: data/raw/2024_ICWSM_Data_Challenge__Post_API_Data_Collection_cleaned.pdf\n",
      "Processing: data/raw/YouTube-COVID_cleaned.pdf\n",
      "Saved cleaned + chunked text for 112 PDFs to data/cleaned_chunked_papers.json\n"
     ]
    }
   ],
   "source": [
    "# Extract, clean, chunk, and store raw chunks for all research paper PDFs\n",
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "# Folder containing PDFs\n",
    "pdf_folder = \"data/raw\"\n",
    "\n",
    "# Output JSON file\n",
    "output_json_path = \"data/cleaned_chunked_papers.json\"\n",
    "\n",
    "# List to store results (each as a dict with id + chunks)\n",
    "all_chunks = []\n",
    "\n",
    "# Loop through all PDF files in the folder\n",
    "for pdf_path in glob(os.path.join(pdf_folder, \"*.pdf\")):\n",
    "    doc_name = os.path.basename(pdf_path)  # Use filename as ID\n",
    "    print(f\"Processing: {pdf_path}\")\n",
    "    try:\n",
    "        chunks = process_pdf_multistage(pdf_path)  # uses your existing function\n",
    "        all_chunks.append({\n",
    "            \"id\": doc_name,\n",
    "            \"chunks\": chunks\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {e}\")\n",
    "\n",
    "# Save results to JSON\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved cleaned + chunked text for {len(all_chunks)} PDFs to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-shot Prompt Setup for Assessing Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"prompt_evaluation_log_cleaned.json\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# FEW-SHOT FALLBACK EXAMPLES\n",
    "# ------------------------------------------------------------\n",
    "FALLBACK_EXAMPLES = [\n",
    "    (\n",
    "        \"Digital CBT for Adolescents\",\n",
    "        \"\"\"{\n",
    "  \"criteria_results\": {\n",
    "    \"domain_relevance\": \"YES - focuses on mental health digital interventions\",\n",
    "    \"methodological_alignment\": \"YES - randomized controlled trial design\",\n",
    "    \"theoretical_connection\": \"NO - lacks explicit framework reference\",\n",
    "    \"practical_application\": \"YES - informs deployment in youth settings\"\n",
    "  },\n",
    "  \"decision\": \"RELEVANT\",\n",
    "  \"confidence\": \"85\",\n",
    "  \"summary\": \"This study evaluates a mobile CBT app for adolescents, showing significant reduction in anxiety and depression symptoms compared to control. It highlights engagement strategies relevant to NOFO objectives.\"\n",
    "}\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"Oncology Drug Delivery Review\",\n",
    "        \"\"\"{\n",
    "  \"criteria_results\": {\n",
    "    \"domain_relevance\": \"NO - focuses on oncology drug mechanisms\",\n",
    "    \"methodological_alignment\": \"NO\",\n",
    "    \"theoretical_connection\": \"NO\",\n",
    "    \"practical_application\": \"NO\"\n",
    "  },\n",
    "  \"decision\": \"PAPER NOT RELATED TO TOPIC\",\n",
    "  \"confidence\": \"0\",\n",
    "  \"summary\": null\n",
    "}\"\"\"\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# FEW-SHOT RETRIEVAL + PROMPT BUILDER\n",
    "# ------------------------------------------------------------\n",
    "def get_few_shot_examples(\n",
    "    json_path,\n",
    "# Define configuration for few-shot example retrieval (number of examples)\n",
    "    max_examples=FEW_SHOT_MAX_EXAMPLES,\n",
    "# Minimum confidence threshold for including examples in few-shot prompting\n",
    "    min_confidence=MIN_CONFIDENCE_FOR_FEWSHOT\n",
    "):\n",
    "    \"\"\"\n",
    "    Pulls balanced high-confidence examples from log or uses fallback if none found.\n",
    "    \"\"\"\n",
    "# Import required libraries for core functionality\n",
    "    import os, json, random\n",
    "\n",
    "    examples = []\n",
    "\n",
    "    # Attempt to pull from log\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                data = []\n",
    "\n",
    "        relevant_examples, irrelevant_examples = [], []\n",
    "\n",
    "        for iteration in data:\n",
    "            for doc in iteration.get(\"relevant_documents\", []):\n",
    "                hybrid_conf = max(doc.get(\"model_confidence\", 0), doc.get(\"rule_confidence\", 0))\n",
    "                if hybrid_conf >= min_confidence:\n",
    "                    relevant_examples.append((doc[\"title\"], doc[\"reasoning\"]))\n",
    "\n",
    "            for doc in iteration.get(\"irrelevant_documents\", []):\n",
    "                irrelevant_examples.append((doc, \"PAPER NOT RELATED TO TOPIC\"))\n",
    "\n",
    "        # Balance relevant and irrelevant (half and half)\n",
    "        half = max_examples // 2\n",
    "        random.shuffle(relevant_examples)\n",
    "        random.shuffle(irrelevant_examples)\n",
    "        selected_relevant = relevant_examples[:half]\n",
    "        selected_irrelevant = irrelevant_examples[:half]\n",
    "        examples = selected_relevant + selected_irrelevant\n",
    "\n",
    "    # Fallback if no examples found\n",
    "    if not examples:\n",
    "        print(\"No high-confidence examples found. Using fallback seed examples.\")\n",
    "        examples = FALLBACK_EXAMPLES[:max_examples]\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def build_prompt_with_examples(topic, base_prompt, examples):\n",
    "    \"\"\"\n",
    "    Prepend few-shot examples (from log or fallback) to the base prompt.\n",
    "    \"\"\"\n",
    "    if not examples:\n",
    "        return base_prompt\n",
    "\n",
    "    examples_str = \"\\n\\n\".join(\n",
    "        [f\"Example ({title}):\\n{reasoning}\" for title, reasoning in examples]\n",
    "    )\n",
    "\n",
    "    return f\"\"\"\n",
    "You are a research grant specialist evaluating research papers for relevance to NIH NOFO objectives: {topic}.\n",
    "\n",
    "Below are examples of prior evaluations for context:\n",
    "{examples_str}\n",
    "\n",
    "Now evaluate the following paper using the same structure and logic:\n",
    "\n",
    "{base_prompt}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import numpy as np\n",
    "# from rank_bm25 import BM25Okapi\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # -------------------------------------------------------------------------\n",
    "# # Load Pre-trained Model for Semantic Embeddings\n",
    "# # -------------------------------------------------------------------------\n",
    "# # We use a compact, general-purpose sentence embedding model (\"all-MiniLM-L6-v2\")\n",
    "# # from SentenceTransformers. It transforms text chunks and queries into dense\n",
    "# # vector representations, which allows us to measure semantic similarity.\n",
    "# #\n",
    "# # Why MiniLM-L6-v2?\n",
    "# # - Lightweight: Fast inference, suitable for many documents (112 PDFs in our case).\n",
    "# # - High-quality embeddings: Captures semantic meaning (not just keyword overlap).\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# # -------------------------------------------------------------------------\n",
    "# # Load Preprocessed Chunked Data\n",
    "# # -------------------------------------------------------------------------\n",
    "# # The JSON file \"cleaned_chunked_papers.json\" was created earlier by processing\n",
    "# # 112 research PDFs:\n",
    "# #   - Text was extracted and cleaned\n",
    "# #   - Documents were split into manageable chunks (e.g., 200-500 tokens)\n",
    "# #   - Stored as a list of objects: { \"id\": <pdf_name>, \"chunks\": [chunk1, chunk2, ...] }\n",
    "# #\n",
    "# # We now load this data for use in retrieval and scoring.\n",
    "# json_path = \"data/cleaned_chunked_papers.json\"\n",
    "# with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     papers_data = json.load(f)\n",
    "\n",
    "# # -------------------------------------------------------------------------\n",
    "# # Flatten Chunks for Global Search\n",
    "# # -------------------------------------------------------------------------\n",
    "# # We combine all chunks from all papers into a single list (`all_chunks`) so we\n",
    "# # can perform retrieval across the entire corpus at once.\n",
    "# #\n",
    "# # Additionally, we maintain a parallel list `chunk_to_paper` to keep track of\n",
    "# # which paper each chunk originated from. This is critical for:\n",
    "# # - Providing metadata in search results\n",
    "# # - Avoiding loss of context when presenting retrieved information\n",
    "# all_chunks = []\n",
    "# chunk_to_paper = []  # Map each chunk back to its paper ID for context\n",
    "\n",
    "# for paper in papers_data:\n",
    "#     paper_id = paper[\"id\"]  # e.g., \"paper1.pdf\"\n",
    "#     for chunk in paper[\"chunks\"]:\n",
    "#         all_chunks.append(chunk)\n",
    "#         chunk_to_paper.append(paper_id)\n",
    "\n",
    "# print(f\"Loaded {len(all_chunks)} chunks from {len(papers_data)} papers.\")\n",
    "\n",
    "# # -------------------------------------------------------------------------\n",
    "# # Hybrid Scoring Function (BM25 + Cosine Similarity)\n",
    "# # -------------------------------------------------------------------------\n",
    "# # Why Hybrid?\n",
    "# # - BM25 (lexical scoring): Measures keyword relevance (good for exact matches).\n",
    "# # - Cosine similarity (semantic scoring): Measures meaning similarity via embeddings.\n",
    "# # - Combining both provides robustness:\n",
    "# #     * BM25 captures important keywords (e.g., \"serotonin\", \"MRI\")\n",
    "# #     * Embeddings capture meaning (e.g., \"brain scan\" ≈ \"neuroimaging\")\n",
    "# # - We average them here, but weighting is possible (e.g., 70% semantic, 30% lexical).\n",
    "\n",
    "# def score_chunks(query, chunks):\n",
    "#     \"\"\"\n",
    "#     Compute hybrid BM25 + cosine similarity scores for a query against a list of text chunks.\n",
    "\n",
    "#     Args:\n",
    "#         query (str): The search query (e.g., \"neural networks for mental health prediction\")\n",
    "#         chunks (list[str]): List of text chunks to score\n",
    "\n",
    "#     Returns:\n",
    "#         combined_scores (list[float]): Hybrid scores (0-1-ish) for each chunk\n",
    "#     \"\"\"\n",
    "\n",
    "#     # ---------------------------------------------------------------------\n",
    "#     # BM25 Lexical Scoring\n",
    "#     # ---------------------------------------------------------------------\n",
    "#     # Tokenize chunks into word lists (BM25 expects tokenized input).\n",
    "#     tokenized_chunks = [chunk.split() for chunk in chunks]\n",
    "\n",
    "#     # Initialize BM25 with tokenized corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-compute and Store Embeddings for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.2.1\n",
      "  Downloading torch-2.2.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting filelock (from torch==2.2.1)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch==2.2.1)\n",
      "  Using cached typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy (from torch==2.2.1)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.2.1)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch==2.2.1)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch==2.2.1)\n",
      "  Using cached fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.1)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.1)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.1)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.1)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.1)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.1)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.1)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.1)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.1)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.1)\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.1)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.2.1)\n",
      "  Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.2.1)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.2.1-cp312-cp312-manylinux1_x86_64.whl (755.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, typing-extensions, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch\n",
      "\u001b[2K  Attempting uninstall: mpmath\n",
      "\u001b[2K    Found existing installation: mpmath 1.3.0\n",
      "\u001b[2K    Uninstalling mpmath-1.3.0:\n",
      "\u001b[2K      Successfully uninstalled mpmath-1.3.0\n",
      "\u001b[2K  Attempting uninstall: typing-extensions━━━━━━━\u001b[0m \u001b[32m 0/21\u001b[0m [mpmath]\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.14.121\u001b[0m [mpmath]\n",
      "\u001b[2K    Uninstalling typing_extensions-4.14.1:━━\u001b[0m \u001b[32m 0/21\u001b[0m [mpmath]\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.14.1━━━━━━━━━━━\u001b[0m \u001b[32m 1/21\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: sympy━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/21\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: sympy 1.13.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/21\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling sympy-1.13.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/21\u001b[0m [sympy]ensions]\n",
      "\u001b[2K      Successfully uninstalled sympy-1.13.1━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/21\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvtx-cu12━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/21\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: nvidia-nvtx-cu12 12.4.127━━━━\u001b[0m \u001b[32m 2/21\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling nvidia-nvtx-cu12-12.4.127:━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/21\u001b[0m [nvidia-nvtx-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvtx-cu12-12.4.127━━━━━━\u001b[0m \u001b[32m 3/21\u001b[0m [nvidia-nvtx-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/21\u001b[0m [nvidia-nvtx-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.4.127━━━\u001b[0m \u001b[32m 4/21\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.4.127:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/21\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127━\u001b[0m \u001b[32m 4/21\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/21\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.21.5━━━━━━━━━━\u001b[0m \u001b[32m 5/21\u001b[0m [nvidia-nccl-cu12]]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.21.5:━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/21\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.21.5━━━━━━━━\u001b[0m \u001b[32m 5/21\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-curand-cu12━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/21\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.5.147━━━━\u001b[0m \u001b[32m 6/21\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.5.147:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/21\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.5.147━━\u001b[0m \u001b[32m 6/21\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/21\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.2.1.3━━━\u001b[0m \u001b[32m 6/21\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.2.1.3:━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/21\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3━━━━━\u001b[0m \u001b[32m 6/21\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/21\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127 \u001b[32m 7/21\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:━━━━━━━━━━━\u001b[0m \u001b[32m 7/21\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.1270m \u001b[32m 7/21\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.1270m \u001b[32m 8/21\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\u001b[0m \u001b[32m 8/21\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/21\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.4.1270m \u001b[32m 9/21\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/21\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\u001b[0m \u001b[32m 9/21\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/21\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.4.5.8━━━━━━\u001b[0m \u001b[32m11/21\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.4.5.8:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/21\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8━━━━━━━━\u001b[0m \u001b[32m11/21\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K  Attempting uninstall: networkx\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/21\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Found existing installation: networkx 3.3━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/21\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Uninstalling networkx-3.3:\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/21\u001b[0m [networkx]-cu12]\n",
      "\u001b[2K      Successfully uninstalled networkx-3.3m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/21\u001b[0m [networkx]\n",
      "\u001b[2K  Attempting uninstall: MarkupSafe\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/21\u001b[0m [networkx]\n",
      "\u001b[2K    Found existing installation: MarkupSafe 3.0.2━━━━━━━━━━━━━\u001b[0m \u001b[32m12/21\u001b[0m [networkx]\n",
      "\u001b[2K    Uninstalling MarkupSafe-3.0.2:m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/21\u001b[0m [networkx]\n",
      "\u001b[2K      Successfully uninstalled MarkupSafe-3.0.2━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/21\u001b[0m [networkx]\n",
      "\u001b[2K  Attempting uninstall: fsspec\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/21\u001b[0m [networkx]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.7.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/21\u001b[0m [networkx]\n",
      "\u001b[2K    Uninstalling fsspec-2025.7.0:━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m14/21\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.7.090m━━━━━━━━━━━━━\u001b[0m \u001b[32m14/21\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: filelock━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m14/21\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: filelock 3.18.0m━━━━━━━━━━━━━\u001b[0m \u001b[32m14/21\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling filelock-3.18.0:m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m14/21\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled filelock-3.18.090m━━━━━━━━━━━━━\u001b[0m \u001b[32m14/21\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m14/21\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.3.1.1700m \u001b[32m14/21\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.3.1.170:━━━━━━━━━━━━━\u001b[0m \u001b[32m14/21\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\u001b[0m \u001b[32m14/21\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m16/21\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.1.0.70━━━\u001b[0m \u001b[32m16/21\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m16/21\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70━━━━━\u001b[0m \u001b[32m16/21\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K  Attempting uninstall: jinja2━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m17/21\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Found existing installation: Jinja2 3.1.6╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m17/21\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Uninstalling Jinja2-3.1.6:━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m17/21\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K      Successfully uninstalled Jinja2-3.1.60m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m17/21\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu120m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m18/21\u001b[0m [jinja2]n-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.6.1.9m━━━\u001b[0m \u001b[32m19/21\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.6.1.9:0m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m19/21\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9━━\u001b[0m \u001b[32m19/21\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K  Attempting uninstall: torch━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m19/21\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Found existing installation: torch 2.6.0\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m19/21\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Uninstalling torch-2.6.0:━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m20/21\u001b[0m [torch]olver-cu12]\n",
      "\u001b[2K      Successfully uninstalled torch-2.6.0━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m20/21\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/21\u001b[0m [torch]m20/21\u001b[0m [torch]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-huggingface 0.1.2 requires tokenizers>=0.19.1, but you have tokenizers 0.15.2 which is incompatible.\n",
      "langchain-huggingface 0.1.2 requires transformers>=4.39.0, but you have transformers 4.38.2 which is incompatible.\n",
      "sentence-transformers 4.0.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.7.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 sympy-1.14.0 torch-2.2.1 typing-extensions-4.14.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Collecting transformers==4.39.3\n",
      "  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
      "Collecting filelock (from transformers==4.39.3)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.39.3)\n",
      "  Using cached huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy>=1.17 (from transformers==4.39.3)\n",
      "  Using cached numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting packaging>=20.0 (from transformers==4.39.3)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pyyaml>=5.1 (from transformers==4.39.3)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.39.3)\n",
      "  Using cached regex-2025.7.34-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers==4.39.3)\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.39.3)\n",
      "  Using cached tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.39.3)\n",
      "  Using cached safetensors-0.6.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers==4.39.3)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3)\n",
      "  Using cached fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3)\n",
      "  Using cached typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3)\n",
      "  Using cached hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (703 bytes)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers==4.39.3)\n",
      "  Using cached charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers==4.39.3)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers==4.39.3)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers==4.39.3)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\n",
      "Using cached hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Using cached numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
      "Using cached regex-2025.7.34-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n",
      "Using cached safetensors-0.6.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tqdm, safetensors, regex, pyyaml, packaging, numpy, idna, hf-xet, fsspec, filelock, charset_normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K  Attempting uninstall: urllib3\n",
      "\u001b[2K    Found existing installation: urllib3 2.5.0\n",
      "\u001b[2K    Uninstalling urllib3-2.5.0:\n",
      "\u001b[2K      Successfully uninstalled urllib3-2.5.0\n",
      "\u001b[2K  Attempting uninstall: typing-extensions━━━━━━━\u001b[0m \u001b[32m 0/18\u001b[0m [urllib3]\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.14.118\u001b[0m [urllib3]\n",
      "\u001b[2K    Uninstalling typing_extensions-4.14.1:━━\u001b[0m \u001b[32m 0/18\u001b[0m [urllib3]\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.14.1━━━━━━━━━━━\u001b[0m \u001b[32m 1/18\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: tqdmm━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/18\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: tqdm 4.67.1━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/18\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling tqdm-4.67.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/18\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled tqdm-4.67.1━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/18\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: safetensors━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/18\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: safetensors 0.6.1━━━━━━━━━━━━\u001b[0m \u001b[32m 1/18\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling safetensors-0.6.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/18\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled safetensors-0.6.1━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/18\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: regex━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/18\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: regex 2025.7.34━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/18\u001b[0m [regex]ensions]\n",
      "\u001b[2K    Uninstalling regex-2025.7.34:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/18\u001b[0m [regex]\n",
      "\u001b[2K      Successfully uninstalled regex-2025.7.34━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/18\u001b[0m [regex]\n",
      "\u001b[2K  Attempting uninstall: pyyamlm━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/18\u001b[0m [regex]\n",
      "\u001b[2K    Found existing installation: PyYAML 6.0.2━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/18\u001b[0m [regex]\n",
      "\u001b[2K    Uninstalling PyYAML-6.0.2:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/18\u001b[0m [regex]\n",
      "\u001b[2K      Successfully uninstalled PyYAML-6.0.2━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/18\u001b[0m [regex]\n",
      "\u001b[2K  Attempting uninstall: packaging\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/18\u001b[0m [pyyaml]\n",
      "\u001b[2K    Found existing installation: packaging 25.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/18\u001b[0m [pyyaml]\n",
      "\u001b[2K    Uninstalling packaging-25.0:0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/18\u001b[0m [pyyaml]\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/18\u001b[0m [pyyaml]\n",
      "\u001b[2K  Attempting uninstall: numpy\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/18\u001b[0m [pyyaml]\n",
      "\u001b[2K    Found existing installation: numpy 2.3.2━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/18\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling numpy-2.3.2:\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/18\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.3.2━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/18\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: idna[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/18\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: idna 3.10━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/18\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling idna-3.10:m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/18\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled idna-3.10[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/18\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: hf-xet[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/18\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: hf-xet 1.1.7━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/18\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling hf-xet-1.1.7:╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/18\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled hf-xet-1.1.7━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/18\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: fsspec╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/18\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.7.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/18\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling fsspec-2025.7.0:0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/18\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.7.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/18\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: filelock0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/18\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: filelock 3.18.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/18\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling filelock-3.18.0:0m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/18\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled filelock-3.18.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/18\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: charset_normalizer90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/18\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: charset-normalizer 3.4.2━━━━━\u001b[0m \u001b[32m10/18\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling charset-normalizer-3.4.2:90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/18\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled charset-normalizer-3.4.2━━━━━━━━━━━\u001b[0m \u001b[32m12/18\u001b[0m [charset_normalizer]\n",
      "\u001b[2K  Attempting uninstall: certifi[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m12/18\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Found existing installation: certifi 2025.8.3━━━━━━━━━━━━━\u001b[0m \u001b[32m12/18\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Uninstalling certifi-2025.8.3:\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m12/18\u001b[0m [charset_normalizer]\n",
      "\u001b[2K      Successfully uninstalled certifi-2025.8.30m━━━━━━━━━━━━━\u001b[0m \u001b[32m12/18\u001b[0m [charset_normalizer]\n",
      "\u001b[2K  Attempting uninstall: requests0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m12/18\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Found existing installation: requests 2.32.4m━━━━━━━━━━━━━\u001b[0m \u001b[32m12/18\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Uninstalling requests-2.32.4:m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m12/18\u001b[0m [charset_normalizer]\n",
      "\u001b[2K      Successfully uninstalled requests-2.32.490m━━━━━━━━━━━━━\u001b[0m \u001b[32m12/18\u001b[0m [charset_normalizer]\n",
      "\u001b[2K  Attempting uninstall: huggingface-hub\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m14/18\u001b[0m [requests]lizer]\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.34.3━━━━━━━\u001b[0m \u001b[32m14/18\u001b[0m [requests]\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.34.3:[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m14/18\u001b[0m [requests]\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.34.3m━━━━━━━━\u001b[0m \u001b[32m14/18\u001b[0m [requests]\n",
      "\u001b[2K  Attempting uninstall: tokenizers━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m15/18\u001b[0m [huggingface-hub]\n",
      "\u001b[2K    Found existing installation: tokenizers 0.15.2m\u001b[90m━━━━━━\u001b[0m \u001b[32m15/18\u001b[0m [huggingface-hub]\n",
      "\u001b[2K    Uninstalling tokenizers-0.15.2:━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m16/18\u001b[0m [tokenizers]]\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.15.2╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m16/18\u001b[0m [tokenizers]\n",
      "\u001b[2K  Attempting uninstall: transformers━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m16/18\u001b[0m [tokenizers]\n",
      "\u001b[2K    Found existing installation: transformers 4.38.21m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m17/18\u001b[0m [transformers]\n",
      "\u001b[2K    Uninstalling transformers-4.38.2:━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m17/18\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.38.2╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m17/18\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/18\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-huggingface 0.1.2 requires tokenizers>=0.19.1, but you have tokenizers 0.15.2 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
      "sentence-transformers 4.0.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.39.3 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed certifi-2025.8.3 charset_normalizer-3.4.2 filelock-3.18.0 fsspec-2025.7.0 hf-xet-1.1.7 huggingface-hub-0.34.3 idna-3.10 numpy-2.3.2 packaging-25.0 pyyaml-6.0.2 regex-2025.7.34 requests-2.32.4 safetensors-0.6.1 tokenizers-0.15.2 tqdm-4.67.1 transformers-4.39.3 typing-extensions-4.14.1 urllib3-2.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.2.1 --force-reinstall\n",
    "!pip install transformers==4.39.3 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# 1. Initialize embedding model (same as before, but wrapped for LangChain)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Store outside repo to avoid committing\n",
    "persist_dir = \"/workspaces/chroma_storage/chroma_embeddings\"\n",
    "os.makedirs(persist_dir, exist_ok=True)\n",
    "\n",
    "# 3. Prepare documents (combine chunk + metadata)\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "docs = [\n",
    "    Document(page_content=chunk, metadata={\"paper_id\": paper_id})\n",
    "    for chunk, paper_id in zip(all_chunks, chunk_to_paper)\n",
    "]\n",
    "\n",
    "# 4. Create vectorstore and persist\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding_model,\n",
    "    collection_name=\"research_chunks\",\n",
    "    persist_directory=persist_dir\n",
    ")\n",
    "\n",
    "# Confirm\n",
    "import os\n",
    "print(os.listdir(\"./chroma_embeddings\"))\n",
    "print(\"Embeddings precomputed and stored in ChromaDB.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic query to test retrieval from ChromaDB\n",
    "\n",
    "# Reload vectorstore (no need to re-embed)\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"research_chunks\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=persist_dir\n",
    ")\n",
    "\n",
    "priority_topic = \"mental health\"\n",
    "\n",
    "query = priority_topic  # or any dynamic query\n",
    "results = vectorstore.similarity_search(query, k=5)\n",
    "\n",
    "# Display results\n",
    "for r in results:\n",
    "    print(f\"{r.metadata['paper_id']}:\\n{r.page_content[:250]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZG1Ah1eDi7aK"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Step 1: Topic Extraction - [3 Marks]**\n",
    "\n",
    "> **Read the NOFO doc and identify the topic for which the funding is to be given.**\n",
    "---\n",
    "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt and 1 mark for the successful completion of this section, including debugging or modifying the code if necessary.*\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkW_lO_CHSpc"
   },
   "source": [
    "**TASK:** Write an LLM prompt to extract the Topic for what the funding is been provided, from the NOFO document, Ask the LLM to respond back with the topic name only and nothing else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DcEpaUYNM_W"
   },
   "outputs": [],
   "source": [
    "# Topic extraction prompt\n",
    "topic_extraction_prompt = f\"\"\"\n",
    "You are a research grant specialist with expertise in analyzing NIH funding announcements and extracting key research priorities.\n",
    "\n",
    "Your task: Analyze this NOFO document from the National Institute of Mental Health (NIMH) to identify the PRIMARY funding topic.\n",
    "\n",
    "The document may describe multiple research areas, objectives, and priorities. Extract the single overarching topic that encompasses the main focus of this funding opportunity.\n",
    "\n",
    "Return ONLY the primary topic in 3-8 words. No explanations, descriptions, or additional text.\n",
    "\n",
    "Document:\n",
    "{NOFO_pdf[0].page_content}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "WtmCEbaKN9aW",
    "outputId": "7f9d4f38-5e38-4c9c-f70a-3976dfb8c881"
   },
   "outputs": [],
   "source": [
    "# Finding the topic for which the Funding is been given\n",
    "topic_extraction = llm.invoke(topic_extraction_prompt)\n",
    "topic = topic_extraction.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXHRa9IlMycZ"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Step 2: Research Paper Relevance Assessment - [3 Marks]**\n",
    "> **Analyze all the Research Papers and filter out the research papers based on the topic of NOFO**\n",
    "---\n",
    "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt and 1 mark for the successful completion of this section, including debugging or modifying the code if necessary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kWc0LaCGPo3"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "**TASK:** Write an Prompt which can be used to analyze the relevance of the provided research paper in relation to the topic outlined in the NOFO (Notice of Funding Opportunity) document. Determine whether the research aligns with the goals, objectives, and funding criteria specified in the NOFO. Additionally, assess whether the research paper can be used to support or develop a viable project idea that fits within the scope of the funding opportunity.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Note:** If the paper does **not** significantly relate to the topic—by domain, method, theory, or application ask the LLM to return: **\"PAPER NOT RELATED TO TOPIC\"**\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Ask the LLM to respond in the below specified structure:\n",
    "\n",
    "```\n",
    "### Output Format:\n",
    "\"summary\": \"<summary of the paper under 300 words, or return: PAPER NOT RELATED TO TOPIC>\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3LPwoNeyXC7"
   },
   "outputs": [],
   "source": [
    "relevance_prompt_a = f\"\"\"\n",
    "You are a research grant specialist evaluating research papers for relevance to NIH NOFO objectives: {topic}.\n",
    "\n",
    "Evaluate the paper step-by-step against these criteria:\n",
    "1. Domain relevance (mental health, digital health, intervention effectiveness)\n",
    "2. Methodological alignment (clinical trials, user engagement studies, technology development)\n",
    "3. Theoretical connection (frameworks, evidence, insights for intervention design/implementation)\n",
    "4. Practical application (supports development or testing of digital mental health interventions)\n",
    "\n",
    "Instructions:\n",
    "- For EACH criterion, respond YES or NO and justify briefly.\n",
    "- A paper is RELEVANT if at least ONE criterion is YES.\n",
    "- Assign a confidence score (0–100%) to the RELEVANT decision, based on how strongly the paper meets the criteria (higher = more confident relevance).\n",
    "- If RELEVANT: provide a <300-word summary focused on digital mental health intervention insights.\n",
    "- If NOT RELEVANT: return exactly \"PAPER NOT RELATED TO TOPIC\".\n",
    "\n",
    "Output format (JSON):\n",
    "{{\n",
    "  \"criteria_results\": {{\n",
    "    \"domain_relevance\": \"YES/NO - justification\",\n",
    "    \"methodological_alignment\": \"YES/NO - justification\",\n",
    "    \"theoretical_connection\": \"YES/NO - justification\",\n",
    "    \"practical_application\": \"YES/NO - justification\"\n",
    "  }},\n",
    "  \"decision\": \"RELEVANT\" or \"PAPER NOT RELATED TO TOPIC\",\n",
    "  \"confidence\": \"<integer between 0 and 100>\",\n",
    "  \"summary\": \"<summary text or null>\"\n",
    "}}\n",
    "\n",
    "### Paper content:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# FEW-SHOT RETRIEVAL FUNCTION (needed for few_shot_examples)\n",
    "# ------------------------------------------------------------\n",
    "def get_few_shot_examples(\n",
    "    json_path,\n",
    "    max_examples=4,                 # total examples to include\n",
    "    min_confidence=70               # minimum confidence threshold\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieve few-shot examples for prompt building:\n",
    "    - Pulls from prior log entries if available\n",
    "    - Falls back to hardcoded seed examples if log is empty\n",
    "\n",
    "    Why this matters:\n",
    "    - Few-shot examples improve LLM reasoning by showing \"good answers\"\n",
    "    - Ensures consistency in relevance classification over multiple runs\n",
    "    \"\"\"\n",
    "\n",
    "    import os, json, random\n",
    "\n",
    "    examples = []\n",
    "\n",
    "    # -------------------------------\n",
    "    # 1. Attempt to load from log\n",
    "    # -------------------------------\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                data = []\n",
    "\n",
    "        relevant_examples, irrelevant_examples = [], []\n",
    "\n",
    "        # Loop through prior iterations\n",
    "        for iteration in data:\n",
    "            # Pull relevant docs with sufficient confidence\n",
    "            for doc in iteration.get(\"relevant_documents\", []):\n",
    "                hybrid_conf = max(doc.get(\"model_confidence\", 0), doc.get(\"rule_confidence\", 0))\n",
    "                if hybrid_conf >= min_confidence:\n",
    "                    relevant_examples.append((doc[\"title\"], doc[\"reasoning\"]))\n",
    "\n",
    "            # Irrelevant docs (fallback reasoning text)\n",
    "            for doc in iteration.get(\"irrelevant_documents\", []):\n",
    "                irrelevant_examples.append((doc, \"PAPER NOT RELATED TO TOPIC\"))\n",
    "\n",
    "        # Randomly select balanced examples\n",
    "        half = max_examples // 2\n",
    "        random.shuffle(relevant_examples)\n",
    "        random.shuffle(irrelevant_examples)\n",
    "        selected_relevant = relevant_examples[:half]\n",
    "        selected_irrelevant = irrelevant_examples[:half]\n",
    "        examples = selected_relevant + selected_irrelevant\n",
    "\n",
    "    # -------------------------------\n",
    "    # 2. Fallback if log is empty\n",
    "    # -------------------------------\n",
    "    if not examples:\n",
    "        print(\"No high-confidence examples found. Using fallback seed examples.\")\n",
    "        examples = [\n",
    "            (\n",
    "                \"Digital CBT for Adolescents\",\n",
    "                \"\"\"{\n",
    "  \"criteria_results\": {\n",
    "    \"domain_relevance\": \"YES - focuses on mental health digital interventions\",\n",
    "    \"methodological_alignment\": \"YES - randomized controlled trial design\",\n",
    "    \"theoretical_connection\": \"NO - lacks explicit framework reference\",\n",
    "    \"practical_application\": \"YES - informs deployment in youth settings\"\n",
    "  },\n",
    "  \"decision\": \"RELEVANT\",\n",
    "  \"confidence\": \"85\",\n",
    "  \"summary\": \"This study evaluates a mobile CBT app for adolescents, showing significant reduction in anxiety and depression symptoms compared to control.\"\n",
    "}\"\"\"\n",
    "            ),\n",
    "            (\n",
    "                \"Oncology Drug Delivery Review\",\n",
    "                \"\"\"{\n",
    "  \"criteria_results\": {\n",
    "    \"domain_relevance\": \"NO - focuses on oncology drug mechanisms\",\n",
    "    \"methodological_alignment\": \"NO\",\n",
    "    \"theoretical_connection\": \"NO\",\n",
    "    \"practical_application\": \"NO\"\n",
    "  },\n",
    "  \"decision\": \"PAPER NOT RELATED TO TOPIC\",\n",
    "  \"confidence\": \"0\",\n",
    "  \"summary\": null\n",
    "}\"\"\"\n",
    "            )\n",
    "        ][:max_examples]\n",
    "\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# FUNCTION: build_prompt_with_examples\n",
    "# ------------------------------------------------------------\n",
    "def build_prompt_with_examples(topic, base_prompt, examples):\n",
    "    \"\"\"\n",
    "    Build a few-shot prompt for relevance classification.\n",
    "\n",
    "    How it works:\n",
    "    - Prepends example evaluations (few-shot) before the actual task prompt\n",
    "    - Provides LLM with context: \"Here is how similar papers were judged\"\n",
    "    - Ensures consistent reasoning across multiple runs\n",
    "\n",
    "    Args:\n",
    "        topic (str): The NOFO topic (priority research area)\n",
    "        base_prompt (str): The evaluation instructions prompt\n",
    "        examples (list of tuples): Few-shot examples [(title, reasoning), ...]\n",
    "\n",
    "    Returns:\n",
    "        str: Full prompt with examples + evaluation instructions\n",
    "    \"\"\"\n",
    "\n",
    "    # Format few-shot examples (each example = title + reasoning text)\n",
    "    examples_str = \"\\n\\n\".join(\n",
    "        [f\"Example ({title}):\\n{reasoning}\" for title, reasoning in examples]\n",
    "    )\n",
    "\n",
    "    # Assemble final prompt\n",
    "    prompt = f\"\"\"\n",
    "You are a research grant specialist evaluating research papers for relevance to NIH NOFO objectives: {topic}.\n",
    "\n",
    "Below are examples of prior evaluations for context:\n",
    "{examples_str}\n",
    "\n",
    "Now evaluate the following paper using the same structure and logic:\n",
    "\n",
    "{base_prompt}\n",
    "\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Few-shot setup ---\n",
    "# Retrieve few-shot examples (previous evaluations) and build prompt prefix\n",
    "few_shot_examples = get_few_shot_examples(LOG_PATH)\n",
    "prompt_with_examples = build_prompt_with_examples(topic, relevance_prompt_a, few_shot_examples)\n",
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import tiktoken\n",
    "from datetime import datetime\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import time  # NEW: for batch delay control\n",
    "import numpy as np  # NEW: for percentile calculation\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CONFIGURATION SECTION\n",
    "# ------------------------------------------------------------\n",
    "TEST_MODE = True                  # If True, process only subset of data (not used in Chroma flow)\n",
    "DISCREPANCY_THRESHOLD = 20        # Threshold for flagging model vs rule confidence mismatch\n",
    "\n",
    "# Toggle modes\n",
    "FAST_MODE = True                  # True = embedding-prioritized retrieval (top-K); False = evaluate all papers\n",
    "TOP_K_PAPERS = 50                 # Target number of papers to evaluate after pre-filtering\n",
    "\n",
    "# NEW CONFIG: Overfetch factor for Phase 1\n",
    "# ------------------------------------------------------------\n",
    "# Retrieve MORE chunks than TOP_K_PAPERS to ensure enough unique papers after aggregation.\n",
    "# Example: 50 papers * factor 3 = 150 chunks\n",
    "CHUNK_OVERFETCH_FACTOR = 3\n",
    "\n",
    "# NEW CONFIG: Chunk cap and long-paper handling\n",
    "CHUNK_CAP_PER_PAPER = 10          # Cap top-N chunks per paper (by similarity) to reduce bias & token load\n",
    "LONG_PAPER_THRESHOLD = 30         # If a paper has >30 chunks, summarize chunks before aggregation\n",
    "TOKEN_LIMIT_BEFORE_SUMMARY = 100000  # If estimated tokens exceed this, auto-summarize\n",
    "\n",
    "# Batch processing settings\n",
    "BATCH_SIZE = 10                   # Process papers in groups of 10\n",
    "BATCH_DELAY = 3                   # Delay between batches (seconds) to avoid rate-limit errors\n",
    "\n",
    "# Token cost estimation (OpenAI GPT-4o-mini pricing as of Aug 2025)\n",
    "INPUT_COST_PER_1K = 0.00015       # $ per 1K input tokens\n",
    "OUTPUT_COST_PER_1K = 0.0006       # $ per 1K output tokens\n",
    "\n",
    "# Initialize cost tracking variables\n",
    "total_input_tokens = 0            # Cumulative input tokens sent to API\n",
    "total_output_tokens = 0           # Approximate output tokens (assume 10% of input)\n",
    "total_cost_usd = 0.0              # Running cost estimate\n",
    "\n",
    "prior_classification = {\n",
    "    \"relevant\": [],\n",
    "    \"irrelevant\": [],\n",
    "    \"unknown\": []\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# LOGGING FUNCTION (unchanged)\n",
    "# ------------------------------------------------------------\n",
    "def log_prompt_iteration(\n",
    "    json_path,\n",
    "    prompt,\n",
    "    relevant_docs_with_reasoning,\n",
    "    irrelevant_docs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Appends classification results to master JSON log for auditability and trend tracking.\n",
    "    \"\"\"\n",
    "    iteration_id = len(json.load(open(json_path))) + 1 if os.path.exists(json_path) else 1\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    entry = {\n",
    "        \"iteration_id\": iteration_id,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"prompt\": prompt,\n",
    "        \"relevant_documents\": relevant_docs_with_reasoning,\n",
    "        \"irrelevant_documents\": irrelevant_docs\n",
    "    }\n",
    "\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                data = []\n",
    "    else:\n",
    "        data = []\n",
    "\n",
    "    data.append(entry)\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Logged iteration {iteration_id} to {json_path}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SELF-CHECK FUNCTION (unchanged)\n",
    "# ------------------------------------------------------------\n",
    "def verify_decision(llm, reasoning_output):\n",
    "    \"\"\"\n",
    "    Performs secondary pass: verifies relevance decision from reasoning text.\n",
    "    Only returns YES/NO (binary) to catch contradictory outputs.\n",
    "    \"\"\"\n",
    "    verification_prompt = f\"\"\"\n",
    "You are verifying the relevance decision based on the following evaluation:\n",
    "\n",
    "{reasoning_output}\n",
    "\n",
    "Only answer with 'YES' if the decision should be considered relevant, or 'NO' if not relevant.\n",
    "    \"\"\"\n",
    "    verification_response = llm.invoke(verification_prompt)\n",
    "    return \"YES\" in verification_response.content.upper()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# RULE-DERIVED CONFIDENCE FUNCTION (unchanged)\n",
    "# ------------------------------------------------------------\n",
    "def calculate_rule_confidence(criteria_results):\n",
    "    \"\"\"\n",
    "    Calculates deterministic confidence (0-95%) based on count of YES answers\n",
    "    in criteria evaluations (domain, methods, theory, application).\n",
    "    \"\"\"\n",
    "    yes_count = sum(1 for v in criteria_results.values() if v.upper().startswith(\"YES\"))\n",
    "    if yes_count == 0:\n",
    "        return 0\n",
    "    elif yes_count == 1:\n",
    "        return 50\n",
    "    elif yes_count == 2:\n",
    "        return 70\n",
    "    elif yes_count == 3:\n",
    "        return 85\n",
    "    else:\n",
    "        return 95\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CHROMADB INTEGRATION\n",
    "# ------------------------------------------------------------\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Load same embedding model used for precomputing\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "persist_dir = \"./chroma_embeddings\"\n",
    "\n",
    "# Connect to persisted Chroma vectorstore (contains chunks + metadata)\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"research_chunks\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=persist_dir\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 1: PAPER-LEVEL PRE-FILTER (OVERFETCH + RANKING)\n",
    "# ============================================================\n",
    "\n",
    "print(f\"[PHASE 1] Overfetching chunks for paper-level scoring (factor={CHUNK_OVERFETCH_FACTOR})\")\n",
    "\n",
    "# Step 1: Retrieve top-N chunks by similarity (overfetch)\n",
    "overfetch_k = TOP_K_PAPERS * CHUNK_OVERFETCH_FACTOR\n",
    "retrieved_chunks = vectorstore.similarity_search_with_score(topic, k=overfetch_k)  # returns (doc, score)\n",
    "\n",
    "# Step 2: Aggregate scores per paper_id\n",
    "paper_scores = {}         # {paper_id: [scores]}\n",
    "for doc, score in retrieved_chunks:\n",
    "    pid = doc.metadata.get(\"paper_id\", \"Unknown_Paper\")\n",
    "    if pid not in paper_scores:\n",
    "        paper_scores[pid] = []\n",
    "    paper_scores[pid].append(score)\n",
    "\n",
    "# Step 3: Rank papers by **max score** (primary) and print **average** for debugging\n",
    "ranked_papers = sorted(\n",
    "    paper_scores.items(),\n",
    "    key=lambda x: max(x[1]),  # still using max for ranking\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# Step 4: Select top-K unique papers\n",
    "top_paper_ids = [pid for pid, _ in ranked_papers[:TOP_K_PAPERS]]\n",
    "\n",
    "# Debug: Print ranking summary\n",
    "print(f\"Selected top {len(top_paper_ids)} papers for Phase 2 evaluation:\")\n",
    "for pid in top_paper_ids:\n",
    "    print(f\" - {pid}: max={max(paper_scores[pid]):.2f}, avg={sum(paper_scores[pid])/len(paper_scores[pid]):.2f}, chunks={len(paper_scores[pid])}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# NEW: Percentile Statistics Printout for Debugging\n",
    "# ------------------------------------------------------------\n",
    "scores_max = [max(scores) for scores in paper_scores.values()]\n",
    "if scores_max:\n",
    "    percentiles = [25, 50, 75, 90]\n",
    "    print(\"\\n[PHASE 1] Score Percentile Summary (max chunk scores per paper):\")\n",
    "    for p in percentiles:\n",
    "        val = np.percentile(scores_max, p)\n",
    "        print(f\"  {p}th percentile: {val:.2f}\")\n",
    "    print(f\"  Min score: {min(scores_max):.2f}\")\n",
    "    print(f\"  Max score: {max(scores_max):.2f}\")\n",
    "else:\n",
    "    print(\"[PHASE 1] No scores available to compute percentiles.\")\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 2: FULL-CHUNK RETRIEVAL WITH CAP + EARLY SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print(\"[PHASE 2] Retrieving all chunks (bulk) and filtering for top papers...\")\n",
    "\n",
    "# Retrieve all chunks at once (faster than per-paper search)\n",
    "all_chunks = vectorstore.similarity_search_with_score(topic, k=9999)\n",
    "\n",
    "# Organize chunks by paper_id and sort by score (so we can cap top-N)\n",
    "paper_chunk_data = {}  # {paper_id: [(score, text), ...]}\n",
    "\n",
    "for doc, score in all_chunks:\n",
    "    pid = doc.metadata.get(\"paper_id\", \"Unknown_Paper\")\n",
    "    if pid in top_paper_ids:\n",
    "        paper_chunk_data.setdefault(pid, []).append((score, doc.page_content))\n",
    "\n",
    "# Build aggregated papers with **chunk cap** and **early summary for long papers**\n",
    "paper_chunks = {}\n",
    "paper_chunk_counts = {}\n",
    "\n",
    "for pid, chunks in paper_chunk_data.items():\n",
    "    # Sort chunks by descending similarity score (keep top most relevant)\n",
    "    sorted_chunks = sorted(chunks, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Cap to top-N chunks (prevents extremely long papers from dominating)\n",
    "    capped_chunks = sorted_chunks[:CHUNK_CAP_PER_PAPER]\n",
    "\n",
    "    # Count total chunks for debug\n",
    "    paper_chunk_counts[pid] = len(sorted_chunks)\n",
    "\n",
    "    # If paper has > LONG_PAPER_THRESHOLD chunks → summarize early\n",
    "    if len(sorted_chunks) > LONG_PAPER_THRESHOLD:\n",
    "        print(f\"[EARLY SUMMARY] Paper '{pid}' exceeds {LONG_PAPER_THRESHOLD} chunks → summarizing chunks before aggregation.\")\n",
    "        # Summarize each chunk first, then combine summaries\n",
    "        chunk_summaries = []\n",
    "        for _, chunk_text in capped_chunks:\n",
    "            summary = summarize_text(chunk_text)  # reuse summarization helper\n",
    "            chunk_summaries.append(summary)\n",
    "        combined_summary = \"\\n\".join(chunk_summaries)\n",
    "        paper_chunks[pid] = combined_summary\n",
    "    else:\n",
    "        # Normal case: concatenate capped chunks directly\n",
    "        combined_text = \"\\n\".join([text for _, text in capped_chunks])\n",
    "        paper_chunks[pid] = combined_text\n",
    "\n",
    "# Final aggregated papers for downstream evaluation\n",
    "aggregated_papers = list(paper_chunks.items())\n",
    "print(f\"[PHASE 2] Aggregated {len(aggregated_papers)} papers for full relevance evaluation.\")\n",
    "\n",
    "# Debug print: show capped chunk count (original vs capped)\n",
    "print(\"[PHASE 2] Chunk count per selected paper (post-aggregation with cap):\")\n",
    "for pid, total_chunks in paper_chunk_counts.items():\n",
    "    print(f\" - {pid}: {min(total_chunks, CHUNK_CAP_PER_PAPER)} chunks used (original {total_chunks})\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# NEW: Automatic Warning for Overrepresented Papers\n",
    "# ------------------------------------------------------------\n",
    "# This block calculates the percentage of total chunks contributed by each paper\n",
    "# (AFTER capping) and issues a warning if any paper contributes more than X% of total chunks.\n",
    "# This helps detect potential bias where one very long paper dominates context.\n",
    "\n",
    "WARNING_THRESHOLD_PERCENT = 25  # e.g., warn if >25% of chunks come from a single paper\n",
    "\n",
    "# Total chunks after capping (sum of min(total, cap))\n",
    "total_chunks_used = sum(min(count, CHUNK_CAP_PER_PAPER) for count in paper_chunk_counts.values())\n",
    "\n",
    "for pid, count in paper_chunk_counts.items():\n",
    "    used_chunks = min(count, CHUNK_CAP_PER_PAPER)\n",
    "    percent = (used_chunks / total_chunks_used) * 100 if total_chunks_used > 0 else 0\n",
    "    if percent > WARNING_THRESHOLD_PERCENT:\n",
    "        print(f\"*** WARNING: Paper '{pid}' contributes {percent:.1f}% of total chunks \"\n",
    "              f\"({used_chunks}/{total_chunks_used}) → may indicate overrepresentation. ***\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TOKENIZER SETUP\n",
    "# ------------------------------------------------------------\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "MAX_TOKENS = 300000  # Safety ceiling for prompt+context\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SUMMARIZATION HELPER\n",
    "# ------------------------------------------------------------\n",
    "def summarize_text(paper_text):\n",
    "    \"\"\"\n",
    "    Summarizes full paper to ~300 words focusing on digital mental health interventions.\n",
    "    Reduces token load while preserving conceptual content for classification.\n",
    "    \"\"\"\n",
    "    summary_prompt = f\"\"\"\n",
    "    Summarize the following research paper into ~300 words, focusing on\n",
    "    digital mental health interventions, methods, and outcomes:\n",
    "\n",
    "    {paper_text}\n",
    "    \"\"\"\n",
    "    summary_response = llm.invoke(summary_prompt)\n",
    "    return summary_response.content.strip()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# MAIN LOOP: BATCH PROCESSING + COST TRACKING\n",
    "# ------------------------------------------------------------\n",
    "documents = []\n",
    "irrelevant_docs_list = []\n",
    "progress_cnt = 1\n",
    "relevant_papers_count = 0\n",
    "irrelevant_papers_count = 0\n",
    "total_files = len(aggregated_papers)\n",
    "\n",
    "for batch_start in range(0, total_files, BATCH_SIZE):\n",
    "    batch = aggregated_papers[batch_start: batch_start + BATCH_SIZE]\n",
    "    print(f\"\\nProcessing batch {batch_start//BATCH_SIZE + 1} \"\n",
    "          f\"({len(batch)} papers) out of {total_files} total papers...\")\n",
    "\n",
    "    for paper_id, paper_text in batch:\n",
    "        try:\n",
    "            # --- Dynamic Token Budget Check ---\n",
    "            # Estimate token count BEFORE building full prompt\n",
    "            token_estimate = len(encoding.encode(paper_text))\n",
    "            if token_estimate > TOKEN_LIMIT_BEFORE_SUMMARY:\n",
    "                print(f\"[TOKEN GUARD] Paper '{paper_id}' estimated {token_estimate} tokens → auto-summarizing.\")\n",
    "                paper_text = summarize_text(paper_text)  # Summarize entire paper\n",
    "\n",
    "            # --- Summarize paper (fallback for smaller papers) ---\n",
    "            summarized_text = summarize_text(paper_text)\n",
    "\n",
    "            # --- Build relevance prompt ---\n",
    "            available_tokens = MAX_TOKENS - len(encoding.encode(prompt_with_examples))\n",
    "            truncated_text = encoding.decode(encoding.encode(summarized_text)[:available_tokens])\n",
    "            full_prompt = prompt_with_examples + truncated_text\n",
    "\n",
    "            # --- Token count + cost estimation ---\n",
    "            token_count = len(encoding.encode(full_prompt))\n",
    "            total_input_tokens += token_count\n",
    "            total_output_tokens += int(token_count * 0.1)  # estimate 10% output\n",
    "            total_cost_usd = (\n",
    "                (total_input_tokens / 1000) * INPUT_COST_PER_1K +\n",
    "                (total_output_tokens / 1000) * OUTPUT_COST_PER_1K\n",
    "            )\n",
    "            print(f\"[Token Count] {paper_id}: {token_count} tokens \"\n",
    "                  f\"(Estimated running cost: ${total_cost_usd:.4f})\")\n",
    "\n",
    "            # --- LLM relevance classification ---\n",
    "            response = llm.invoke(full_prompt)\n",
    "            print(f\"Successfully processed paper {progress_cnt}/{total_files} ({paper_id})\")\n",
    "            progress_cnt += 1\n",
    "\n",
    "            # --- Self-check verification ---\n",
    "            is_relevant = verify_decision(llm, response.content)\n",
    "\n",
    "            if not is_relevant or \"PAPER NOT RELATED TO TOPIC\" in response.content:\n",
    "                irrelevant_papers_count += 1\n",
    "                irrelevant_docs_list.append(paper_id)\n",
    "                continue\n",
    "\n",
    "            # --- Parse LLM JSON output ---\n",
    "            try:\n",
    "                parsed_json = json.loads(response.content)\n",
    "            except json.JSONDecodeError:\n",
    "                json_match = re.search(r\"\\{.*\\}\", response.content, re.DOTALL)\n",
    "                parsed_json = json.loads(json_match.group(0)) if json_match else {}\n",
    "\n",
    "            # --- Confidence scoring (model + rule) ---\n",
    "            model_confidence = int(parsed_json.get(\"confidence\", 0)) if parsed_json else None\n",
    "            rule_confidence = calculate_rule_confidence(parsed_json[\"criteria_results\"]) \\\n",
    "                if \"criteria_results\" in parsed_json else 0\n",
    "            discrepancy = abs(model_confidence - rule_confidence) if model_confidence else None\n",
    "            flagged = discrepancy > DISCREPANCY_THRESHOLD if discrepancy is not None else False\n",
    "\n",
    "            # --- Store result ---\n",
    "            documents.append({\n",
    "                'title': paper_id,\n",
    "                'file_path': \"(from ChromaDB)\",\n",
    "                'llm_reasoning': response.content,\n",
    "                'model_confidence': model_confidence,\n",
    "                'rule_confidence': rule_confidence,\n",
    "                'confidence_discrepancy': discrepancy,\n",
    "                'flagged_for_review': flagged\n",
    "            })\n",
    "            relevant_papers_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!!! Error processing {paper_id}: {str(e)}\")\n",
    "\n",
    "    # --- Delay between batches ---\n",
    "    print(f\"Batch {batch_start//BATCH_SIZE + 1} complete. Sleeping {BATCH_DELAY} seconds...\")\n",
    "    time.sleep(BATCH_DELAY)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SUMMARY OUTPUT\n",
    "# ------------------------------------------------------------\n",
    "print(\"=\" * 50)\n",
    "print(f\"Relevant Papers: {relevant_papers_count}/{total_files}\")\n",
    "print(f\"Irrelevant Papers: {irrelevant_papers_count}/{total_files}\")\n",
    "print(f\"Estimated Total Input Tokens: {total_input_tokens}\")\n",
    "print(f\"Estimated Total Output Tokens: {total_output_tokens}\")\n",
    "print(f\"Estimated Total Cost: ${total_cost_usd:.4f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nList of relevant papers:\")\n",
    "for doc in documents:\n",
    "    print(f\"\\nTitle: {doc['title']}\")\n",
    "    print(f\"Model Confidence: {doc['model_confidence']}\")\n",
    "    print(f\"Rule Confidence: {doc['rule_confidence']}\")\n",
    "    print(f\"Discrepancy: {doc['confidence_discrepancy']} (Flagged: {doc['flagged_for_review']})\")\n",
    "    print(f\"Reasoning (truncated): {doc['llm_reasoning'][:500]}...\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# LOGGING\n",
    "# ------------------------------------------------------------\n",
    "relevant_docs_with_reasoning = [\n",
    "    {\n",
    "        \"title\": doc['title'],\n",
    "        \"reasoning\": doc['llm_reasoning'],\n",
    "        \"model_confidence\": doc['model_confidence'],\n",
    "        \"rule_confidence\": doc['rule_confidence'],\n",
    "        \"confidence_discrepancy\": doc['confidence_discrepancy'],\n",
    "        \"flagged_for_review\": doc['flagged_for_review']\n",
    "    }\n",
    "    for doc in documents\n",
    "]\n",
    "\n",
    "log_prompt_iteration(\n",
    "    json_path=\"prompt_evaluation_log_cleaned.json\",\n",
    "    prompt=prompt_with_examples,\n",
    "    relevant_docs_with_reasoning=relevant_docs_with_reasoning,\n",
    "    irrelevant_docs=irrelevant_docs_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNdBg6Iei7VJ"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Step 3: Proposal Ideation Based on Filtered Research - [4 marks]**\n",
    "> **Use the filtered papers, to generate ideas for the Reseach Proposal.**\n",
    "---\n",
    "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt, 1 mark for the Generating Idea and 1 mark for fetching file path of chosen idea along with successful completion of this section, including debugging or modifying the code if necessary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN5c3WhIEpzL"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "**TASK:** Write an Prompt which can be used to generate 5 ideas for the Research Proposal, each idea should consist:\n",
    "\n",
    "1. **Idea X:** [Concise Title of the Project Idea]  \\n\n",
    "2. **Description:** [Brief and targeted description summarizing the objectives, innovative elements, scientific rationale, and anticipated impact.]  \\n\n",
    "3. **Citation:** [Author(s), Year or Paper Title]  \\n\n",
    "4. **NOFO Alignment:** [List two or more specific NOFO requirements that this idea directly addresses]  \\n\n",
    "5. **File Path of the Research Paper:** [Exact file path, ending in .pdf]\n",
    "\n",
    "- Use the Delimiter `---` for defining the structure of the sample outputs in the prompt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLgOVonjveNM"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "#### Generating 5 Ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NoHQ6HH1kiD4"
   },
   "outputs": [],
   "source": [
    "# Note to self: Be sure to add additional details from page linked in the NOFO pdf\n",
    "# Also need to include constraints, e.g., \"Digital health test beds that leverage well-established \n",
    "# digital health platforms to optimize evidence-based digital mental health interventions\"\n",
    "\n",
    "gen_idea_prompt = f\"\"\"\n",
    "\n",
    "\n",
    "<WRITE YOUR PROMPT HERE>\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-oCJeXkcBKd"
   },
   "outputs": [],
   "source": [
    "ideas = llm.invoke(gen_idea_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952
    },
    "id": "TQpW_7cKco8Q",
    "outputId": "6b59af58-d64c-4804-f5a6-bfe89bb023dd"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(ideas.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For consideration if extracted text is not clean enough\n",
    "# Add post-extraction GPT-enabled noise removal step\n",
    "# to remove additional noise from chunks\n",
    "\n",
    "# Too resource intensive for full data set. Add later if needed.\n",
    "\n",
    "# import json\n",
    "# from openai import OpenAI\n",
    "\n",
    "# # Initialize OpenAI client\n",
    "# client = OpenAI()\n",
    "\n",
    "# def semantic_clean_text(raw_text):\n",
    "#     prompt = f\"\"\"\n",
    "# You are a document cleaner. Extract ONLY the main body text from the following academic or technical document:\n",
    "# - Remove page numbers, headers/footers\n",
    "# - Remove title page, author affiliations, figure/table captions\n",
    "# - Remove references/bibliography sections\n",
    "# - Keep abstracts, introductions, main sections, and conclusions\n",
    "\n",
    "# Document:\n",
    "# \\\"\\\"\\\"{raw_text}\\\"\\\"\\\"\n",
    "\n",
    "# Return only the cleaned text.\n",
    "# \"\"\"\n",
    "#     response = client.responses.create(\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         input=prompt,\n",
    "#         max_output_tokens=4000\n",
    "#     )\n",
    "#     return response.output_text\n",
    "\n",
    "# # --- Ingest cleaned + chunked data and post-process with GPT ---\n",
    "# input_path = \"data/cleaned_chunked_papers.json\"\n",
    "# output_path = \"data/cleaned_gpt.json\"\n",
    "\n",
    "# # Load chunked data\n",
    "# with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     chunked_data = json.load(f)\n",
    "\n",
    "# # Prepare list for GPT-processed results\n",
    "# gpt_cleaned_data = []\n",
    "\n",
    "# # Loop through each document\n",
    "# for record in chunked_data:\n",
    "#     doc_id = record[\"id\"]\n",
    "#     gpt_chunks = []\n",
    "\n",
    "#     print(f\"Post-processing (GPT cleanup): {doc_id}\")\n",
    "\n",
    "#     # Apply GPT cleaning to each chunk\n",
    "#     for chunk in record[\"chunks\"]:\n",
    "#         cleaned_chunk = semantic_clean_text(chunk)\n",
    "#         gpt_chunks.append(cleaned_chunk)\n",
    "\n",
    "#     # Store result\n",
    "#     gpt_cleaned_data.append({\n",
    "#         \"id\": doc_id,\n",
    "#         \"chunks\": gpt_chunks\n",
    "#     })\n",
    "\n",
    "# # Save GPT-cleaned data\n",
    "# with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(gpt_cleaned_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# print(f\"Saved GPT post-processed chunks to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLD4_7trvhKL"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "#### Choosing 1 Idea and fetching details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3ulgD6_dkrJ"
   },
   "outputs": [],
   "source": [
    "# Modify the idea_number for choosing the different idea\n",
    "idea_number = 5   # change the number if you wish to choose and generate the research proposal for another idea\n",
    "chosen_idea = ideas.content.split(\"---\")[idea_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DcV8QY1irIyH",
    "outputId": "d71e7380-e5d3-49d8-cd93-3dceedd57cf8"
   },
   "outputs": [],
   "source": [
    "# Import required libraries for core functionality\n",
    "import re\n",
    "\n",
    "# Use a regular expression to find the file path of the research paper\n",
    "\n",
    "pattern = r\"File Path of the Research Paper:\\*\\*\\s*(.+?)\\n\"\n",
    "# If you are unable to extract the file path successfully using this pattern, use the `ChatGPT` or any other LLM to find the pattern that works for you, simply provide the LLM the sample response of your whole ideas and ask the LLM to generate the regex patterm for extracting the \"File Path of the Research Paper\"\n",
    "\n",
    "match = re.search(pattern, chosen_idea)\n",
    "\n",
    "if match:\n",
    "  idea_generated_from_research_paper = match.group(1).strip()\n",
    "  print(\"Filepath : \", idea_generated_from_research_paper)\n",
    "else:\n",
    "  print(\"File Path of the Research Paper not found in the chosen idea.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51quLkMgi7S5"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Step 4: Proposal Blueprint Preparation - [3 Marks]**\n",
    "\n",
    "> **Select appropriate research ideas for the proposal and supply 'Sample Research Proposals' as templates to the LLM to support the generation of the final proposal.**\n",
    "---   \n",
    "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt and 1 mark for the successful completion of this section, including debugging or modifying the code if necessary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJhO1BFHC7cE"
   },
   "source": [
    "**TASK:** Write an Prompt which can be used to generate the Research Proposal.\n",
    "\n",
    "The prompt should be able to craft a research proposal based on the sample research proposal template, using one of the ideas generated above. The proposal should include references to the actual research papers from which the ideas are derived and should align well with the NOFO documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMOe-9_AgKvN"
   },
   "outputs": [],
   "source": [
    "# Here we need to add the full papers instead of the summary\n",
    "# Load PDF files and extract content using PyPDFLoader\n",
    "chosen_idea_rp = PyPDFLoader(idea_generated_from_research_paper, mode=\"single\").load()\n",
    "\n",
    "# Loading the sample research proposal template\n",
    "# Load PDF files and extract content using PyPDFLoader\n",
    "research_proposal_template = PyPDFLoader(\" <Path of Research Proposal Template> \", mode=\"single\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e6c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pypdf import PdfReader\n",
    "import camelot\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import tiktoken\n",
    "\n",
    "# --- Tokenization setup ---\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "MAX_TOKENS = 127500          # total model context window\n",
    "EXTRACTION_BUDGET = 100000   # reserve ~20% for prompts/response\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Count tokens using tiktoken encoding.\"\"\"\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# --- Load matching papers from JSON log ---\n",
    "def load_matched_papers(json_path, pdf_folder=\"content\"):\n",
    "    \"\"\"\n",
    "    Extract list of relevant document file paths from the latest JSON iteration.\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Take the last iteration's relevant_documents\n",
    "    last_iteration = data[-1]\n",
    "    relevant_docs = last_iteration.get(\"relevant_documents\", [])\n",
    "    \n",
    "    # Build file paths for each relevant doc (assumes they exist in pdf_folder)\n",
    "    file_paths = []\n",
    "    for doc in relevant_docs:\n",
    "        title = doc[\"title\"]\n",
    "        pdf_path = os.path.join(pdf_folder, title)\n",
    "        if os.path.exists(pdf_path):\n",
    "            file_paths.append(pdf_path)\n",
    "        else:\n",
    "            print(f\"Warning: {pdf_path} not found. Skipping.\")\n",
    "    return file_paths\n",
    "\n",
    "# --- Stage 1 & 2: Text + Table extraction ---\n",
    "def extract_text_and_tables(file_path, token_budget):\n",
    "    \"\"\"Extract text and tables within token budget.\"\"\"\n",
    "    content = \"\"\n",
    "    token_count = 0\n",
    "\n",
    "    # Stage 1: PyPDF text extraction\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text() or \"\"\n",
    "            token_count += count_tokens(page_text)\n",
    "            if token_count > token_budget:\n",
    "                print(f\"Token budget reached during text extraction: {file_path}\")\n",
    "                break\n",
    "            content += page_text\n",
    "    except Exception as e:\n",
    "        print(f\"PyPDF extraction failed: {e}\")\n",
    "\n",
    "    # Stage 2: Table extraction (Camelot)\n",
    "    # try:\n",
    "    #     tables = camelot.read_pdf(file_path, pages='all')\n",
    "    #     for table in tables:\n",
    "    #         table_text = \"\\n[Table Extracted]\\n\" + table.df.to_string()\n",
    "    #         token_count += count_tokens(table_text)\n",
    "    #         if token_count > token_budget:\n",
    "    #             print(f\"Token budget reached during table extraction: {file_path}\")\n",
    "    #             break\n",
    "    #         content += table_text\n",
    "    # except Exception:\n",
    "    #     pass\n",
    "\n",
    "    return content, token_count\n",
    "\n",
    "# --- Stage 3: OCR extraction ---\n",
    "# def extract_ocr(file_path, token_budget, current_tokens=0):\n",
    "#     \"\"\"Extract OCR text (figures/scanned pages) within remaining token budget.\"\"\"\n",
    "#     content = \"\"\n",
    "#     token_count = current_tokens\n",
    "\n",
    "#     try:\n",
    "#         images = convert_from_path(file_path)\n",
    "#         for image in images:\n",
    "#             ocr_text = pytesseract.image_to_string(image)\n",
    "#             token_count += count_tokens(ocr_text)\n",
    "#             if token_count > token_budget:\n",
    "#                 print(f\"Token budget reached during OCR extraction: {file_path}\")\n",
    "#                 break\n",
    "#             content += \"\\n[OCR Extracted]\\n\" + ocr_text\n",
    "#     except Exception:\n",
    "#         pass\n",
    "\n",
    "    return content\n",
    "\n",
    "# --- Process all matched papers ---\n",
    "def process_matched_papers(json_path, pdf_folder=\"content\"):\n",
    "    \"\"\"\n",
    "    Load matched papers from JSON and process them using multi-stage extraction:\n",
    "    Pass 1: Text + Tables\n",
    "    Pass 2: OCR (Figures)\n",
    "    Returns dict mapping filename -> combined extracted content.\n",
    "    \"\"\"\n",
    "    matched_files = load_matched_papers(json_path, pdf_folder)\n",
    "    text_table_data = {}\n",
    "    token_usage = {}\n",
    "\n",
    "    for file_path in matched_files:\n",
    "        print(f\"Extracting text/tables: {os.path.basename(file_path)}\")\n",
    "        content, tokens_used = extract_text_and_tables(file_path, EXTRACTION_BUDGET)\n",
    "        text_table_data[os.path.basename(file_path)] = content\n",
    "        token_usage[os.path.basename(file_path)] = tokens_used\n",
    "\n",
    "    # Return text_table_data directly\n",
    "    return text_table_data\n",
    "\n",
    "    # Pass 2: Extract OCR for all files (if budget allows)\n",
    "    # for file_path in matched_files:\n",
    "    #     filename = os.path.basename(file_path)\n",
    "    #     remaining_budget = EXTRACTION_BUDGET - token_usage.get(filename, 0)\n",
    "    #     if remaining_budget > 0:\n",
    "    #         print(f\"Extracting OCR: {filename} (remaining budget: {remaining_budget})\")\n",
    "    #         ocr_content = extract_ocr(file_path, EXTRACTION_BUDGET, token_usage[filename])\n",
    "    #         results[filename] = text_table_data[filename] + ocr_content\n",
    "    #     else:\n",
    "    #         print(f\"Skipping OCR for {filename} (no remaining token budget)\")\n",
    "    #         results[filename] = text_table_data[filename]\n",
    "\n",
    "# Example usage:\n",
    "# matched_content = process_matched_papers(\"/mnt/data/prompt_evaluation_log_cleaned.json\", pdf_folder=\"../content\")\n",
    "# print(matched_content.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_content = process_matched_papers(\"prompt_evaluation_log_cleaned.json\", pdf_folder=\"data/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matched_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OGcodZLo7VE"
   },
   "outputs": [],
   "source": [
    "research_proposal_template_prompt = f\"\"\"\n",
    "\n",
    "\n",
    "<WRITE YOUR PROMPT HERE>\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gXIZE0jkieg"
   },
   "outputs": [],
   "source": [
    "research_plan = llm.invoke(research_proposal_template_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1UFS-Vxlkib0",
    "outputId": "2761571a-d2a0-421c-b429-252cc78ddd41"
   },
   "outputs": [],
   "source": [
    "display(Markdown(research_plan.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkuiPncysdCa"
   },
   "outputs": [],
   "source": [
    "# @title **Optional Part - Creating a PDF of the Research Proposal**\n",
    "# The code in this cell block is used for printing out the output in the PDF format\n",
    "from markdown_pdf import MarkdownPdf, Section\n",
    "\n",
    "pdf = MarkdownPdf()\n",
    "pdf.add_section(Section(research_plan.content))\n",
    "pdf.save(\"Reseach Proposal First Draft.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77t_iYgni7QV"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Step 5: Proposal Evaluation Against NOFO Criteria - [3 Marks]**\n",
    "> **Use the LLM to evaluate the generated proposal (LLM-as-Judge) and assess its alignment with the NOFO criteria.**\n",
    "   \n",
    "\n",
    "---\n",
    "<font color=Red>**Note:**</font> *2 marks are awarded for the prompt and 1 mark for the successful completion of this section, including debugging or modifying the code if necessary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXWK_mZewlim"
   },
   "source": [
    "**TASK:** Write an Prompt which can be used to evaluate the Research Proposal based on:\n",
    "1. **Innovation**\n",
    "2. **Significance**\n",
    "3. **Approach**\n",
    "4. **Investigator Expertise**\n",
    "\n",
    "- Ask the LLM to rate on each of the criteria from **1 (Poor)** to **5 (Excellent)**\n",
    "- Ask the LLM to provide the resonse in the json format\n",
    "```JSON\n",
    "name: Innovation\n",
    "    justification: \"<Justification>\"\n",
    "    score: <1-5>\n",
    "    strengths: \"<Strength 1>\"\n",
    "    weaknesses: \"<Weakness 1>\"\n",
    "    recommendations: \"<Recommendation 1>\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ax5H703ZhZ7y"
   },
   "outputs": [],
   "source": [
    "evaluation_prompt = f'''\n",
    "\n",
    "\n",
    "<WRITE YOUR PROMPT HERE>\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aZox8iNhZ5g"
   },
   "outputs": [],
   "source": [
    "# Call the LLM with the prepared prompt and truncated paper content\n",
    "eval_response = llm.invoke(evaluation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCx7_am-hZ3H"
   },
   "outputs": [],
   "source": [
    "# Import required libraries for core functionality\n",
    "import json\n",
    "json_resp = json.loads(eval_response.content[7:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vwAFtxolhZpD",
    "outputId": "a5d2b346-065a-428a-8c5d-a38cd57a90ae"
   },
   "outputs": [],
   "source": [
    "for key, value in json_resp.items():\n",
    "  print(f\"---\\n{key}:\")\n",
    "  if isinstance(value, list):\n",
    "    for item in value:\n",
    "      for k, v in item.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "      print(\"=\"*50)\n",
    "  elif isinstance(value, dict):\n",
    "    for k, v in value.items():\n",
    "      print(f\"  {k}: {v}\")\n",
    "  else:\n",
    "    print(f\"  {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fomQFyZAi7N4"
   },
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "## **Step 6: Human Review and Refinement of Proposal**\n",
    "> **Perform Human Evaluation of the generated Proposal. Edit or Modify the proposal as necessary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HWwfHOXHmMOu",
    "outputId": "87320d16-f244-429e-e5cb-215e5eae01e7"
   },
   "outputs": [],
   "source": [
    "display(Markdown(research_plan.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRbWRHAJ_KXz"
   },
   "source": [
    "# **Step 7: Summary and Recommendation - [2 Marks]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jC6F4JezA840"
   },
   "source": [
    "Based on the projects, learners are expected to share their observations, key learnings, and insights related to this business use case, including the challenges they encountered.\n",
    "\n",
    "Additionally, they should recommend or explain any changes that could improve the project, along with suggesting additional steps that could be taken for further enhancement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8_WFIYIB12b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c868c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Enhanced PDF Processing (Commenting original PyPDF-only approach) ---\n",
    "# Original starter code (commented for traceability):\n",
    "# Load PDF files and extract content using PyPDFLoader\n",
    "# docs = PyPDFLoader(file_path, mode=\"single\").load()\n",
    "\n",
    "# New Implementation: Multi-stage parsing (PyPDF → Camelot/Tabula → OCR fallback)\n",
    "# Purpose: Capture text, tables, and figures from diverse PDF formats (Mermaid C node, Rubric Step 2).\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "# Import required libraries for core functionality\n",
    "import camelot\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def process_pdf_multistage(file_path):\n",
    "    \"\"\"\n",
    "    Multi-stage pipeline for extracting text, tables, and figures from PDFs.\n",
    "    Stages:\n",
    "    1. PyPDF (text)\n",
    "    2. Camelot/Tabula (tables)\n",
    "    3. OCR (scanned pages/figures)\n",
    "    \"\"\"\n",
    "    content = \"\"\n",
    "\n",
    "    # Stage 1: PyPDF text extraction\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        for page in reader.pages:\n",
    "            content += page.extract_text() or \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"PyPDF extraction failed: {e}\")\n",
    "\n",
    "    # Stage 2: Table extraction (Camelot)\n",
    "    try:\n",
    "        tables = camelot.read_pdf(file_path, pages='all')\n",
    "        for table in tables:\n",
    "            content += \"\\n[Table Extracted]\\n\" + table.df.to_string()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Stage 3: OCR fallback for scanned pages or figures\n",
    "    try:\n",
    "        images = convert_from_path(file_path)\n",
    "        for image in images:\n",
    "            text = pytesseract.image_to_string(image)\n",
    "            content += \"\\n[OCR Extracted]\\n\" + text\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6527505",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Hybrid Retrieval (BM25 + Embeddings) ---\n",
    "# Original code used either BM25 OR embeddings; this combines both (Mermaid D node, Rubric Step 2).\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def hybrid_retrieval_setup(docs_text):\n",
    "    \"\"\"\n",
    "    Creates BM25 and embedding indexes for hybrid search.\n",
    "    \"\"\"\n",
    "    # BM25 Index\n",
    "    tokenized_corpus = [doc.split(\" \") for doc in docs_text]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    # Embedding Index\n",
    "    embed_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectorstore = Chroma.from_texts(docs_text, embed_model)\n",
    "\n",
    "    return bm25, vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aabb353",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Agentic Components (Research Analyst, Proposal Writer, Compliance Checker) ---\n",
    "# Implements multi-agent workflow (Mermaid E subgraph, Rubric Step 3-4).\n",
    "\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "def analyze_papers(query):\n",
    "    return \"Synthesis of relevant papers\"\n",
    "\n",
    "def check_compliance(proposal):\n",
    "    return \"Compliance report\"\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"Research Analyst\", func=analyze_papers, description=\"Synthesizes relevant papers.\"),\n",
    "    Tool(name=\"Compliance Checker\", func=check_compliance, description=\"Ensures NOFO alignment.\")\n",
    "]\n",
    "\n",
    "# Initialize agent with zero-shot reasoning and tools\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a917953",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Agentic Components (Research Analyst, Proposal Writer, Compliance Checker) ---\n",
    "# Implements multi-agent workflow (Mermaid E subgraph, Rubric Step 3-4).\n",
    "\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "def analyze_papers(query):\n",
    "    return \"Synthesis of relevant papers\"\n",
    "\n",
    "def check_compliance(proposal):\n",
    "    return \"Compliance report\"\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"Research Analyst\", func=analyze_papers, description=\"Synthesizes relevant papers.\"),\n",
    "    Tool(name=\"Compliance Checker\", func=check_compliance, description=\"Ensures NOFO alignment.\")\n",
    "]\n",
    "\n",
    "# Initialize agent with zero-shot reasoning and tools\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5141a498",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Multi-Criteria Evaluation with Guardrails ---\n",
    "# Original evaluation only scored NIH criteria; now adds guardrail flags (Mermaid G node, Rubric Step 5).\n",
    "\n",
    "evaluation_prompt = f\"\"\"\n",
    "Evaluate the proposal on:\n",
    "1. Innovation\n",
    "2. Significance\n",
    "3. Approach\n",
    "4. Investigator Expertise\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"criteria\": [\n",
    "    {{\n",
    "      \"name\": \"Innovation\",\n",
    "      \"score\": 1-5,\n",
    "      \"strengths\": \"...\",\n",
    "      \"weaknesses\": \"...\",\n",
    "      \"recommendations\": \"...\"\n",
    "    }},\n",
    "    ...\n",
    "  ],\n",
    "  \"overall_score\": 1-5,\n",
    "  \"guardrail_flags\": [\"hallucination risk\", \"compliance gap\"]\n",
    "}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b907ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Caching Intermediate Steps ---\n",
    "# Saves embeddings, filtered papers, and draft proposals for reuse (Mermaid J node, Rubric Step 7).\n",
    "\n",
    "# Import required libraries for core functionality\n",
    "import pickle\n",
    "\n",
    "def save_checkpoint(data, name):\n",
    "    with open(f\"checkpoint_{name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_checkpoint(name):\n",
    "    try:\n",
    "        with open(f\"checkpoint_{name}.pkl\", \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7667a6",
   "metadata": {},
   "source": [
    "> **Note:** The following section explains core functionality and workflow.\n",
    "\n",
    "\n",
    "# Quick Reference: Few-Shot + Agentic Enhancements\n",
    "\n",
    "This section provides details about the few-shot pool, semantic versioning, and agentic conflict resolver integrated into this workflow.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "**Semantic Versioning**\n",
    "- Automatically increments version numbers (`v2-fewshot`, `v3-agentic`) based on features used.\n",
    "- Few-shot only → `-fewshot`\n",
    "- Few-shot + agentic resolver → `-agentic`\n",
    "\n",
    "**Few-Shot Pool**\n",
    "- Derived from cleaned log (`prompt_evaluation_log_cleaned.json`).\n",
    "- Filters examples with ≥80% hybrid confidence.\n",
    "- Balances relevant/irrelevant examples 50/50 and ensures diversity.\n",
    "\n",
    "**Agentic Conflict Resolver**\n",
    "- Activates when model vs. rule confidence differs by >20%.\n",
    "- Produces reconciled decision and rationale logged under `agentic_resolution`.\n",
    "\n",
    "**Enhanced Logging Fields**\n",
    "- `decision_source`: hybrid (model + rule)\n",
    "- `hybrid_confidence`: average of model and rule confidence\n",
    "- `agentic_resolution`: reconciliation result (if applicable)\n",
    "- `prompt_version`: auto-generated semantic version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da55b749",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# VERSION TRACKING + FEW-SHOT REBUILDER + AGENTIC RESOLVER\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Function: Determine the next semantic version string for the prompt\n",
    "def get_next_prompt_version(log_path, agentic_enabled=False):\n",
    "    \"\"\"\n",
    "    Determine next semantic version based on last logged version.\n",
    "    Increments number, adds suffix based on features used.\n",
    "    \"\"\"\n",
    "# Import required libraries for core functionality\n",
    "    import os, json, re\n",
    "    version_num = 1\n",
    "    if os.path.exists(log_path):\n",
    "        with open(log_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                data = []\n",
    "        # Extract last version number\n",
    "        for entry in reversed(data):\n",
    "            if \"prompt_version\" in entry:\n",
    "                match = re.match(r\"v(\\d+)\", entry[\"prompt_version\"])\n",
    "                if match:\n",
    "                    version_num = int(match.group(1)) + 1\n",
    "                break\n",
    "\n",
    "    suffix = \"-agentic\" if agentic_enabled else \"-fewshot\"\n",
    "    return f\"v{version_num}{suffix}\"\n",
    "\n",
    "\n",
    "# Function: Build balanced high-confidence few-shot example pool from the log\n",
    "def rebuild_few_shot_pool(cleaned_log_path, min_conf=80, max_examples=4):\n",
    "    \"\"\"\n",
    "    Build balanced high-confidence few-shot pool from cleaned log.\n",
    "    Balances relevant and irrelevant, ensures diversity.\n",
    "    \"\"\"\n",
    "# Import required libraries for core functionality\n",
    "    import json, random\n",
    "    with open(cleaned_log_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    relevant, irrelevant = [], []\n",
    "    for iteration in data:\n",
    "        for doc in iteration.get(\"relevant_documents\", []):\n",
    "            hybrid_conf = max(doc.get(\"model_confidence\", 0), doc.get(\"rule_confidence\", 0))\n",
    "            if hybrid_conf >= min_conf:\n",
    "                relevant.append((doc[\"title\"], doc[\"reasoning\"]))\n",
    "        for doc in iteration.get(\"irrelevant_documents\", []):\n",
    "            irrelevant.append((doc, \"PAPER NOT RELATED TO TOPIC\"))\n",
    "\n",
    "    # Shuffle and balance\n",
    "    half = max_examples // 2\n",
    "    random.shuffle(relevant)\n",
    "    random.shuffle(irrelevant)\n",
    "    return relevant[:half] + irrelevant[:half]\n",
    "\n",
    "\n",
    "# Function: Resolve discrepancies between model and rule confidences using agentic logic\n",
    "def agentic_conflict_resolver(doc_title, reasoning_json, model_conf, rule_conf):\n",
    "    \"\"\"\n",
    "    Agentic layer to reconcile conflicts:\n",
    "    - Triggered when discrepancy exceeds threshold\n",
    "    - Returns reconciled decision and rationale\n",
    "    \"\"\"\n",
    "    rationale = []\n",
    "    if abs(model_conf - rule_conf) > 20:\n",
    "        if rule_conf > model_conf:\n",
    "            final_decision = \"RELEVANT\" if rule_conf >= 50 else \"PAPER NOT RELATED TO TOPIC\"\n",
    "            rationale.append(\"Rule confidence higher; prioritizing deterministic criteria.\")\n",
    "        else:\n",
    "            final_decision = \"RELEVANT\" if model_conf >= 50 else \"PAPER NOT RELATED TO TOPIC\"\n",
    "            rationale.append(\"Model confidence higher; prioritizing LLM interpretation.\")\n",
    "    else:\n",
    "        final_decision = \"RELEVANT\" if (model_conf + rule_conf) / 2 >= 50 else \"PAPER NOT RELATED TO TOPIC\"\n",
    "        rationale.append(\"Confidences close; hybrid average used for decision.\")\n",
    "\n",
    "    return {\n",
    "        \"final_decision\": final_decision,\n",
    "        \"rationale\": \" \".join(rationale)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16955fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# ENHANCED LOGGING WITH SEMANTIC VERSIONING AND AGENTIC RESOLUTION\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Ensure this cell is run AFTER document processing and building relevant_docs_with_reasoning\n",
    "\n",
    "# Define constants for few-shot\n",
    "# Define configuration for few-shot example retrieval (number of examples)\n",
    "FEW_SHOT_MAX_EXAMPLES = 4\n",
    "# Minimum confidence threshold for including examples in few-shot prompting\n",
    "MIN_CONFIDENCE_FOR_FEWSHOT = 70\n",
    "# Path to the cleaned JSON log file where prompt evaluation iterations are stored\n",
    "LOG_PATH = \"prompt_evaluation_log_cleaned.json\"\n",
    "\n",
    "# Determine prompt version\n",
    "# Path to the cleaned JSON log file where prompt evaluation iterations are stored\n",
    "current_version = get_next_prompt_version(LOG_PATH, agentic_enabled=any(doc.get('flagged_for_review', False) for doc in relevant_docs_with_reasoning))\n",
    "\n",
    "# Add decision source and hybrid confidence\n",
    "for doc in relevant_docs_with_reasoning:\n",
    "    doc[\"decision_source\"] = \"hybrid\"\n",
    "    doc[\"hybrid_confidence\"] = (doc[\"model_confidence\"] + doc[\"rule_confidence\"]) / 2\n",
    "\n",
    "# Add agentic resolution for flagged docs\n",
    "for doc in relevant_docs_with_reasoning:\n",
    "    if doc.get(\"flagged_for_review\"):\n",
    "        resolution = agentic_conflict_resolver(\n",
    "            doc_title=doc[\"title\"],\n",
    "            reasoning_json=doc[\"reasoning\"],\n",
    "            model_conf=doc[\"model_confidence\"],\n",
    "            rule_conf=doc[\"rule_confidence\"]\n",
    "        )\n",
    "        doc[\"agentic_resolution\"] = resolution\n",
    "\n",
    "# Append prompt_version to log\n",
    "# Path to the cleaned JSON log file where prompt evaluation iterations are stored\n",
    "with open(LOG_PATH, \"r+\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    if data:\n",
    "        data[-1][\"prompt_version\"] = current_version\n",
    "    f.seek(0)\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    f.truncate()\n",
    "\n",
    "print(f\"Logged with prompt version: {current_version}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional enhancements proposed by Claude\n",
    "\n",
    "Your flowchart shows a well-structured approach to the RFP response generation system. Here are several improvements I'd recommend to enhance the robustness and effectiveness of your solution:\n",
    "\n",
    "1. Enhanced RFP Requirements Extraction\n",
    "After step B, add a sub-process for:\n",
    "\n",
    "Requirement Categorization: Classify requirements into mandatory vs. optional, technical vs. administrative\n",
    "Scoring Rubric Extraction: Specifically parse how proposals will be evaluated\n",
    "Budget Constraints Analysis: Extract funding limits and cost-effectiveness criteria\n",
    "Timeline Extraction: Identify key dates and milestone requirements\n",
    "\n",
    "2. Improved Paper Processing Pipeline\n",
    "Between steps C and D, consider adding:\n",
    "\n",
    "Citation Network Analysis: Map relationships between papers to identify influential work\n",
    "Method/Innovation Extraction: Specifically extract methodologies and novel approaches\n",
    "Results/Outcomes Extraction: Capture quantitative results and impact metrics\n",
    "Quality Assessment: Add a paper quality scoring mechanism (impact factor, recency, relevance)\n",
    "\n",
    "3. Enhanced Retrieval and Ranking\n",
    "Expand step D with:\n",
    "\n",
    "Multi-Query Generation: Generate multiple search queries from different RFP aspects\n",
    "Cross-Reference Validation: Verify that selected papers actually support proposed innovations\n",
    "Diversity Scoring: Ensure selected papers cover different aspects of the RFP\n",
    "Gap Analysis: Identify what the RFP asks for that isn't well-covered in existing research\n",
    "\n",
    "4. Strengthened Agentic Architecture\n",
    "Add these specialized agents to your existing three:\n",
    "\n",
    "Innovation Synthesizer Agent: Combines findings from multiple papers into novel approaches\n",
    "Budget Estimator Agent: Ensures proposals are financially realistic\n",
    "Risk Assessment Agent: Identifies potential implementation challenges\n",
    "Competitive Analysis Agent: Positions your proposal against likely competitors\n",
    "\n",
    "5. Improved Evaluation and Refinement\n",
    "Enhance the evaluation loop (G-I) with:\n",
    "\n",
    "Specific Weakness Detection: Not just overall score, but identify specific weak sections\n",
    "Competitive Benchmarking: Compare against successful past proposals if available\n",
    "Consistency Checking: Ensure all sections align and support each other\n",
    "Technical Feasibility Validation: Verify proposed solutions are implementable\n",
    "\n",
    "6. Additional Process Improvements\n",
    "Consider these architectural enhancements:\n",
    "flowchart LR\n",
    "    subgraph \"Knowledge Management\"\n",
    "        KB1[Domain Ontology]\n",
    "        KB2[Success Patterns DB]\n",
    "        KB3[Common Pitfalls DB]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Feedback Loops\"\n",
    "        FL1[Real-time Agent Collaboration]\n",
    "        FL2[Iterative Improvement Tracking]\n",
    "        FL3[Version Control System]\n",
    "    end\n",
    "7. Quality Assurance Additions\n",
    "\n",
    "Plagiarism Detection: Ensure generated content is original\n",
    "Fact Verification: Cross-check claims against source papers\n",
    "Readability Analysis: Ensure proposal meets target audience expectations\n",
    "Compliance Validation: Automated checks against all RFP requirements\n",
    "\n",
    "8. Output Enhancement\n",
    "For the final deliverables (step N), consider generating:\n",
    "\n",
    "Executive Summary: One-page overview for quick review\n",
    "Technical Appendix: Detailed methodology descriptions\n",
    "Budget Justification: Line-by-line cost explanations\n",
    "Risk Mitigation Plan: Addressing identified challenges\n",
    "Evaluation Metrics: How success will be measured\n",
    "\n",
    "9. Monitoring and Logging\n",
    "Add throughout the pipeline:\n",
    "\n",
    "Decision Logging: Track why papers were selected/rejected\n",
    "Agent Reasoning Traces: Understand how proposals were generated\n",
    "Performance Metrics: Time taken, resources used, quality scores\n",
    "Error Handling: Graceful degradation if components fail\n",
    "\n",
    "10. Advanced Features\n",
    "Consider these stretch goals:\n",
    "\n",
    "Multi-RFP Learning: Learn from multiple RFPs to improve over time\n",
    "Collaborative Filtering: If multiple users, learn from collective behavior\n",
    "Adaptive Prompting: Adjust prompts based on intermediate results\n",
    "Uncertainty Quantification: Flag areas where the system is less confident\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3cxtywSY4tq0",
    "ZG1Ah1eDi7aK",
    "cXHRa9IlMycZ"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
